==========================================
SLURM_JOB_ID = 2505486
SLURM_NODELIST = gnode073
SLURM_JOB_GPUS = 0
==========================================
09/30 09:02:45 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.23 (main, Jun  5 2025, 13:40:20) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 268722126
    GPU 0: NVIDIA GeForce RTX 2080 Ti
    CUDA_HOME: /opt/cuda-12.1/
    NVCC: Cuda compilation tools, release 12.1, V12.1.66
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.1.2
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.2
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: True
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 268722126
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

09/30 09:02:46 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=2, enable=False)
crop_size = (
    512,
    1024,
)
data_preprocessor = dict(
    bgr_to_rgb=True,
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    pad_val=0,
    seg_pad_val=255,
    size=(
        512,
        1024,
    ),
    std=[
        58.395,
        57.12,
        57.375,
    ],
    test_cfg=dict(size_divisor=32),
    type='SegDataPreProcessor')
data_root = '/scratch/segmentation_benchmark/food_FINAL_resized/'
dataset_type = 'YourDataset_BIG'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=40000,
        save_best='mIoU',
        type='CheckpointHook'),
    logger=dict(interval=50, log_metric_by_epoch=False, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'))
default_scope = 'mmseg'
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=True,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
img_ratios = [
    0.5,
    0.75,
    1.0,
    1.25,
    1.5,
    1.75,
]
img_suffix = '.jpg'
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=False)
model = dict(
    backbone=dict(
        deep_stem=False,
        depth=50,
        frozen_stages=-1,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=False, type='SyncBN'),
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNet'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_val=0,
        seg_pad_val=255,
        size=(
            512,
            1024,
        ),
        std=[
            58.395,
            57.12,
            57.375,
        ],
        test_cfg=dict(size_divisor=32),
        type='SegDataPreProcessor'),
    decode_head=dict(
        align_corners=False,
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                0.1,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='mmdet.DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=True),
        num_classes=49,
        num_queries=100,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                init_cfg=None,
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=1024,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        im2col_step=64,
                        init_cfg=None,
                        norm_cfg=None,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            init_cfg=None,
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='mmdet.MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        train_cfg=dict(
            assigner=dict(
                match_costs=[
                    dict(type='mmdet.ClassificationCost', weight=2.0),
                    dict(
                        type='mmdet.CrossEntropyLossCost',
                        use_sigmoid=True,
                        weight=5.0),
                    dict(
                        eps=1.0,
                        pred_act=True,
                        type='mmdet.DiceCost',
                        weight=5.0),
                ],
                type='mmdet.HungarianAssigner'),
            importance_sample_ratio=0.75,
            num_points=12544,
            oversample_ratio=3.0,
            sampler=dict(type='mmdet.MaskPseudoSampler')),
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    attn_drop=0.0,
                    batch_first=True,
                    dropout_layer=None,
                    embed_dims=256,
                    num_heads=8,
                    proj_drop=0.0),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    add_identity=True,
                    dropout_layer=None,
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    attn_drop=0.0,
                    batch_first=True,
                    dropout_layer=None,
                    embed_dims=256,
                    num_heads=8,
                    proj_drop=0.0)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(mode='whole'),
    train_cfg=dict(),
    type='EncoderDecoder')
num_classes = 49
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(decay_mult=1.0, lr_mult=0.1),
            level_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_feat=dict(decay_mult=0.0, lr_mult=1.0)),
        norm_decay_mult=0.0),
    type='OptimWrapper')
optimizer = dict(
    betas=(
        0.9,
        0.999,
    ),
    eps=1e-08,
    lr=0.0001,
    type='AdamW',
    weight_decay=0.05)
param_scheduler = [
    dict(
        begin=0,
        by_epoch=False,
        end=320000,
        eta_min=0,
        power=0.9,
        type='PolyLR'),
]
randomness = dict(seed=268722126)
resume = False
seg_map_suffix = '.png'
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_prefix=dict(img_path='test/images', seg_map_path='test/masks'),
        data_root='/scratch/segmentation_benchmark/food_FINAL_resized/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                2048,
                1024,
            ), type='Resize'),
            dict(type='LoadAnnotations'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=8,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    classwise=True, iou_metrics=[
        'mIoU',
    ], type='IoUNanAbsent')
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        2048,
        1024,
    ), type='Resize'),
    dict(type='LoadAnnotations'),
    dict(type='PackSegInputs'),
]
train_cfg = dict(
    max_iters=320000, type='IterBasedTrainLoop', val_interval=40000)
train_dataloader = dict(
    batch_size=2,
    dataset=dict(
        data_prefix=dict(img_path='train/images', seg_map_path='train/masks'),
        data_root='/scratch/segmentation_benchmark/food_FINAL_resized/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(
                max_size=4096,
                resize_type='ResizeShortestEdge',
                scales=[
                    512,
                    614,
                    716,
                    819,
                    921,
                    1024,
                    1126,
                    1228,
                    1331,
                    1433,
                    1536,
                    1638,
                    1740,
                    1843,
                    1945,
                    2048,
                ],
                type='RandomChoiceResize'),
            dict(
                cat_max_ratio=0.75, crop_size=(
                    512,
                    1024,
                ), type='RandomCrop'),
            dict(prob=0.5, type='RandomFlip'),
            dict(type='PhotoMetricDistortion'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='InfiniteSampler'))
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(
        max_size=4096,
        resize_type='ResizeShortestEdge',
        scales=[
            512,
            614,
            716,
            819,
            921,
            1024,
            1126,
            1228,
            1331,
            1433,
            1536,
            1638,
            1740,
            1843,
            1945,
            2048,
        ],
        type='RandomChoiceResize'),
    dict(cat_max_ratio=0.75, crop_size=(
        512,
        1024,
    ), type='RandomCrop'),
    dict(prob=0.5, type='RandomFlip'),
    dict(type='PhotoMetricDistortion'),
    dict(type='PackSegInputs'),
]
tta_model = dict(type='SegTTAModel')
tta_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        transforms=[
            [
                dict(keep_ratio=True, scale_factor=0.5, type='Resize'),
                dict(keep_ratio=True, scale_factor=0.75, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.0, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.25, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.5, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.75, type='Resize'),
            ],
            [
                dict(direction='horizontal', prob=0.0, type='RandomFlip'),
                dict(direction='horizontal', prob=1.0, type='RandomFlip'),
            ],
            [
                dict(type='LoadAnnotations'),
            ],
            [
                dict(type='PackSegInputs'),
            ],
        ],
        type='TestTimeAug'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_prefix=dict(img_path='test/images', seg_map_path='test/masks'),
        data_root='/scratch/segmentation_benchmark/food_FINAL_resized/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                2048,
                1024,
            ), type='Resize'),
            dict(type='LoadAnnotations'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=8,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    iou_metrics=[
        'mIoU',
    ], type='IoUMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='SegLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = '/scratch/segmentation_benchmark/mmseg_work_dir/AR_mask2former'

09/30 09:02:55 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
09/30 09:02:55 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.0.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.0.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.0.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.0.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.0.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.0.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.0.downsample.1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.0.downsample.1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.1.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.1.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.1.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.1.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.1.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.1.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.2.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.2.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.2.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.2.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.2.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer1.2.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.0.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.0.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.0.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.0.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.0.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.0.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.0.downsample.1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.0.downsample.1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.1.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.1.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.1.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.1.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.1.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.1.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.2.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.2.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.2.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.2.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.2.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.2.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.3.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.3.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.3.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.3.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.3.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer2.3.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.0.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.0.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.0.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.0.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.0.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.0.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.0.downsample.1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.0.downsample.1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.1.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.1.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.1.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.1.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.1.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.1.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.2.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.2.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.2.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.2.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.2.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.2.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.3.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.3.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.3.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.3.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.3.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.3.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.4.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.4.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.4.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.4.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.4.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.4.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.5.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.5.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.5.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.5.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.5.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer3.5.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.0.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.0.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.0.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.0.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.0.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.0.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.0.downsample.1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.0.downsample.1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.1.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.1.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.1.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.1.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.1.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.1.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.2.bn1.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.2.bn1.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.2.bn2.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.2.bn2.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr=1e-05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:weight_decay=0.05
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr_mult=0.1
09/30 09:02:56 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:decay_mult=1.0
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.2.bn3.weight is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - WARNING - backbone.layer4.2.bn3.bias is skipped since its requires_grad=False
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.bias:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr=0.0001
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr_mult=1.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:decay_mult=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr=0.0001
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr_mult=1.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:decay_mult=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr=0.0001
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:weight_decay=0.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr_mult=1.0
09/30 09:02:56 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:decay_mult=0.0
09/30 09:02:56 - mmengine - WARNING - The prefix is not set in metric class IoUMetric.
09/30 09:02:56 - mmengine - INFO - load model from: torchvision://resnet50
09/30 09:02:56 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
09/30 09:03:02 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

09/30 09:03:03 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
09/30 09:03:03 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
09/30 09:03:03 - mmengine - INFO - Checkpoints will be saved to /scratch/segmentation_benchmark/mmseg_work_dir/AR_mask2former.
/home2/yasharora120/miniconda3/envs/mmseg/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400441250/work/aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
09/30 09:03:37 - mmengine - INFO - Iter(train) [    50/320000]  base_lr: 9.9986e-05 lr: 9.9986e-06  eta: 2 days, 13:07:56  time: 0.4199  data_time: 0.0086  memory: 10587  grad_norm: 225.1705  loss: 92.2626  decode.loss_cls: 3.3063  decode.loss_mask: 2.7368  decode.loss_dice: 3.8831  decode.d0.loss_cls: 7.8988  decode.d0.loss_mask: 2.1602  decode.d0.loss_dice: 2.9693  decode.d1.loss_cls: 3.1074  decode.d1.loss_mask: 2.1661  decode.d1.loss_dice: 2.9334  decode.d2.loss_cls: 2.9024  decode.d2.loss_mask: 2.2616  decode.d2.loss_dice: 2.8968  decode.d3.loss_cls: 2.8907  decode.d3.loss_mask: 2.2047  decode.d3.loss_dice: 2.9147  decode.d4.loss_cls: 2.8885  decode.d4.loss_mask: 2.2418  decode.d4.loss_dice: 2.9675  decode.d5.loss_cls: 2.9540  decode.d5.loss_mask: 2.3126  decode.d5.loss_dice: 3.0179  decode.d6.loss_cls: 3.0770  decode.d6.loss_mask: 2.7095  decode.d6.loss_dice: 3.3195  decode.d7.loss_cls: 3.2146  decode.d7.loss_mask: 2.8461  decode.d7.loss_dice: 3.4643  decode.d8.loss_cls: 3.2809  decode.d8.loss_mask: 3.0462  decode.d8.loss_dice: 3.6900
09/30 09:03:58 - mmengine - INFO - Iter(train) [   100/320000]  base_lr: 9.9972e-05 lr: 9.9972e-06  eta: 2 days, 1:19:50  time: 0.4237  data_time: 0.0089  memory: 5180  grad_norm: 409.7046  loss: 69.8492  decode.loss_cls: 2.5822  decode.loss_mask: 2.0165  decode.loss_dice: 2.4088  decode.d0.loss_cls: 7.7036  decode.d0.loss_mask: 1.8138  decode.d0.loss_dice: 2.2177  decode.d1.loss_cls: 2.5399  decode.d1.loss_mask: 1.7823  decode.d1.loss_dice: 2.1859  decode.d2.loss_cls: 2.4612  decode.d2.loss_mask: 1.7127  decode.d2.loss_dice: 2.0494  decode.d3.loss_cls: 2.2486  decode.d3.loss_mask: 1.7796  decode.d3.loss_dice: 2.1509  decode.d4.loss_cls: 2.2583  decode.d4.loss_mask: 1.7640  decode.d4.loss_dice: 2.0941  decode.d5.loss_cls: 2.2787  decode.d5.loss_mask: 1.9098  decode.d5.loss_dice: 2.1738  decode.d6.loss_cls: 2.3660  decode.d6.loss_mask: 1.8878  decode.d6.loss_dice: 2.1437  decode.d7.loss_cls: 2.3863  decode.d7.loss_mask: 1.9010  decode.d7.loss_dice: 2.2742  decode.d8.loss_cls: 2.4822  decode.d8.loss_mask: 1.9818  decode.d8.loss_dice: 2.2944
09/30 09:04:19 - mmengine - INFO - Iter(train) [   150/320000]  base_lr: 9.9958e-05 lr: 9.9958e-06  eta: 1 day, 21:27:47  time: 0.4257  data_time: 0.0086  memory: 5161  grad_norm: 494.3880  loss: 60.2107  decode.loss_cls: 2.2835  decode.loss_mask: 1.6119  decode.loss_dice: 1.6908  decode.d0.loss_cls: 7.6650  decode.d0.loss_mask: 1.4160  decode.d0.loss_dice: 1.7346  decode.d1.loss_cls: 2.4254  decode.d1.loss_mask: 1.5091  decode.d1.loss_dice: 1.5380  decode.d2.loss_cls: 2.3273  decode.d2.loss_mask: 1.5747  decode.d2.loss_dice: 1.4949  decode.d3.loss_cls: 2.2072  decode.d3.loss_mask: 1.7360  decode.d3.loss_dice: 1.5353  decode.d4.loss_cls: 2.2038  decode.d4.loss_mask: 1.7122  decode.d4.loss_dice: 1.5421  decode.d5.loss_cls: 2.2018  decode.d5.loss_mask: 1.7076  decode.d5.loss_dice: 1.5840  decode.d6.loss_cls: 2.1984  decode.d6.loss_mask: 1.7066  decode.d6.loss_dice: 1.6787  decode.d7.loss_cls: 2.1711  decode.d7.loss_mask: 1.6156  decode.d7.loss_dice: 1.6599  decode.d8.loss_cls: 2.1903  decode.d8.loss_mask: 1.6400  decode.d8.loss_dice: 1.6489
09/30 09:04:41 - mmengine - INFO - Iter(train) [   200/320000]  base_lr: 9.9944e-05 lr: 9.9944e-06  eta: 1 day, 19:33:18  time: 0.4258  data_time: 0.0085  memory: 5147  grad_norm: 322.9121  loss: 50.1553  decode.loss_cls: 2.1498  decode.loss_mask: 1.2651  decode.loss_dice: 1.1283  decode.d0.loss_cls: 7.5582  decode.d0.loss_mask: 1.3188  decode.d0.loss_dice: 1.2681  decode.d1.loss_cls: 2.0837  decode.d1.loss_mask: 1.3080  decode.d1.loss_dice: 1.1374  decode.d2.loss_cls: 2.0790  decode.d2.loss_mask: 1.2672  decode.d2.loss_dice: 1.0476  decode.d3.loss_cls: 2.0537  decode.d3.loss_mask: 1.3182  decode.d3.loss_dice: 1.1072  decode.d4.loss_cls: 2.0207  decode.d4.loss_mask: 1.2941  decode.d4.loss_dice: 1.1062  decode.d5.loss_cls: 2.0344  decode.d5.loss_mask: 1.2499  decode.d5.loss_dice: 1.0940  decode.d6.loss_cls: 2.1415  decode.d6.loss_mask: 1.1832  decode.d6.loss_dice: 1.0815  decode.d7.loss_cls: 2.0437  decode.d7.loss_mask: 1.2458  decode.d7.loss_dice: 1.1033  decode.d8.loss_cls: 2.0947  decode.d8.loss_mask: 1.2743  decode.d8.loss_dice: 1.0976
09/30 09:05:02 - mmengine - INFO - Iter(train) [   250/320000]  base_lr: 9.9930e-05 lr: 9.9930e-06  eta: 1 day, 18:26:08  time: 0.4298  data_time: 0.0089  memory: 5180  grad_norm: 453.6443  loss: 49.4665  decode.loss_cls: 2.3082  decode.loss_mask: 0.9210  decode.loss_dice: 1.0476  decode.d0.loss_cls: 7.4027  decode.d0.loss_mask: 1.0256  decode.d0.loss_dice: 1.4039  decode.d1.loss_cls: 2.1899  decode.d1.loss_mask: 1.0996  decode.d1.loss_dice: 1.2108  decode.d2.loss_cls: 2.1694  decode.d2.loss_mask: 1.0811  decode.d2.loss_dice: 1.1846  decode.d3.loss_cls: 2.1199  decode.d3.loss_mask: 1.1474  decode.d3.loss_dice: 1.1259  decode.d4.loss_cls: 2.1628  decode.d4.loss_mask: 1.1426  decode.d4.loss_dice: 1.1414  decode.d5.loss_cls: 2.2076  decode.d5.loss_mask: 1.1283  decode.d5.loss_dice: 1.1220  decode.d6.loss_cls: 2.2929  decode.d6.loss_mask: 1.0496  decode.d6.loss_dice: 1.1098  decode.d7.loss_cls: 2.2794  decode.d7.loss_mask: 0.9946  decode.d7.loss_dice: 1.0827  decode.d8.loss_cls: 2.3391  decode.d8.loss_mask: 0.9257  decode.d8.loss_dice: 1.0502
09/30 09:05:24 - mmengine - INFO - Iter(train) [   300/320000]  base_lr: 9.9916e-05 lr: 9.9916e-06  eta: 1 day, 17:42:12  time: 0.4295  data_time: 0.0086  memory: 5180  grad_norm: 274.7039  loss: 45.4411  decode.loss_cls: 2.2884  decode.loss_mask: 0.9060  decode.loss_dice: 1.0007  decode.d0.loss_cls: 7.2052  decode.d0.loss_mask: 0.8975  decode.d0.loss_dice: 1.2669  decode.d1.loss_cls: 2.2727  decode.d1.loss_mask: 0.8961  decode.d1.loss_dice: 1.0406  decode.d2.loss_cls: 2.1460  decode.d2.loss_mask: 0.7755  decode.d2.loss_dice: 0.9703  decode.d3.loss_cls: 2.1415  decode.d3.loss_mask: 0.8148  decode.d3.loss_dice: 0.9507  decode.d4.loss_cls: 2.2521  decode.d4.loss_mask: 0.6909  decode.d4.loss_dice: 0.9197  decode.d5.loss_cls: 2.3110  decode.d5.loss_mask: 0.7098  decode.d5.loss_dice: 0.9176  decode.d6.loss_cls: 2.3185  decode.d6.loss_mask: 0.7623  decode.d6.loss_dice: 0.9558  decode.d7.loss_cls: 2.3226  decode.d7.loss_mask: 0.7796  decode.d7.loss_dice: 0.9036  decode.d8.loss_cls: 2.2238  decode.d8.loss_mask: 0.8255  decode.d8.loss_dice: 0.9754
09/30 09:05:45 - mmengine - INFO - Iter(train) [   350/320000]  base_lr: 9.9902e-05 lr: 9.9902e-06  eta: 1 day, 17:10:41  time: 0.4290  data_time: 0.0088  memory: 5180  grad_norm: 303.3671  loss: 39.6417  decode.loss_cls: 1.8726  decode.loss_mask: 0.9219  decode.loss_dice: 0.7265  decode.d0.loss_cls: 7.0543  decode.d0.loss_mask: 0.9149  decode.d0.loss_dice: 0.9438  decode.d1.loss_cls: 1.7405  decode.d1.loss_mask: 0.8652  decode.d1.loss_dice: 0.8542  decode.d2.loss_cls: 1.6899  decode.d2.loss_mask: 0.8930  decode.d2.loss_dice: 0.7829  decode.d3.loss_cls: 1.7529  decode.d3.loss_mask: 0.8690  decode.d3.loss_dice: 0.7289  decode.d4.loss_cls: 1.7285  decode.d4.loss_mask: 0.8777  decode.d4.loss_dice: 0.6925  decode.d5.loss_cls: 1.7751  decode.d5.loss_mask: 0.9035  decode.d5.loss_dice: 0.7041  decode.d6.loss_cls: 1.8080  decode.d6.loss_mask: 0.8840  decode.d6.loss_dice: 0.7125  decode.d7.loss_cls: 1.8116  decode.d7.loss_mask: 0.9174  decode.d7.loss_dice: 0.7256  decode.d8.loss_cls: 1.8061  decode.d8.loss_mask: 0.9585  decode.d8.loss_dice: 0.7261
09/30 09:06:06 - mmengine - INFO - Iter(train) [   400/320000]  base_lr: 9.9888e-05 lr: 9.9888e-06  eta: 1 day, 16:47:04  time: 0.4281  data_time: 0.0085  memory: 5161  grad_norm: 403.3743  loss: 42.6586  decode.loss_cls: 2.0615  decode.loss_mask: 0.9912  decode.loss_dice: 0.8916  decode.d0.loss_cls: 6.9445  decode.d0.loss_mask: 0.9399  decode.d0.loss_dice: 1.0894  decode.d1.loss_cls: 1.9479  decode.d1.loss_mask: 0.9028  decode.d1.loss_dice: 0.8449  decode.d2.loss_cls: 1.9140  decode.d2.loss_mask: 0.9300  decode.d2.loss_dice: 0.8211  decode.d3.loss_cls: 1.9355  decode.d3.loss_mask: 0.9197  decode.d3.loss_dice: 0.8504  decode.d4.loss_cls: 1.9280  decode.d4.loss_mask: 0.9415  decode.d4.loss_dice: 0.8561  decode.d5.loss_cls: 1.9100  decode.d5.loss_mask: 0.8511  decode.d5.loss_dice: 0.9185  decode.d6.loss_cls: 1.9250  decode.d6.loss_mask: 0.8360  decode.d6.loss_dice: 0.8510  decode.d7.loss_cls: 1.9551  decode.d7.loss_mask: 0.9526  decode.d7.loss_dice: 0.9191  decode.d8.loss_cls: 2.0355  decode.d8.loss_mask: 0.9125  decode.d8.loss_dice: 0.8821
09/30 09:06:22 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 09:06:28 - mmengine - INFO - Iter(train) [   450/320000]  base_lr: 9.9874e-05 lr: 9.9874e-06  eta: 1 day, 16:28:50  time: 0.4297  data_time: 0.0088  memory: 5160  grad_norm: 358.7970  loss: 34.5986  decode.loss_cls: 1.6422  decode.loss_mask: 0.8436  decode.loss_dice: 0.5330  decode.d0.loss_cls: 6.7033  decode.d0.loss_mask: 0.8698  decode.d0.loss_dice: 0.6695  decode.d1.loss_cls: 1.5352  decode.d1.loss_mask: 0.8666  decode.d1.loss_dice: 0.5839  decode.d2.loss_cls: 1.4182  decode.d2.loss_mask: 0.8442  decode.d2.loss_dice: 0.5261  decode.d3.loss_cls: 1.4241  decode.d3.loss_mask: 0.8659  decode.d3.loss_dice: 0.5630  decode.d4.loss_cls: 1.4431  decode.d4.loss_mask: 0.9024  decode.d4.loss_dice: 0.5763  decode.d5.loss_cls: 1.4090  decode.d5.loss_mask: 0.9301  decode.d5.loss_dice: 0.5639  decode.d6.loss_cls: 1.4677  decode.d6.loss_mask: 0.8927  decode.d6.loss_dice: 0.5603  decode.d7.loss_cls: 1.4843  decode.d7.loss_mask: 0.9164  decode.d7.loss_dice: 0.5625  decode.d8.loss_cls: 1.5842  decode.d8.loss_mask: 0.8842  decode.d8.loss_dice: 0.5327
09/30 09:06:49 - mmengine - INFO - Iter(train) [   500/320000]  base_lr: 9.9860e-05 lr: 9.9860e-06  eta: 1 day, 16:15:29  time: 0.4301  data_time: 0.0088  memory: 5147  grad_norm: 237.1181  loss: 34.2204  decode.loss_cls: 1.6469  decode.loss_mask: 0.7062  decode.loss_dice: 0.5665  decode.d0.loss_cls: 6.5127  decode.d0.loss_mask: 0.7263  decode.d0.loss_dice: 0.7794  decode.d1.loss_cls: 1.6754  decode.d1.loss_mask: 0.6768  decode.d1.loss_dice: 0.6093  decode.d2.loss_cls: 1.5605  decode.d2.loss_mask: 0.6988  decode.d2.loss_dice: 0.6212  decode.d3.loss_cls: 1.5735  decode.d3.loss_mask: 0.7147  decode.d3.loss_dice: 0.6247  decode.d4.loss_cls: 1.5520  decode.d4.loss_mask: 0.6999  decode.d4.loss_dice: 0.5799  decode.d5.loss_cls: 1.5929  decode.d5.loss_mask: 0.6807  decode.d5.loss_dice: 0.5464  decode.d6.loss_cls: 1.6350  decode.d6.loss_mask: 0.6633  decode.d6.loss_dice: 0.5616  decode.d7.loss_cls: 1.6895  decode.d7.loss_mask: 0.6973  decode.d7.loss_dice: 0.5997  decode.d8.loss_cls: 1.7049  decode.d8.loss_mask: 0.7069  decode.d8.loss_dice: 0.6174
09/30 09:07:11 - mmengine - INFO - Iter(train) [   550/320000]  base_lr: 9.9846e-05 lr: 9.9846e-06  eta: 1 day, 16:03:59  time: 0.4305  data_time: 0.0086  memory: 5160  grad_norm: 255.8197  loss: 35.1156  decode.loss_cls: 1.6254  decode.loss_mask: 0.8384  decode.loss_dice: 0.5663  decode.d0.loss_cls: 6.3560  decode.d0.loss_mask: 0.7998  decode.d0.loss_dice: 0.7347  decode.d1.loss_cls: 1.6828  decode.d1.loss_mask: 0.7702  decode.d1.loss_dice: 0.6267  decode.d2.loss_cls: 1.6343  decode.d2.loss_mask: 0.7648  decode.d2.loss_dice: 0.5651  decode.d3.loss_cls: 1.6052  decode.d3.loss_mask: 0.7868  decode.d3.loss_dice: 0.5570  decode.d4.loss_cls: 1.6483  decode.d4.loss_mask: 0.8415  decode.d4.loss_dice: 0.6421  decode.d5.loss_cls: 1.6060  decode.d5.loss_mask: 0.7963  decode.d5.loss_dice: 0.6212  decode.d6.loss_cls: 1.6243  decode.d6.loss_mask: 0.7918  decode.d6.loss_dice: 0.5863  decode.d7.loss_cls: 1.6411  decode.d7.loss_mask: 0.7916  decode.d7.loss_dice: 0.6031  decode.d8.loss_cls: 1.7023  decode.d8.loss_mask: 0.7584  decode.d8.loss_dice: 0.5477
09/30 09:07:32 - mmengine - INFO - Iter(train) [   600/320000]  base_lr: 9.9832e-05 lr: 9.9832e-06  eta: 1 day, 15:54:07  time: 0.4304  data_time: 0.0090  memory: 5161  grad_norm: 337.9048  loss: 35.7646  decode.loss_cls: 1.9092  decode.loss_mask: 0.8290  decode.loss_dice: 0.4916  decode.d0.loss_cls: 6.2135  decode.d0.loss_mask: 0.6844  decode.d0.loss_dice: 0.6493  decode.d1.loss_cls: 1.8338  decode.d1.loss_mask: 0.6526  decode.d1.loss_dice: 0.4978  decode.d2.loss_cls: 1.7639  decode.d2.loss_mask: 0.7018  decode.d2.loss_dice: 0.4413  decode.d3.loss_cls: 1.8072  decode.d3.loss_mask: 0.8075  decode.d3.loss_dice: 0.4762  decode.d4.loss_cls: 1.8650  decode.d4.loss_mask: 0.8043  decode.d4.loss_dice: 0.5023  decode.d5.loss_cls: 1.9103  decode.d5.loss_mask: 0.7648  decode.d5.loss_dice: 0.5018  decode.d6.loss_cls: 1.9595  decode.d6.loss_mask: 0.7481  decode.d6.loss_dice: 0.4640  decode.d7.loss_cls: 1.9502  decode.d7.loss_mask: 0.7917  decode.d7.loss_dice: 0.5071  decode.d8.loss_cls: 1.9931  decode.d8.loss_mask: 0.7649  decode.d8.loss_dice: 0.4783
09/30 09:07:54 - mmengine - INFO - Iter(train) [   650/320000]  base_lr: 9.9817e-05 lr: 9.9817e-06  eta: 1 day, 15:45:58  time: 0.4298  data_time: 0.0085  memory: 5180  grad_norm: 331.9722  loss: 33.1630  decode.loss_cls: 1.4685  decode.loss_mask: 0.8133  decode.loss_dice: 0.5778  decode.d0.loss_cls: 5.9867  decode.d0.loss_mask: 0.8764  decode.d0.loss_dice: 0.7225  decode.d1.loss_cls: 1.5013  decode.d1.loss_mask: 0.9565  decode.d1.loss_dice: 0.6232  decode.d2.loss_cls: 1.4116  decode.d2.loss_mask: 0.8619  decode.d2.loss_dice: 0.5495  decode.d3.loss_cls: 1.4525  decode.d3.loss_mask: 0.8534  decode.d3.loss_dice: 0.5206  decode.d4.loss_cls: 1.4089  decode.d4.loss_mask: 0.8199  decode.d4.loss_dice: 0.5294  decode.d5.loss_cls: 1.4331  decode.d5.loss_mask: 0.8706  decode.d5.loss_dice: 0.5314  decode.d6.loss_cls: 1.4290  decode.d6.loss_mask: 0.8478  decode.d6.loss_dice: 0.5218  decode.d7.loss_cls: 1.4116  decode.d7.loss_mask: 0.8439  decode.d7.loss_dice: 0.5260  decode.d8.loss_cls: 1.4654  decode.d8.loss_mask: 0.8129  decode.d8.loss_dice: 0.5357
09/30 09:08:16 - mmengine - INFO - Iter(train) [   700/320000]  base_lr: 9.9803e-05 lr: 9.9803e-06  eta: 1 day, 15:38:38  time: 0.4299  data_time: 0.0084  memory: 5193  grad_norm: 195.2503  loss: 28.8802  decode.loss_cls: 1.5933  decode.loss_mask: 0.4092  decode.loss_dice: 0.3772  decode.d0.loss_cls: 5.8113  decode.d0.loss_mask: 0.4642  decode.d0.loss_dice: 0.5738  decode.d1.loss_cls: 1.6321  decode.d1.loss_mask: 0.3990  decode.d1.loss_dice: 0.4161  decode.d2.loss_cls: 1.5995  decode.d2.loss_mask: 0.4317  decode.d2.loss_dice: 0.3314  decode.d3.loss_cls: 1.6007  decode.d3.loss_mask: 0.4606  decode.d3.loss_dice: 0.3845  decode.d4.loss_cls: 1.6464  decode.d4.loss_mask: 0.4071  decode.d4.loss_dice: 0.3655  decode.d5.loss_cls: 1.6614  decode.d5.loss_mask: 0.4761  decode.d5.loss_dice: 0.4255  decode.d6.loss_cls: 1.6900  decode.d6.loss_mask: 0.3870  decode.d6.loss_dice: 0.3579  decode.d7.loss_cls: 1.7190  decode.d7.loss_mask: 0.4343  decode.d7.loss_dice: 0.3962  decode.d8.loss_cls: 1.6556  decode.d8.loss_mask: 0.4043  decode.d8.loss_dice: 0.3693
09/30 09:08:37 - mmengine - INFO - Iter(train) [   750/320000]  base_lr: 9.9789e-05 lr: 9.9789e-06  eta: 1 day, 15:32:30  time: 0.4303  data_time: 0.0087  memory: 5160  grad_norm: 263.0345  loss: 30.4525  decode.loss_cls: 1.5455  decode.loss_mask: 0.6236  decode.loss_dice: 0.4564  decode.d0.loss_cls: 5.6346  decode.d0.loss_mask: 0.6155  decode.d0.loss_dice: 0.5883  decode.d1.loss_cls: 1.4410  decode.d1.loss_mask: 0.7482  decode.d1.loss_dice: 0.5102  decode.d2.loss_cls: 1.3817  decode.d2.loss_mask: 0.7175  decode.d2.loss_dice: 0.5303  decode.d3.loss_cls: 1.4949  decode.d3.loss_mask: 0.6327  decode.d3.loss_dice: 0.4739  decode.d4.loss_cls: 1.4360  decode.d4.loss_mask: 0.6797  decode.d4.loss_dice: 0.5340  decode.d5.loss_cls: 1.4660  decode.d5.loss_mask: 0.6623  decode.d5.loss_dice: 0.4516  decode.d6.loss_cls: 1.4555  decode.d6.loss_mask: 0.6435  decode.d6.loss_dice: 0.4394  decode.d7.loss_cls: 1.5100  decode.d7.loss_mask: 0.6500  decode.d7.loss_dice: 0.4730  decode.d8.loss_cls: 1.5732  decode.d8.loss_mask: 0.6455  decode.d8.loss_dice: 0.4383
09/30 09:08:59 - mmengine - INFO - Iter(train) [   800/320000]  base_lr: 9.9775e-05 lr: 9.9775e-06  eta: 1 day, 15:26:42  time: 0.4296  data_time: 0.0084  memory: 5145  grad_norm: 222.1036  loss: 34.1816  decode.loss_cls: 1.7373  decode.loss_mask: 0.8118  decode.loss_dice: 0.6220  decode.d0.loss_cls: 5.4729  decode.d0.loss_mask: 0.7214  decode.d0.loss_dice: 0.6815  decode.d1.loss_cls: 1.5165  decode.d1.loss_mask: 0.8158  decode.d1.loss_dice: 0.6362  decode.d2.loss_cls: 1.6359  decode.d2.loss_mask: 0.7627  decode.d2.loss_dice: 0.6183  decode.d3.loss_cls: 1.6343  decode.d3.loss_mask: 0.7676  decode.d3.loss_dice: 0.5891  decode.d4.loss_cls: 1.5753  decode.d4.loss_mask: 0.8227  decode.d4.loss_dice: 0.5991  decode.d5.loss_cls: 1.6185  decode.d5.loss_mask: 0.7618  decode.d5.loss_dice: 0.5773  decode.d6.loss_cls: 1.6247  decode.d6.loss_mask: 0.8712  decode.d6.loss_dice: 0.6165  decode.d7.loss_cls: 1.7182  decode.d7.loss_mask: 0.7705  decode.d7.loss_dice: 0.6324  decode.d8.loss_cls: 1.6805  decode.d8.loss_mask: 0.6813  decode.d8.loss_dice: 0.6082
09/30 09:09:20 - mmengine - INFO - Iter(train) [   850/320000]  base_lr: 9.9761e-05 lr: 9.9761e-06  eta: 1 day, 15:21:44  time: 0.4310  data_time: 0.0085  memory: 5180  grad_norm: 239.7608  loss: 33.5182  decode.loss_cls: 1.7759  decode.loss_mask: 0.7228  decode.loss_dice: 0.5799  decode.d0.loss_cls: 5.2676  decode.d0.loss_mask: 0.7167  decode.d0.loss_dice: 0.6791  decode.d1.loss_cls: 1.8027  decode.d1.loss_mask: 0.6533  decode.d1.loss_dice: 0.5949  decode.d2.loss_cls: 1.8034  decode.d2.loss_mask: 0.6258  decode.d2.loss_dice: 0.5176  decode.d3.loss_cls: 1.7692  decode.d3.loss_mask: 0.6582  decode.d3.loss_dice: 0.5522  decode.d4.loss_cls: 1.6772  decode.d4.loss_mask: 0.6960  decode.d4.loss_dice: 0.5705  decode.d5.loss_cls: 1.7970  decode.d5.loss_mask: 0.6622  decode.d5.loss_dice: 0.5528  decode.d6.loss_cls: 1.7716  decode.d6.loss_mask: 0.6315  decode.d6.loss_dice: 0.5150  decode.d7.loss_cls: 1.7457  decode.d7.loss_mask: 0.6647  decode.d7.loss_dice: 0.5345  decode.d8.loss_cls: 1.7982  decode.d8.loss_mask: 0.6566  decode.d8.loss_dice: 0.5254
09/30 09:09:42 - mmengine - INFO - Iter(train) [   900/320000]  base_lr: 9.9747e-05 lr: 9.9747e-06  eta: 1 day, 15:17:18  time: 0.4299  data_time: 0.0085  memory: 5180  grad_norm: 334.9883  loss: 34.7798  decode.loss_cls: 1.8020  decode.loss_mask: 0.7372  decode.loss_dice: 0.6579  decode.d0.loss_cls: 5.0315  decode.d0.loss_mask: 0.7102  decode.d0.loss_dice: 0.8000  decode.d1.loss_cls: 1.8939  decode.d1.loss_mask: 0.5979  decode.d1.loss_dice: 0.6929  decode.d2.loss_cls: 1.8827  decode.d2.loss_mask: 0.5870  decode.d2.loss_dice: 0.6527  decode.d3.loss_cls: 1.8118  decode.d3.loss_mask: 0.5880  decode.d3.loss_dice: 0.6120  decode.d4.loss_cls: 1.7038  decode.d4.loss_mask: 0.7794  decode.d4.loss_dice: 0.6980  decode.d5.loss_cls: 1.7259  decode.d5.loss_mask: 0.7133  decode.d5.loss_dice: 0.6557  decode.d6.loss_cls: 1.7309  decode.d6.loss_mask: 0.7656  decode.d6.loss_dice: 0.6644  decode.d7.loss_cls: 1.7481  decode.d7.loss_mask: 0.7547  decode.d7.loss_dice: 0.6819  decode.d8.loss_cls: 1.7492  decode.d8.loss_mask: 0.7093  decode.d8.loss_dice: 0.6418
09/30 09:10:03 - mmengine - INFO - Iter(train) [   950/320000]  base_lr: 9.9733e-05 lr: 9.9733e-06  eta: 1 day, 15:13:19  time: 0.4306  data_time: 0.0088  memory: 5147  grad_norm: 214.8063  loss: 22.8621  decode.loss_cls: 1.1491  decode.loss_mask: 0.4362  decode.loss_dice: 0.3738  decode.d0.loss_cls: 4.7073  decode.d0.loss_mask: 0.4782  decode.d0.loss_dice: 0.4301  decode.d1.loss_cls: 1.0651  decode.d1.loss_mask: 0.4849  decode.d1.loss_dice: 0.4578  decode.d2.loss_cls: 1.0537  decode.d2.loss_mask: 0.4213  decode.d2.loss_dice: 0.4044  decode.d3.loss_cls: 1.0472  decode.d3.loss_mask: 0.4677  decode.d3.loss_dice: 0.3842  decode.d4.loss_cls: 1.1313  decode.d4.loss_mask: 0.4018  decode.d4.loss_dice: 0.3513  decode.d5.loss_cls: 1.0923  decode.d5.loss_mask: 0.3951  decode.d5.loss_dice: 0.3693  decode.d6.loss_cls: 1.0990  decode.d6.loss_mask: 0.4144  decode.d6.loss_dice: 0.3718  decode.d7.loss_cls: 1.0857  decode.d7.loss_mask: 0.4547  decode.d7.loss_dice: 0.3854  decode.d8.loss_cls: 1.0781  decode.d8.loss_mask: 0.4642  decode.d8.loss_dice: 0.4067
09/30 09:10:25 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 09:10:25 - mmengine - INFO - Iter(train) [  1000/320000]  base_lr: 9.9719e-05 lr: 9.9719e-06  eta: 1 day, 15:09:50  time: 0.4303  data_time: 0.0086  memory: 5161  grad_norm: 336.8786  loss: 28.1618  decode.loss_cls: 1.1706  decode.loss_mask: 0.6384  decode.loss_dice: 0.5050  decode.d0.loss_cls: 4.6388  decode.d0.loss_mask: 0.6560  decode.d0.loss_dice: 0.6066  decode.d1.loss_cls: 1.3915  decode.d1.loss_mask: 0.6107  decode.d1.loss_dice: 0.5162  decode.d2.loss_cls: 1.3522  decode.d2.loss_mask: 0.6604  decode.d2.loss_dice: 0.5263  decode.d3.loss_cls: 1.3049  decode.d3.loss_mask: 0.6641  decode.d3.loss_dice: 0.5167  decode.d4.loss_cls: 1.2725  decode.d4.loss_mask: 0.6467  decode.d4.loss_dice: 0.5157  decode.d5.loss_cls: 1.2180  decode.d5.loss_mask: 0.7515  decode.d5.loss_dice: 0.5440  decode.d6.loss_cls: 1.1618  decode.d6.loss_mask: 0.7853  decode.d6.loss_dice: 0.5493  decode.d7.loss_cls: 1.1884  decode.d7.loss_mask: 0.7277  decode.d7.loss_dice: 0.5184  decode.d8.loss_cls: 1.2414  decode.d8.loss_mask: 0.7504  decode.d8.loss_dice: 0.5322
09/30 09:10:46 - mmengine - INFO - Iter(train) [  1050/320000]  base_lr: 9.9705e-05 lr: 9.9705e-06  eta: 1 day, 15:06:21  time: 0.4294  data_time: 0.0083  memory: 5147  grad_norm: 130.2090  loss: 23.3844  decode.loss_cls: 1.3547  decode.loss_mask: 0.3082  decode.loss_dice: 0.3430  decode.d0.loss_cls: 4.4466  decode.d0.loss_mask: 0.2979  decode.d0.loss_dice: 0.4033  decode.d1.loss_cls: 1.4431  decode.d1.loss_mask: 0.2737  decode.d1.loss_dice: 0.3011  decode.d2.loss_cls: 1.4890  decode.d2.loss_mask: 0.2741  decode.d2.loss_dice: 0.3003  decode.d3.loss_cls: 1.4738  decode.d3.loss_mask: 0.2806  decode.d3.loss_dice: 0.2860  decode.d4.loss_cls: 1.4666  decode.d4.loss_mask: 0.2828  decode.d4.loss_dice: 0.2948  decode.d5.loss_cls: 1.4615  decode.d5.loss_mask: 0.2763  decode.d5.loss_dice: 0.3125  decode.d6.loss_cls: 1.4079  decode.d6.loss_mask: 0.2721  decode.d6.loss_dice: 0.3074  decode.d7.loss_cls: 1.4508  decode.d7.loss_mask: 0.2830  decode.d7.loss_dice: 0.2970  decode.d8.loss_cls: 1.4172  decode.d8.loss_mask: 0.2903  decode.d8.loss_dice: 0.2887
09/30 09:11:08 - mmengine - INFO - Iter(train) [  1100/320000]  base_lr: 9.9691e-05 lr: 9.9691e-06  eta: 1 day, 15:03:26  time: 0.4318  data_time: 0.0085  memory: 5180  grad_norm: 244.5338  loss: 22.1340  decode.loss_cls: 1.2458  decode.loss_mask: 0.3763  decode.loss_dice: 0.3222  decode.d0.loss_cls: 4.1368  decode.d0.loss_mask: 0.3752  decode.d0.loss_dice: 0.4519  decode.d1.loss_cls: 1.2553  decode.d1.loss_mask: 0.3603  decode.d1.loss_dice: 0.3242  decode.d2.loss_cls: 1.1924  decode.d2.loss_mask: 0.3661  decode.d2.loss_dice: 0.3380  decode.d3.loss_cls: 1.2040  decode.d3.loss_mask: 0.3592  decode.d3.loss_dice: 0.3273  decode.d4.loss_cls: 1.1963  decode.d4.loss_mask: 0.3683  decode.d4.loss_dice: 0.3141  decode.d5.loss_cls: 1.1928  decode.d5.loss_mask: 0.3545  decode.d5.loss_dice: 0.3399  decode.d6.loss_cls: 1.1893  decode.d6.loss_mask: 0.3732  decode.d6.loss_dice: 0.3389  decode.d7.loss_cls: 1.1881  decode.d7.loss_mask: 0.3640  decode.d7.loss_dice: 0.3324  decode.d8.loss_cls: 1.2288  decode.d8.loss_mask: 0.3781  decode.d8.loss_dice: 0.3404
09/30 09:11:29 - mmengine - INFO - Iter(train) [  1150/320000]  base_lr: 9.9677e-05 lr: 9.9677e-06  eta: 1 day, 15:00:32  time: 0.4305  data_time: 0.0087  memory: 5180  grad_norm: 159.8827  loss: 25.6956  decode.loss_cls: 1.3478  decode.loss_mask: 0.5075  decode.loss_dice: 0.4674  decode.d0.loss_cls: 3.9894  decode.d0.loss_mask: 0.4555  decode.d0.loss_dice: 0.5395  decode.d1.loss_cls: 1.3835  decode.d1.loss_mask: 0.4605  decode.d1.loss_dice: 0.4703  decode.d2.loss_cls: 1.4065  decode.d2.loss_mask: 0.4270  decode.d2.loss_dice: 0.4705  decode.d3.loss_cls: 1.4069  decode.d3.loss_mask: 0.4417  decode.d3.loss_dice: 0.4615  decode.d4.loss_cls: 1.4247  decode.d4.loss_mask: 0.4461  decode.d4.loss_dice: 0.4642  decode.d5.loss_cls: 1.3243  decode.d5.loss_mask: 0.4529  decode.d5.loss_dice: 0.4543  decode.d6.loss_cls: 1.4447  decode.d6.loss_mask: 0.4445  decode.d6.loss_dice: 0.4374  decode.d7.loss_cls: 1.3774  decode.d7.loss_mask: 0.4458  decode.d7.loss_dice: 0.4738  decode.d8.loss_cls: 1.3088  decode.d8.loss_mask: 0.4663  decode.d8.loss_dice: 0.4951
09/30 09:11:51 - mmengine - INFO - Iter(train) [  1200/320000]  base_lr: 9.9663e-05 lr: 9.9663e-06  eta: 1 day, 14:58:07  time: 0.4307  data_time: 0.0085  memory: 5161  grad_norm: 155.1869  loss: 26.2555  decode.loss_cls: 1.4181  decode.loss_mask: 0.5602  decode.loss_dice: 0.4664  decode.d0.loss_cls: 3.9348  decode.d0.loss_mask: 0.4865  decode.d0.loss_dice: 0.5092  decode.d1.loss_cls: 1.4474  decode.d1.loss_mask: 0.4602  decode.d1.loss_dice: 0.4517  decode.d2.loss_cls: 1.3228  decode.d2.loss_mask: 0.5203  decode.d2.loss_dice: 0.4639  decode.d3.loss_cls: 1.3107  decode.d3.loss_mask: 0.4862  decode.d3.loss_dice: 0.4922  decode.d4.loss_cls: 1.3901  decode.d4.loss_mask: 0.5229  decode.d4.loss_dice: 0.4469  decode.d5.loss_cls: 1.4325  decode.d5.loss_mask: 0.4966  decode.d5.loss_dice: 0.4866  decode.d6.loss_cls: 1.4228  decode.d6.loss_mask: 0.5299  decode.d6.loss_dice: 0.4316  decode.d7.loss_cls: 1.3555  decode.d7.loss_mask: 0.5476  decode.d7.loss_dice: 0.4609  decode.d8.loss_cls: 1.3552  decode.d8.loss_mask: 0.5630  decode.d8.loss_dice: 0.4828
09/30 09:12:12 - mmengine - INFO - Iter(train) [  1250/320000]  base_lr: 9.9649e-05 lr: 9.9649e-06  eta: 1 day, 14:56:35  time: 0.4304  data_time: 0.0085  memory: 5161  grad_norm: 174.7964  loss: 21.4766  decode.loss_cls: 1.0223  decode.loss_mask: 0.4931  decode.loss_dice: 0.3838  decode.d0.loss_cls: 3.5046  decode.d0.loss_mask: 0.5025  decode.d0.loss_dice: 0.4336  decode.d1.loss_cls: 1.1802  decode.d1.loss_mask: 0.4554  decode.d1.loss_dice: 0.3798  decode.d2.loss_cls: 1.1416  decode.d2.loss_mask: 0.4284  decode.d2.loss_dice: 0.3703  decode.d3.loss_cls: 1.1400  decode.d3.loss_mask: 0.4064  decode.d3.loss_dice: 0.3521  decode.d4.loss_cls: 1.1222  decode.d4.loss_mask: 0.4091  decode.d4.loss_dice: 0.3470  decode.d5.loss_cls: 1.0851  decode.d5.loss_mask: 0.3924  decode.d5.loss_dice: 0.3596  decode.d6.loss_cls: 1.0772  decode.d6.loss_mask: 0.4384  decode.d6.loss_dice: 0.3255  decode.d7.loss_cls: 1.0386  decode.d7.loss_mask: 0.4394  decode.d7.loss_dice: 0.3512  decode.d8.loss_cls: 1.0115  decode.d8.loss_mask: 0.4907  decode.d8.loss_dice: 0.3946
09/30 09:12:34 - mmengine - INFO - Iter(train) [  1300/320000]  base_lr: 9.9635e-05 lr: 9.9635e-06  eta: 1 day, 14:54:24  time: 0.4315  data_time: 0.0085  memory: 5160  grad_norm: 251.1737  loss: 23.1797  decode.loss_cls: 1.1840  decode.loss_mask: 0.4410  decode.loss_dice: 0.4181  decode.d0.loss_cls: 3.3655  decode.d0.loss_mask: 0.4311  decode.d0.loss_dice: 0.4943  decode.d1.loss_cls: 1.2617  decode.d1.loss_mask: 0.4342  decode.d1.loss_dice: 0.4353  decode.d2.loss_cls: 1.2541  decode.d2.loss_mask: 0.4703  decode.d2.loss_dice: 0.4108  decode.d3.loss_cls: 1.2281  decode.d3.loss_mask: 0.4898  decode.d3.loss_dice: 0.4189  decode.d4.loss_cls: 1.2369  decode.d4.loss_mask: 0.4829  decode.d4.loss_dice: 0.4122  decode.d5.loss_cls: 1.2462  decode.d5.loss_mask: 0.4359  decode.d5.loss_dice: 0.4084  decode.d6.loss_cls: 1.2028  decode.d6.loss_mask: 0.4493  decode.d6.loss_dice: 0.4109  decode.d7.loss_cls: 1.2167  decode.d7.loss_mask: 0.4578  decode.d7.loss_dice: 0.4429  decode.d8.loss_cls: 1.1986  decode.d8.loss_mask: 0.4429  decode.d8.loss_dice: 0.3982
09/30 09:12:56 - mmengine - INFO - Iter(train) [  1350/320000]  base_lr: 9.9621e-05 lr: 9.9621e-06  eta: 1 day, 14:52:25  time: 0.4323  data_time: 0.0089  memory: 5161  grad_norm: 234.0474  loss: 21.5285  decode.loss_cls: 1.0573  decode.loss_mask: 0.5094  decode.loss_dice: 0.3517  decode.d0.loss_cls: 3.1100  decode.d0.loss_mask: 0.5016  decode.d0.loss_dice: 0.4306  decode.d1.loss_cls: 1.1234  decode.d1.loss_mask: 0.4811  decode.d1.loss_dice: 0.3771  decode.d2.loss_cls: 1.1221  decode.d2.loss_mask: 0.4784  decode.d2.loss_dice: 0.3697  decode.d3.loss_cls: 1.0428  decode.d3.loss_mask: 0.4914  decode.d3.loss_dice: 0.3612  decode.d4.loss_cls: 1.0913  decode.d4.loss_mask: 0.4987  decode.d4.loss_dice: 0.3578  decode.d5.loss_cls: 1.0472  decode.d5.loss_mask: 0.5162  decode.d5.loss_dice: 0.3610  decode.d6.loss_cls: 1.1007  decode.d6.loss_mask: 0.5506  decode.d6.loss_dice: 0.3702  decode.d7.loss_cls: 1.0690  decode.d7.loss_mask: 0.5232  decode.d7.loss_dice: 0.3682  decode.d8.loss_cls: 1.0229  decode.d8.loss_mask: 0.4965  decode.d8.loss_dice: 0.3473
09/30 09:13:17 - mmengine - INFO - Iter(train) [  1400/320000]  base_lr: 9.9606e-05 lr: 9.9606e-06  eta: 1 day, 14:50:30  time: 0.4305  data_time: 0.0085  memory: 5145  grad_norm: 90.6035  loss: 18.9389  decode.loss_cls: 1.0073  decode.loss_mask: 0.2980  decode.loss_dice: 0.3355  decode.d0.loss_cls: 3.0657  decode.d0.loss_mask: 0.3022  decode.d0.loss_dice: 0.4212  decode.d1.loss_cls: 1.0742  decode.d1.loss_mask: 0.3032  decode.d1.loss_dice: 0.3689  decode.d2.loss_cls: 1.0875  decode.d2.loss_mask: 0.3038  decode.d2.loss_dice: 0.3392  decode.d3.loss_cls: 1.0612  decode.d3.loss_mask: 0.2934  decode.d3.loss_dice: 0.3391  decode.d4.loss_cls: 1.0534  decode.d4.loss_mask: 0.2951  decode.d4.loss_dice: 0.3161  decode.d5.loss_cls: 1.0381  decode.d5.loss_mask: 0.2925  decode.d5.loss_dice: 0.3433  decode.d6.loss_cls: 1.0386  decode.d6.loss_mask: 0.2918  decode.d6.loss_dice: 0.3327  decode.d7.loss_cls: 1.0464  decode.d7.loss_mask: 0.2909  decode.d7.loss_dice: 0.3362  decode.d8.loss_cls: 1.0382  decode.d8.loss_mask: 0.2941  decode.d8.loss_dice: 0.3313
09/30 09:13:39 - mmengine - INFO - Iter(train) [  1450/320000]  base_lr: 9.9592e-05 lr: 9.9592e-06  eta: 1 day, 14:48:37  time: 0.4316  data_time: 0.0089  memory: 5180  grad_norm: 218.8976  loss: 19.3898  decode.loss_cls: 0.8947  decode.loss_mask: 0.4152  decode.loss_dice: 0.3976  decode.d0.loss_cls: 2.6892  decode.d0.loss_mask: 0.4254  decode.d0.loss_dice: 0.4569  decode.d1.loss_cls: 1.0019  decode.d1.loss_mask: 0.4130  decode.d1.loss_dice: 0.4130  decode.d2.loss_cls: 0.9901  decode.d2.loss_mask: 0.4094  decode.d2.loss_dice: 0.3967  decode.d3.loss_cls: 0.9972  decode.d3.loss_mask: 0.4000  decode.d3.loss_dice: 0.3763  decode.d4.loss_cls: 0.8926  decode.d4.loss_mask: 0.4313  decode.d4.loss_dice: 0.3988  decode.d5.loss_cls: 0.9217  decode.d5.loss_mask: 0.4078  decode.d5.loss_dice: 0.3912  decode.d6.loss_cls: 0.9330  decode.d6.loss_mask: 0.4017  decode.d6.loss_dice: 0.4015  decode.d7.loss_cls: 0.9564  decode.d7.loss_mask: 0.4103  decode.d7.loss_dice: 0.4257  decode.d8.loss_cls: 0.9312  decode.d8.loss_mask: 0.4094  decode.d8.loss_dice: 0.4009
09/30 09:14:00 - mmengine - INFO - Iter(train) [  1500/320000]  base_lr: 9.9578e-05 lr: 9.9578e-06  eta: 1 day, 14:46:57  time: 0.4305  data_time: 0.0084  memory: 5161  grad_norm: 159.9188  loss: 19.6597  decode.loss_cls: 1.0474  decode.loss_mask: 0.3934  decode.loss_dice: 0.3462  decode.d0.loss_cls: 2.7881  decode.d0.loss_mask: 0.4303  decode.d0.loss_dice: 0.4083  decode.d1.loss_cls: 1.0800  decode.d1.loss_mask: 0.3833  decode.d1.loss_dice: 0.3489  decode.d2.loss_cls: 1.1072  decode.d2.loss_mask: 0.3719  decode.d2.loss_dice: 0.3239  decode.d3.loss_cls: 1.0343  decode.d3.loss_mask: 0.3897  decode.d3.loss_dice: 0.3221  decode.d4.loss_cls: 1.0265  decode.d4.loss_mask: 0.4120  decode.d4.loss_dice: 0.3733  decode.d5.loss_cls: 1.0579  decode.d5.loss_mask: 0.3853  decode.d5.loss_dice: 0.3277  decode.d6.loss_cls: 1.1028  decode.d6.loss_mask: 0.3752  decode.d6.loss_dice: 0.3335  decode.d7.loss_cls: 0.9981  decode.d7.loss_mask: 0.3987  decode.d7.loss_dice: 0.3287  decode.d8.loss_cls: 1.0455  decode.d8.loss_mask: 0.3804  decode.d8.loss_dice: 0.3387
09/30 09:14:22 - mmengine - INFO - Iter(train) [  1550/320000]  base_lr: 9.9564e-05 lr: 9.9564e-06  eta: 1 day, 14:45:19  time: 0.4319  data_time: 0.0089  memory: 5161  grad_norm: 148.5545  loss: 21.1039  decode.loss_cls: 1.0049  decode.loss_mask: 0.4892  decode.loss_dice: 0.4058  decode.d0.loss_cls: 2.5754  decode.d0.loss_mask: 0.5132  decode.d0.loss_dice: 0.4620  decode.d1.loss_cls: 1.2428  decode.d1.loss_mask: 0.4844  decode.d1.loss_dice: 0.4166  decode.d2.loss_cls: 1.0691  decode.d2.loss_mask: 0.4832  decode.d2.loss_dice: 0.3761  decode.d3.loss_cls: 1.0298  decode.d3.loss_mask: 0.5156  decode.d3.loss_dice: 0.3924  decode.d4.loss_cls: 1.0203  decode.d4.loss_mask: 0.5010  decode.d4.loss_dice: 0.3843  decode.d5.loss_cls: 1.0019  decode.d5.loss_mask: 0.5185  decode.d5.loss_dice: 0.3994  decode.d6.loss_cls: 1.0435  decode.d6.loss_mask: 0.5011  decode.d6.loss_dice: 0.3852  decode.d7.loss_cls: 1.0448  decode.d7.loss_mask: 0.4932  decode.d7.loss_dice: 0.4066  decode.d8.loss_cls: 1.0384  decode.d8.loss_mask: 0.4975  decode.d8.loss_dice: 0.4079
09/30 09:14:43 - mmengine - INFO - Iter(train) [  1600/320000]  base_lr: 9.9550e-05 lr: 9.9550e-06  eta: 1 day, 14:43:44  time: 0.4298  data_time: 0.0086  memory: 5180  grad_norm: 126.3025  loss: 17.7679  decode.loss_cls: 0.8088  decode.loss_mask: 0.4503  decode.loss_dice: 0.3900  decode.d0.loss_cls: 2.2760  decode.d0.loss_mask: 0.4363  decode.d0.loss_dice: 0.4153  decode.d1.loss_cls: 0.8837  decode.d1.loss_mask: 0.4323  decode.d1.loss_dice: 0.3820  decode.d2.loss_cls: 0.8830  decode.d2.loss_mask: 0.4524  decode.d2.loss_dice: 0.3691  decode.d3.loss_cls: 0.8252  decode.d3.loss_mask: 0.4647  decode.d3.loss_dice: 0.3526  decode.d4.loss_cls: 0.7700  decode.d4.loss_mask: 0.4498  decode.d4.loss_dice: 0.3672  decode.d5.loss_cls: 0.7364  decode.d5.loss_mask: 0.4670  decode.d5.loss_dice: 0.3726  decode.d6.loss_cls: 0.7504  decode.d6.loss_mask: 0.4593  decode.d6.loss_dice: 0.3767  decode.d7.loss_cls: 0.7654  decode.d7.loss_mask: 0.4467  decode.d7.loss_dice: 0.3716  decode.d8.loss_cls: 0.7817  decode.d8.loss_mask: 0.4544  decode.d8.loss_dice: 0.3771
09/30 09:15:05 - mmengine - INFO - Iter(train) [  1650/320000]  base_lr: 9.9536e-05 lr: 9.9536e-06  eta: 1 day, 14:42:12  time: 0.4317  data_time: 0.0088  memory: 5160  grad_norm: 188.6059  loss: 17.3146  decode.loss_cls: 0.7909  decode.loss_mask: 0.4587  decode.loss_dice: 0.3614  decode.d0.loss_cls: 2.3173  decode.d0.loss_mask: 0.4395  decode.d0.loss_dice: 0.4032  decode.d1.loss_cls: 0.8942  decode.d1.loss_mask: 0.4106  decode.d1.loss_dice: 0.3224  decode.d2.loss_cls: 0.8043  decode.d2.loss_mask: 0.4062  decode.d2.loss_dice: 0.3140  decode.d3.loss_cls: 0.7773  decode.d3.loss_mask: 0.4175  decode.d3.loss_dice: 0.3175  decode.d4.loss_cls: 0.7868  decode.d4.loss_mask: 0.4094  decode.d4.loss_dice: 0.3295  decode.d5.loss_cls: 0.7539  decode.d5.loss_mask: 0.4210  decode.d5.loss_dice: 0.3338  decode.d6.loss_cls: 0.8077  decode.d6.loss_mask: 0.4123  decode.d6.loss_dice: 0.3441  decode.d7.loss_cls: 0.8296  decode.d7.loss_mask: 0.4240  decode.d7.loss_dice: 0.3497  decode.d8.loss_cls: 0.7916  decode.d8.loss_mask: 0.4800  decode.d8.loss_dice: 0.4064
09/30 09:15:26 - mmengine - INFO - Iter(train) [  1700/320000]  base_lr: 9.9522e-05 lr: 9.9522e-06  eta: 1 day, 14:40:49  time: 0.4307  data_time: 0.0086  memory: 5160  grad_norm: 157.8234  loss: 14.9281  decode.loss_cls: 0.7987  decode.loss_mask: 0.2873  decode.loss_dice: 0.2143  decode.d0.loss_cls: 2.2569  decode.d0.loss_mask: 0.2809  decode.d0.loss_dice: 0.2713  decode.d1.loss_cls: 0.9191  decode.d1.loss_mask: 0.2925  decode.d1.loss_dice: 0.2206  decode.d2.loss_cls: 0.8630  decode.d2.loss_mask: 0.2896  decode.d2.loss_dice: 0.2151  decode.d3.loss_cls: 0.8694  decode.d3.loss_mask: 0.2821  decode.d3.loss_dice: 0.2051  decode.d4.loss_cls: 0.8475  decode.d4.loss_mask: 0.2978  decode.d4.loss_dice: 0.2113  decode.d5.loss_cls: 0.8314  decode.d5.loss_mask: 0.2898  decode.d5.loss_dice: 0.2090  decode.d6.loss_cls: 0.8190  decode.d6.loss_mask: 0.2904  decode.d6.loss_dice: 0.2103  decode.d7.loss_cls: 0.8598  decode.d7.loss_mask: 0.2876  decode.d7.loss_dice: 0.2070  decode.d8.loss_cls: 0.7821  decode.d8.loss_mask: 0.2933  decode.d8.loss_dice: 0.2260
09/30 09:15:48 - mmengine - INFO - Iter(train) [  1750/320000]  base_lr: 9.9508e-05 lr: 9.9508e-06  eta: 1 day, 14:39:29  time: 0.4323  data_time: 0.0089  memory: 5161  grad_norm: 270.4349  loss: 18.5486  decode.loss_cls: 0.9499  decode.loss_mask: 0.4457  decode.loss_dice: 0.3576  decode.d0.loss_cls: 2.0787  decode.d0.loss_mask: 0.4245  decode.d0.loss_dice: 0.3706  decode.d1.loss_cls: 0.9982  decode.d1.loss_mask: 0.4016  decode.d1.loss_dice: 0.3534  decode.d2.loss_cls: 0.9423  decode.d2.loss_mask: 0.3929  decode.d2.loss_dice: 0.3451  decode.d3.loss_cls: 0.9562  decode.d3.loss_mask: 0.4091  decode.d3.loss_dice: 0.3795  decode.d4.loss_cls: 1.0016  decode.d4.loss_mask: 0.4197  decode.d4.loss_dice: 0.3618  decode.d5.loss_cls: 1.0154  decode.d5.loss_mask: 0.3777  decode.d5.loss_dice: 0.3711  decode.d6.loss_cls: 0.9416  decode.d6.loss_mask: 0.4249  decode.d6.loss_dice: 0.3678  decode.d7.loss_cls: 0.9415  decode.d7.loss_mask: 0.4204  decode.d7.loss_dice: 0.3661  decode.d8.loss_cls: 0.9441  decode.d8.loss_mask: 0.4323  decode.d8.loss_dice: 0.3573
09/30 09:16:09 - mmengine - INFO - Iter(train) [  1800/320000]  base_lr: 9.9494e-05 lr: 9.9494e-06  eta: 1 day, 14:38:10  time: 0.4308  data_time: 0.0087  memory: 5160  grad_norm: 181.4968  loss: 21.3530  decode.loss_cls: 1.0269  decode.loss_mask: 0.4662  decode.loss_dice: 0.4339  decode.d0.loss_cls: 2.1027  decode.d0.loss_mask: 0.4902  decode.d0.loss_dice: 0.5710  decode.d1.loss_cls: 1.2998  decode.d1.loss_mask: 0.4326  decode.d1.loss_dice: 0.4443  decode.d2.loss_cls: 1.1526  decode.d2.loss_mask: 0.4577  decode.d2.loss_dice: 0.4632  decode.d3.loss_cls: 1.0802  decode.d3.loss_mask: 0.4992  decode.d3.loss_dice: 0.4392  decode.d4.loss_cls: 1.0070  decode.d4.loss_mask: 0.5065  decode.d4.loss_dice: 0.4559  decode.d5.loss_cls: 1.0764  decode.d5.loss_mask: 0.4727  decode.d5.loss_dice: 0.4286  decode.d6.loss_cls: 1.1270  decode.d6.loss_mask: 0.4712  decode.d6.loss_dice: 0.4623  decode.d7.loss_cls: 1.0655  decode.d7.loss_mask: 0.5055  decode.d7.loss_dice: 0.4467  decode.d8.loss_cls: 1.0351  decode.d8.loss_mask: 0.4605  decode.d8.loss_dice: 0.4721
09/30 09:16:31 - mmengine - INFO - Iter(train) [  1850/320000]  base_lr: 9.9480e-05 lr: 9.9480e-06  eta: 1 day, 14:37:01  time: 0.4317  data_time: 0.0087  memory: 5161  grad_norm: 219.4772  loss: 22.5704  decode.loss_cls: 1.0285  decode.loss_mask: 0.5765  decode.loss_dice: 0.5572  decode.d0.loss_cls: 1.9966  decode.d0.loss_mask: 0.5705  decode.d0.loss_dice: 0.5577  decode.d1.loss_cls: 1.0093  decode.d1.loss_mask: 0.5501  decode.d1.loss_dice: 0.5750  decode.d2.loss_cls: 0.9736  decode.d2.loss_mask: 0.5491  decode.d2.loss_dice: 0.6327  decode.d3.loss_cls: 1.0136  decode.d3.loss_mask: 0.5108  decode.d3.loss_dice: 0.5236  decode.d4.loss_cls: 1.0739  decode.d4.loss_mask: 0.5019  decode.d4.loss_dice: 0.5631  decode.d5.loss_cls: 1.0561  decode.d5.loss_mask: 0.5409  decode.d5.loss_dice: 0.5414  decode.d6.loss_cls: 1.1098  decode.d6.loss_mask: 0.5593  decode.d6.loss_dice: 0.5620  decode.d7.loss_cls: 1.0015  decode.d7.loss_mask: 0.6119  decode.d7.loss_dice: 0.5873  decode.d8.loss_cls: 1.0762  decode.d8.loss_mask: 0.5923  decode.d8.loss_dice: 0.5676
09/30 09:16:53 - mmengine - INFO - Iter(train) [  1900/320000]  base_lr: 9.9466e-05 lr: 9.9466e-06  eta: 1 day, 14:35:44  time: 0.4310  data_time: 0.0087  memory: 5147  grad_norm: 223.1594  loss: 20.2108  decode.loss_cls: 0.9577  decode.loss_mask: 0.4589  decode.loss_dice: 0.5090  decode.d0.loss_cls: 1.8702  decode.d0.loss_mask: 0.4691  decode.d0.loss_dice: 0.4704  decode.d1.loss_cls: 1.1243  decode.d1.loss_mask: 0.4687  decode.d1.loss_dice: 0.4321  decode.d2.loss_cls: 0.9590  decode.d2.loss_mask: 0.5269  decode.d2.loss_dice: 0.4361  decode.d3.loss_cls: 0.9333  decode.d3.loss_mask: 0.4750  decode.d3.loss_dice: 0.4486  decode.d4.loss_cls: 0.9542  decode.d4.loss_mask: 0.5130  decode.d4.loss_dice: 0.4810  decode.d5.loss_cls: 0.9379  decode.d5.loss_mask: 0.4799  decode.d5.loss_dice: 0.4415  decode.d6.loss_cls: 0.9376  decode.d6.loss_mask: 0.5213  decode.d6.loss_dice: 0.4440  decode.d7.loss_cls: 0.9815  decode.d7.loss_mask: 0.4962  decode.d7.loss_dice: 0.4784  decode.d8.loss_cls: 0.9931  decode.d8.loss_mask: 0.5009  decode.d8.loss_dice: 0.5111
09/30 09:17:14 - mmengine - INFO - Iter(train) [  1950/320000]  base_lr: 9.9452e-05 lr: 9.9452e-06  eta: 1 day, 14:34:36  time: 0.4307  data_time: 0.0085  memory: 5147  grad_norm: 148.4679  loss: 14.7008  decode.loss_cls: 0.6006  decode.loss_mask: 0.4277  decode.loss_dice: 0.3745  decode.d0.loss_cls: 1.7085  decode.d0.loss_mask: 0.3254  decode.d0.loss_dice: 0.3405  decode.d1.loss_cls: 0.8238  decode.d1.loss_mask: 0.3050  decode.d1.loss_dice: 0.3303  decode.d2.loss_cls: 0.6686  decode.d2.loss_mask: 0.3112  decode.d2.loss_dice: 0.3621  decode.d3.loss_cls: 0.6425  decode.d3.loss_mask: 0.3412  decode.d3.loss_dice: 0.3731  decode.d4.loss_cls: 0.5893  decode.d4.loss_mask: 0.3315  decode.d4.loss_dice: 0.3810  decode.d5.loss_cls: 0.5502  decode.d5.loss_mask: 0.4299  decode.d5.loss_dice: 0.3770  decode.d6.loss_cls: 0.6081  decode.d6.loss_mask: 0.3838  decode.d6.loss_dice: 0.3796  decode.d7.loss_cls: 0.5501  decode.d7.loss_mask: 0.4192  decode.d7.loss_dice: 0.3976  decode.d8.loss_cls: 0.5847  decode.d8.loss_mask: 0.3902  decode.d8.loss_dice: 0.3935
09/30 09:17:36 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 09:17:36 - mmengine - INFO - Iter(train) [  2000/320000]  base_lr: 9.9438e-05 lr: 9.9438e-06  eta: 1 day, 14:33:29  time: 0.4322  data_time: 0.0088  memory: 5180  grad_norm: 147.8422  loss: 17.9068  decode.loss_cls: 0.8934  decode.loss_mask: 0.3821  decode.loss_dice: 0.3914  decode.d0.loss_cls: 1.8384  decode.d0.loss_mask: 0.4155  decode.d0.loss_dice: 0.4464  decode.d1.loss_cls: 1.1023  decode.d1.loss_mask: 0.3822  decode.d1.loss_dice: 0.4057  decode.d2.loss_cls: 0.9430  decode.d2.loss_mask: 0.3876  decode.d2.loss_dice: 0.4069  decode.d3.loss_cls: 0.9029  decode.d3.loss_mask: 0.3767  decode.d3.loss_dice: 0.3919  decode.d4.loss_cls: 0.8856  decode.d4.loss_mask: 0.3707  decode.d4.loss_dice: 0.3769  decode.d5.loss_cls: 0.8558  decode.d5.loss_mask: 0.3726  decode.d5.loss_dice: 0.3887  decode.d6.loss_cls: 0.8810  decode.d6.loss_mask: 0.3714  decode.d6.loss_dice: 0.3834  decode.d7.loss_cls: 0.9170  decode.d7.loss_mask: 0.3665  decode.d7.loss_dice: 0.3938  decode.d8.loss_cls: 0.9057  decode.d8.loss_mask: 0.3698  decode.d8.loss_dice: 0.4013
09/30 09:17:57 - mmengine - INFO - Iter(train) [  2050/320000]  base_lr: 9.9424e-05 lr: 9.9424e-06  eta: 1 day, 14:32:22  time: 0.4307  data_time: 0.0085  memory: 5145  grad_norm: 164.1092  loss: 15.0943  decode.loss_cls: 0.6826  decode.loss_mask: 0.3982  decode.loss_dice: 0.3368  decode.d0.loss_cls: 1.6895  decode.d0.loss_mask: 0.3861  decode.d0.loss_dice: 0.3678  decode.d1.loss_cls: 0.8001  decode.d1.loss_mask: 0.3791  decode.d1.loss_dice: 0.3224  decode.d2.loss_cls: 0.6949  decode.d2.loss_mask: 0.3744  decode.d2.loss_dice: 0.3319  decode.d3.loss_cls: 0.7346  decode.d3.loss_mask: 0.4111  decode.d3.loss_dice: 0.3238  decode.d4.loss_cls: 0.7341  decode.d4.loss_mask: 0.3661  decode.d4.loss_dice: 0.3245  decode.d5.loss_cls: 0.6950  decode.d5.loss_mask: 0.3752  decode.d5.loss_dice: 0.3108  decode.d6.loss_cls: 0.6530  decode.d6.loss_mask: 0.3778  decode.d6.loss_dice: 0.3183  decode.d7.loss_cls: 0.6299  decode.d7.loss_mask: 0.3840  decode.d7.loss_dice: 0.3275  decode.d8.loss_cls: 0.6543  decode.d8.loss_mask: 0.3861  decode.d8.loss_dice: 0.3246
09/30 09:18:19 - mmengine - INFO - Iter(train) [  2100/320000]  base_lr: 9.9409e-05 lr: 9.9409e-06  eta: 1 day, 14:31:18  time: 0.4309  data_time: 0.0088  memory: 5161  grad_norm: 392.2454  loss: 25.3525  decode.loss_cls: 0.9925  decode.loss_mask: 0.7570  decode.loss_dice: 0.5419  decode.d0.loss_cls: 1.8571  decode.d0.loss_mask: 0.7220  decode.d0.loss_dice: 0.6087  decode.d1.loss_cls: 1.2451  decode.d1.loss_mask: 0.7441  decode.d1.loss_dice: 0.5832  decode.d2.loss_cls: 1.2151  decode.d2.loss_mask: 0.6988  decode.d2.loss_dice: 0.5184  decode.d3.loss_cls: 1.1494  decode.d3.loss_mask: 0.7838  decode.d3.loss_dice: 0.5812  decode.d4.loss_cls: 1.1533  decode.d4.loss_mask: 0.7962  decode.d4.loss_dice: 0.5570  decode.d5.loss_cls: 1.0854  decode.d5.loss_mask: 0.7322  decode.d5.loss_dice: 0.5284  decode.d6.loss_cls: 1.1447  decode.d6.loss_mask: 0.7561  decode.d6.loss_dice: 0.5446  decode.d7.loss_cls: 1.1895  decode.d7.loss_mask: 0.7981  decode.d7.loss_dice: 0.5908  decode.d8.loss_cls: 1.1045  decode.d8.loss_mask: 0.7979  decode.d8.loss_dice: 0.5757
09/30 09:18:40 - mmengine - INFO - Iter(train) [  2150/320000]  base_lr: 9.9395e-05 lr: 9.9395e-06  eta: 1 day, 14:30:17  time: 0.4296  data_time: 0.0091  memory: 5161  grad_norm: 289.5546  loss: 17.1674  decode.loss_cls: 0.7574  decode.loss_mask: 0.4786  decode.loss_dice: 0.3703  decode.d0.loss_cls: 1.5208  decode.d0.loss_mask: 0.4819  decode.d0.loss_dice: 0.4565  decode.d1.loss_cls: 0.8529  decode.d1.loss_mask: 0.5198  decode.d1.loss_dice: 0.4046  decode.d2.loss_cls: 0.8071  decode.d2.loss_mask: 0.5042  decode.d2.loss_dice: 0.3887  decode.d3.loss_cls: 0.7200  decode.d3.loss_mask: 0.4748  decode.d3.loss_dice: 0.3611  decode.d4.loss_cls: 0.7578  decode.d4.loss_mask: 0.4937  decode.d4.loss_dice: 0.3493  decode.d5.loss_cls: 0.6985  decode.d5.loss_mask: 0.5023  decode.d5.loss_dice: 0.4000  decode.d6.loss_cls: 0.7039  decode.d6.loss_mask: 0.4778  decode.d6.loss_dice: 0.3853  decode.d7.loss_cls: 0.7305  decode.d7.loss_mask: 0.4906  decode.d7.loss_dice: 0.3981  decode.d8.loss_cls: 0.7970  decode.d8.loss_mask: 0.4873  decode.d8.loss_dice: 0.3968
09/30 09:19:02 - mmengine - INFO - Iter(train) [  2200/320000]  base_lr: 9.9381e-05 lr: 9.9381e-06  eta: 1 day, 14:29:16  time: 0.4307  data_time: 0.0087  memory: 5180  grad_norm: 163.6394  loss: 21.9664  decode.loss_cls: 1.1622  decode.loss_mask: 0.4620  decode.loss_dice: 0.4965  decode.d0.loss_cls: 1.8580  decode.d0.loss_mask: 0.4193  decode.d0.loss_dice: 0.6089  decode.d1.loss_cls: 1.4065  decode.d1.loss_mask: 0.3652  decode.d1.loss_dice: 0.4887  decode.d2.loss_cls: 1.2270  decode.d2.loss_mask: 0.3449  decode.d2.loss_dice: 0.4656  decode.d3.loss_cls: 1.2129  decode.d3.loss_mask: 0.3454  decode.d3.loss_dice: 0.4716  decode.d4.loss_cls: 1.1856  decode.d4.loss_mask: 0.4266  decode.d4.loss_dice: 0.4823  decode.d5.loss_cls: 1.1919  decode.d5.loss_mask: 0.4385  decode.d5.loss_dice: 0.5234  decode.d6.loss_cls: 1.1976  decode.d6.loss_mask: 0.4543  decode.d6.loss_dice: 0.4991  decode.d7.loss_cls: 1.0976  decode.d7.loss_mask: 0.5052  decode.d7.loss_dice: 0.5037  decode.d8.loss_cls: 1.1720  decode.d8.loss_mask: 0.4422  decode.d8.loss_dice: 0.5118
09/30 09:19:23 - mmengine - INFO - Iter(train) [  2250/320000]  base_lr: 9.9367e-05 lr: 9.9367e-06  eta: 1 day, 14:28:16  time: 0.4313  data_time: 0.0090  memory: 5161  grad_norm: 192.5000  loss: 16.6393  decode.loss_cls: 0.7825  decode.loss_mask: 0.3988  decode.loss_dice: 0.3263  decode.d0.loss_cls: 1.6377  decode.d0.loss_mask: 0.4043  decode.d0.loss_dice: 0.3383  decode.d1.loss_cls: 0.8570  decode.d1.loss_mask: 0.4086  decode.d1.loss_dice: 0.3066  decode.d2.loss_cls: 0.8779  decode.d2.loss_mask: 0.3765  decode.d2.loss_dice: 0.3073  decode.d3.loss_cls: 0.8741  decode.d3.loss_mask: 0.3930  decode.d3.loss_dice: 0.3062  decode.d4.loss_cls: 0.8837  decode.d4.loss_mask: 0.4705  decode.d4.loss_dice: 0.3309  decode.d5.loss_cls: 0.8693  decode.d5.loss_mask: 0.4422  decode.d5.loss_dice: 0.3112  decode.d6.loss_cls: 0.8531  decode.d6.loss_mask: 0.4577  decode.d6.loss_dice: 0.3183  decode.d7.loss_cls: 0.8388  decode.d7.loss_mask: 0.3832  decode.d7.loss_dice: 0.3306  decode.d8.loss_cls: 0.8707  decode.d8.loss_mask: 0.3672  decode.d8.loss_dice: 0.3170
09/30 09:19:45 - mmengine - INFO - Iter(train) [  2300/320000]  base_lr: 9.9353e-05 lr: 9.9353e-06  eta: 1 day, 14:27:20  time: 0.4307  data_time: 0.0089  memory: 5147  grad_norm: 107.9415  loss: 13.7383  decode.loss_cls: 0.6514  decode.loss_mask: 0.2714  decode.loss_dice: 0.2937  decode.d0.loss_cls: 1.5390  decode.d0.loss_mask: 0.2735  decode.d0.loss_dice: 0.3445  decode.d1.loss_cls: 0.8448  decode.d1.loss_mask: 0.2723  decode.d1.loss_dice: 0.3025  decode.d2.loss_cls: 0.6990  decode.d2.loss_mask: 0.2712  decode.d2.loss_dice: 0.2968  decode.d3.loss_cls: 0.6459  decode.d3.loss_mask: 0.2739  decode.d3.loss_dice: 0.3155  decode.d4.loss_cls: 0.7535  decode.d4.loss_mask: 0.2768  decode.d4.loss_dice: 0.3218  decode.d5.loss_cls: 0.6632  decode.d5.loss_mask: 0.2792  decode.d5.loss_dice: 0.3044  decode.d6.loss_cls: 0.6750  decode.d6.loss_mask: 0.3028  decode.d6.loss_dice: 0.3242  decode.d7.loss_cls: 0.6949  decode.d7.loss_mask: 0.2746  decode.d7.loss_dice: 0.2985  decode.d8.loss_cls: 0.7008  decode.d8.loss_mask: 0.2810  decode.d8.loss_dice: 0.2924
09/30 09:20:06 - mmengine - INFO - Iter(train) [  2350/320000]  base_lr: 9.9339e-05 lr: 9.9339e-06  eta: 1 day, 14:26:25  time: 0.4307  data_time: 0.0087  memory: 5161  grad_norm: 78.7807  loss: 14.1038  decode.loss_cls: 0.8546  decode.loss_mask: 0.2434  decode.loss_dice: 0.2269  decode.d0.loss_cls: 1.6931  decode.d0.loss_mask: 0.2513  decode.d0.loss_dice: 0.2660  decode.d1.loss_cls: 0.8760  decode.d1.loss_mask: 0.2455  decode.d1.loss_dice: 0.2557  decode.d2.loss_cls: 0.8764  decode.d2.loss_mask: 0.2508  decode.d2.loss_dice: 0.2338  decode.d3.loss_cls: 0.8637  decode.d3.loss_mask: 0.2420  decode.d3.loss_dice: 0.2422  decode.d4.loss_cls: 0.8457  decode.d4.loss_mask: 0.2747  decode.d4.loss_dice: 0.2554  decode.d5.loss_cls: 0.8225  decode.d5.loss_mask: 0.2484  decode.d5.loss_dice: 0.2413  decode.d6.loss_cls: 0.8087  decode.d6.loss_mask: 0.2462  decode.d6.loss_dice: 0.2332  decode.d7.loss_cls: 0.7695  decode.d7.loss_mask: 0.2441  decode.d7.loss_dice: 0.2256  decode.d8.loss_cls: 0.7969  decode.d8.loss_mask: 0.2454  decode.d8.loss_dice: 0.2245
09/30 09:20:28 - mmengine - INFO - Iter(train) [  2400/320000]  base_lr: 9.9325e-05 lr: 9.9325e-06  eta: 1 day, 14:25:32  time: 0.4307  data_time: 0.0091  memory: 5180  grad_norm: 310.1958  loss: 14.8891  decode.loss_cls: 0.6889  decode.loss_mask: 0.3366  decode.loss_dice: 0.3254  decode.d0.loss_cls: 1.4440  decode.d0.loss_mask: 0.3698  decode.d0.loss_dice: 0.2879  decode.d1.loss_cls: 0.8331  decode.d1.loss_mask: 0.3948  decode.d1.loss_dice: 0.2943  decode.d2.loss_cls: 0.7285  decode.d2.loss_mask: 0.4068  decode.d2.loss_dice: 0.3019  decode.d3.loss_cls: 0.7383  decode.d3.loss_mask: 0.3657  decode.d3.loss_dice: 0.2696  decode.d4.loss_cls: 0.6975  decode.d4.loss_mask: 0.3698  decode.d4.loss_dice: 0.2863  decode.d5.loss_cls: 0.7763  decode.d5.loss_mask: 0.3767  decode.d5.loss_dice: 0.2852  decode.d6.loss_cls: 0.7545  decode.d6.loss_mask: 0.3925  decode.d6.loss_dice: 0.3019  decode.d7.loss_cls: 0.7124  decode.d7.loss_mask: 0.3784  decode.d7.loss_dice: 0.3063  decode.d8.loss_cls: 0.7426  decode.d8.loss_mask: 0.4131  decode.d8.loss_dice: 0.3102
09/30 09:20:50 - mmengine - INFO - Iter(train) [  2450/320000]  base_lr: 9.9311e-05 lr: 9.9311e-06  eta: 1 day, 14:24:42  time: 0.4299  data_time: 0.0087  memory: 5161  grad_norm: 253.5415  loss: 18.5303  decode.loss_cls: 0.8192  decode.loss_mask: 0.5399  decode.loss_dice: 0.3993  decode.d0.loss_cls: 1.6067  decode.d0.loss_mask: 0.4953  decode.d0.loss_dice: 0.3796  decode.d1.loss_cls: 1.1049  decode.d1.loss_mask: 0.4695  decode.d1.loss_dice: 0.3315  decode.d2.loss_cls: 0.8736  decode.d2.loss_mask: 0.4773  decode.d2.loss_dice: 0.3663  decode.d3.loss_cls: 0.8369  decode.d3.loss_mask: 0.5731  decode.d3.loss_dice: 0.4099  decode.d4.loss_cls: 0.8536  decode.d4.loss_mask: 0.6039  decode.d4.loss_dice: 0.3829  decode.d5.loss_cls: 0.8017  decode.d5.loss_mask: 0.5155  decode.d5.loss_dice: 0.3679  decode.d6.loss_cls: 0.8278  decode.d6.loss_mask: 0.5356  decode.d6.loss_dice: 0.3976  decode.d7.loss_cls: 0.8030  decode.d7.loss_mask: 0.5603  decode.d7.loss_dice: 0.3906  decode.d8.loss_cls: 0.8249  decode.d8.loss_mask: 0.5717  decode.d8.loss_dice: 0.4105
09/30 09:21:11 - mmengine - INFO - Iter(train) [  2500/320000]  base_lr: 9.9297e-05 lr: 9.9297e-06  eta: 1 day, 14:23:50  time: 0.4314  data_time: 0.0087  memory: 5147  grad_norm: 172.8762  loss: 21.3115  decode.loss_cls: 1.2002  decode.loss_mask: 0.4731  decode.loss_dice: 0.4788  decode.d0.loss_cls: 1.7573  decode.d0.loss_mask: 0.4109  decode.d0.loss_dice: 0.5305  decode.d1.loss_cls: 1.2263  decode.d1.loss_mask: 0.3859  decode.d1.loss_dice: 0.4363  decode.d2.loss_cls: 1.1340  decode.d2.loss_mask: 0.4221  decode.d2.loss_dice: 0.4639  decode.d3.loss_cls: 1.1946  decode.d3.loss_mask: 0.3918  decode.d3.loss_dice: 0.4578  decode.d4.loss_cls: 1.1971  decode.d4.loss_mask: 0.4046  decode.d4.loss_dice: 0.4638  decode.d5.loss_cls: 1.0516  decode.d5.loss_mask: 0.4724  decode.d5.loss_dice: 0.4531  decode.d6.loss_cls: 1.1244  decode.d6.loss_mask: 0.4418  decode.d6.loss_dice: 0.4744  decode.d7.loss_cls: 1.1989  decode.d7.loss_mask: 0.4400  decode.d7.loss_dice: 0.4738  decode.d8.loss_cls: 1.1261  decode.d8.loss_mask: 0.5360  decode.d8.loss_dice: 0.4900
09/30 09:21:33 - mmengine - INFO - Iter(train) [  2550/320000]  base_lr: 9.9283e-05 lr: 9.9283e-06  eta: 1 day, 14:22:59  time: 0.4308  data_time: 0.0088  memory: 5161  grad_norm: 222.2485  loss: 14.4759  decode.loss_cls: 0.7100  decode.loss_mask: 0.4840  decode.loss_dice: 0.2912  decode.d0.loss_cls: 1.4497  decode.d0.loss_mask: 0.3314  decode.d0.loss_dice: 0.3059  decode.d1.loss_cls: 0.7777  decode.d1.loss_mask: 0.3467  decode.d1.loss_dice: 0.2571  decode.d2.loss_cls: 0.6628  decode.d2.loss_mask: 0.4033  decode.d2.loss_dice: 0.2719  decode.d3.loss_cls: 0.6513  decode.d3.loss_mask: 0.4334  decode.d3.loss_dice: 0.2479  decode.d4.loss_cls: 0.6680  decode.d4.loss_mask: 0.4160  decode.d4.loss_dice: 0.2405  decode.d5.loss_cls: 0.7110  decode.d5.loss_mask: 0.4140  decode.d5.loss_dice: 0.2472  decode.d6.loss_cls: 0.6880  decode.d6.loss_mask: 0.3585  decode.d6.loss_dice: 0.2838  decode.d7.loss_cls: 0.7677  decode.d7.loss_mask: 0.3715  decode.d7.loss_dice: 0.2836  decode.d8.loss_cls: 0.7383  decode.d8.loss_mask: 0.3606  decode.d8.loss_dice: 0.3026
09/30 09:21:54 - mmengine - INFO - Iter(train) [  2600/320000]  base_lr: 9.9269e-05 lr: 9.9269e-06  eta: 1 day, 14:22:13  time: 0.4305  data_time: 0.0087  memory: 5180  grad_norm: 340.7953  loss: 13.8363  decode.loss_cls: 0.6122  decode.loss_mask: 0.4202  decode.loss_dice: 0.2554  decode.d0.loss_cls: 1.3756  decode.d0.loss_mask: 0.4284  decode.d0.loss_dice: 0.2596  decode.d1.loss_cls: 0.7351  decode.d1.loss_mask: 0.4290  decode.d1.loss_dice: 0.2530  decode.d2.loss_cls: 0.6379  decode.d2.loss_mask: 0.4273  decode.d2.loss_dice: 0.2670  decode.d3.loss_cls: 0.5967  decode.d3.loss_mask: 0.4223  decode.d3.loss_dice: 0.2694  decode.d4.loss_cls: 0.6068  decode.d4.loss_mask: 0.4072  decode.d4.loss_dice: 0.2554  decode.d5.loss_cls: 0.6302  decode.d5.loss_mask: 0.4203  decode.d5.loss_dice: 0.2567  decode.d6.loss_cls: 0.5957  decode.d6.loss_mask: 0.4271  decode.d6.loss_dice: 0.2533  decode.d7.loss_cls: 0.6393  decode.d7.loss_mask: 0.4115  decode.d7.loss_dice: 0.2538  decode.d8.loss_cls: 0.6163  decode.d8.loss_mask: 0.4270  decode.d8.loss_dice: 0.2469
09/30 09:22:16 - mmengine - INFO - Iter(train) [  2650/320000]  base_lr: 9.9255e-05 lr: 9.9255e-06  eta: 1 day, 14:21:27  time: 0.4314  data_time: 0.0088  memory: 5147  grad_norm: 88.5734  loss: 12.8921  decode.loss_cls: 0.5681  decode.loss_mask: 0.3341  decode.loss_dice: 0.3015  decode.d0.loss_cls: 1.3389  decode.d0.loss_mask: 0.3050  decode.d0.loss_dice: 0.3727  decode.d1.loss_cls: 0.6389  decode.d1.loss_mask: 0.3167  decode.d1.loss_dice: 0.2911  decode.d2.loss_cls: 0.5424  decode.d2.loss_mask: 0.3099  decode.d2.loss_dice: 0.2979  decode.d3.loss_cls: 0.5238  decode.d3.loss_mask: 0.3117  decode.d3.loss_dice: 0.3054  decode.d4.loss_cls: 0.5612  decode.d4.loss_mask: 0.3183  decode.d4.loss_dice: 0.2906  decode.d5.loss_cls: 0.5564  decode.d5.loss_mask: 0.3118  decode.d5.loss_dice: 0.3050  decode.d6.loss_cls: 0.5997  decode.d6.loss_mask: 0.3155  decode.d6.loss_dice: 0.3261  decode.d7.loss_cls: 0.6601  decode.d7.loss_mask: 0.3162  decode.d7.loss_dice: 0.3182  decode.d8.loss_cls: 0.6109  decode.d8.loss_mask: 0.3214  decode.d8.loss_dice: 0.3227
09/30 09:22:37 - mmengine - INFO - Iter(train) [  2700/320000]  base_lr: 9.9241e-05 lr: 9.9241e-06  eta: 1 day, 14:20:40  time: 0.4311  data_time: 0.0089  memory: 5161  grad_norm: 147.5363  loss: 14.0691  decode.loss_cls: 0.6360  decode.loss_mask: 0.3107  decode.loss_dice: 0.3467  decode.d0.loss_cls: 1.3421  decode.d0.loss_mask: 0.2910  decode.d0.loss_dice: 0.3301  decode.d1.loss_cls: 0.8174  decode.d1.loss_mask: 0.2737  decode.d1.loss_dice: 0.3000  decode.d2.loss_cls: 0.6678  decode.d2.loss_mask: 0.3518  decode.d2.loss_dice: 0.3070  decode.d3.loss_cls: 0.6452  decode.d3.loss_mask: 0.3928  decode.d3.loss_dice: 0.3170  decode.d4.loss_cls: 0.6747  decode.d4.loss_mask: 0.4918  decode.d4.loss_dice: 0.3277  decode.d5.loss_cls: 0.6996  decode.d5.loss_mask: 0.3937  decode.d5.loss_dice: 0.3144  decode.d6.loss_cls: 0.6622  decode.d6.loss_mask: 0.3702  decode.d6.loss_dice: 0.3181  decode.d7.loss_cls: 0.6069  decode.d7.loss_mask: 0.3432  decode.d7.loss_dice: 0.3009  decode.d8.loss_cls: 0.5953  decode.d8.loss_mask: 0.3361  decode.d8.loss_dice: 0.3051
09/30 09:22:59 - mmengine - INFO - Iter(train) [  2750/320000]  base_lr: 9.9227e-05 lr: 9.9227e-06  eta: 1 day, 14:19:53  time: 0.4296  data_time: 0.0086  memory: 5161  grad_norm: 324.1247  loss: 17.6756  decode.loss_cls: 0.7680  decode.loss_mask: 0.5091  decode.loss_dice: 0.4514  decode.d0.loss_cls: 1.5037  decode.d0.loss_mask: 0.4011  decode.d0.loss_dice: 0.3897  decode.d1.loss_cls: 0.8577  decode.d1.loss_mask: 0.3759  decode.d1.loss_dice: 0.3706  decode.d2.loss_cls: 0.8369  decode.d2.loss_mask: 0.4062  decode.d2.loss_dice: 0.3882  decode.d3.loss_cls: 0.8148  decode.d3.loss_mask: 0.4748  decode.d3.loss_dice: 0.4027  decode.d4.loss_cls: 0.8142  decode.d4.loss_mask: 0.5366  decode.d4.loss_dice: 0.4070  decode.d5.loss_cls: 0.7693  decode.d5.loss_mask: 0.5035  decode.d5.loss_dice: 0.4083  decode.d6.loss_cls: 0.8012  decode.d6.loss_mask: 0.5245  decode.d6.loss_dice: 0.4632  decode.d7.loss_cls: 0.8465  decode.d7.loss_mask: 0.4778  decode.d7.loss_dice: 0.4824  decode.d8.loss_cls: 0.8320  decode.d8.loss_mask: 0.4431  decode.d8.loss_dice: 0.4152
09/30 09:23:20 - mmengine - INFO - Iter(train) [  2800/320000]  base_lr: 9.9212e-05 lr: 9.9212e-06  eta: 1 day, 14:19:08  time: 0.4319  data_time: 0.0090  memory: 5147  grad_norm: 170.8000  loss: 17.1709  decode.loss_cls: 0.9583  decode.loss_mask: 0.3227  decode.loss_dice: 0.3454  decode.d0.loss_cls: 1.5989  decode.d0.loss_mask: 0.3369  decode.d0.loss_dice: 0.4738  decode.d1.loss_cls: 0.9713  decode.d1.loss_mask: 0.3182  decode.d1.loss_dice: 0.3997  decode.d2.loss_cls: 0.9202  decode.d2.loss_mask: 0.3056  decode.d2.loss_dice: 0.3525  decode.d3.loss_cls: 0.9204  decode.d3.loss_mask: 0.3196  decode.d3.loss_dice: 0.3613  decode.d4.loss_cls: 0.9256  decode.d4.loss_mask: 0.3293  decode.d4.loss_dice: 0.3526  decode.d5.loss_cls: 0.9740  decode.d5.loss_mask: 0.3367  decode.d5.loss_dice: 0.3625  decode.d6.loss_cls: 0.9336  decode.d6.loss_mask: 0.3196  decode.d6.loss_dice: 0.3569  decode.d7.loss_cls: 0.9677  decode.d7.loss_mask: 0.3416  decode.d7.loss_dice: 0.3784  decode.d8.loss_cls: 0.9723  decode.d8.loss_mask: 0.3469  decode.d8.loss_dice: 0.3684
09/30 09:23:42 - mmengine - INFO - Iter(train) [  2850/320000]  base_lr: 9.9198e-05 lr: 9.9198e-06  eta: 1 day, 14:18:24  time: 0.4316  data_time: 0.0087  memory: 5161  grad_norm: 136.7216  loss: 13.9344  decode.loss_cls: 0.5914  decode.loss_mask: 0.3487  decode.loss_dice: 0.2864  decode.d0.loss_cls: 1.5258  decode.d0.loss_mask: 0.3396  decode.d0.loss_dice: 0.3706  decode.d1.loss_cls: 0.8954  decode.d1.loss_mask: 0.3006  decode.d1.loss_dice: 0.3456  decode.d2.loss_cls: 0.6692  decode.d2.loss_mask: 0.3315  decode.d2.loss_dice: 0.2996  decode.d3.loss_cls: 0.6822  decode.d3.loss_mask: 0.3365  decode.d3.loss_dice: 0.3002  decode.d4.loss_cls: 0.5823  decode.d4.loss_mask: 0.3172  decode.d4.loss_dice: 0.3055  decode.d5.loss_cls: 0.5786  decode.d5.loss_mask: 0.3798  decode.d5.loss_dice: 0.2973  decode.d6.loss_cls: 0.5983  decode.d6.loss_mask: 0.3426  decode.d6.loss_dice: 0.2814  decode.d7.loss_cls: 0.6007  decode.d7.loss_mask: 0.3953  decode.d7.loss_dice: 0.3388  decode.d8.loss_cls: 0.5947  decode.d8.loss_mask: 0.3803  decode.d8.loss_dice: 0.3184
09/30 09:24:04 - mmengine - INFO - Iter(train) [  2900/320000]  base_lr: 9.9184e-05 lr: 9.9184e-06  eta: 1 day, 14:18:02  time: 0.4308  data_time: 0.0088  memory: 5147  grad_norm: 200.3912  loss: 15.8107  decode.loss_cls: 0.6547  decode.loss_mask: 0.5506  decode.loss_dice: 0.3020  decode.d0.loss_cls: 1.2902  decode.d0.loss_mask: 0.4775  decode.d0.loss_dice: 0.3529  decode.d1.loss_cls: 0.7818  decode.d1.loss_mask: 0.5267  decode.d1.loss_dice: 0.3530  decode.d2.loss_cls: 0.7428  decode.d2.loss_mask: 0.5026  decode.d2.loss_dice: 0.3455  decode.d3.loss_cls: 0.7214  decode.d3.loss_mask: 0.4906  decode.d3.loss_dice: 0.3423  decode.d4.loss_cls: 0.6480  decode.d4.loss_mask: 0.5085  decode.d4.loss_dice: 0.3318  decode.d5.loss_cls: 0.6876  decode.d5.loss_mask: 0.4713  decode.d5.loss_dice: 0.3206  decode.d6.loss_cls: 0.6326  decode.d6.loss_mask: 0.4676  decode.d6.loss_dice: 0.3192  decode.d7.loss_cls: 0.6505  decode.d7.loss_mask: 0.4810  decode.d7.loss_dice: 0.3120  decode.d8.loss_cls: 0.7025  decode.d8.loss_mask: 0.5297  decode.d8.loss_dice: 0.3132
09/30 09:24:25 - mmengine - INFO - Iter(train) [  2950/320000]  base_lr: 9.9170e-05 lr: 9.9170e-06  eta: 1 day, 14:17:21  time: 0.4317  data_time: 0.0089  memory: 5161  grad_norm: 231.9795  loss: 16.4164  decode.loss_cls: 0.5530  decode.loss_mask: 0.4964  decode.loss_dice: 0.3594  decode.d0.loss_cls: 1.2881  decode.d0.loss_mask: 0.5478  decode.d0.loss_dice: 0.4118  decode.d1.loss_cls: 0.7819  decode.d1.loss_mask: 0.5221  decode.d1.loss_dice: 0.3757  decode.d2.loss_cls: 0.7263  decode.d2.loss_mask: 0.5239  decode.d2.loss_dice: 0.3574  decode.d3.loss_cls: 0.7025  decode.d3.loss_mask: 0.5069  decode.d3.loss_dice: 0.3461  decode.d4.loss_cls: 0.7450  decode.d4.loss_mask: 0.5181  decode.d4.loss_dice: 0.3699  decode.d5.loss_cls: 0.7149  decode.d5.loss_mask: 0.5206  decode.d5.loss_dice: 0.3771  decode.d6.loss_cls: 0.6902  decode.d6.loss_mask: 0.5082  decode.d6.loss_dice: 0.3670  decode.d7.loss_cls: 0.7026  decode.d7.loss_mask: 0.5049  decode.d7.loss_dice: 0.3660  decode.d8.loss_cls: 0.6573  decode.d8.loss_mask: 0.5116  decode.d8.loss_dice: 0.3635
09/30 09:24:47 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 09:24:47 - mmengine - INFO - Iter(train) [  3000/320000]  base_lr: 9.9156e-05 lr: 9.9156e-06  eta: 1 day, 14:16:37  time: 0.4307  data_time: 0.0089  memory: 5180  grad_norm: 177.9724  loss: 17.2376  decode.loss_cls: 0.8816  decode.loss_mask: 0.3715  decode.loss_dice: 0.3601  decode.d0.loss_cls: 1.4193  decode.d0.loss_mask: 0.4036  decode.d0.loss_dice: 0.4075  decode.d1.loss_cls: 0.9271  decode.d1.loss_mask: 0.3762  decode.d1.loss_dice: 0.3525  decode.d2.loss_cls: 0.9043  decode.d2.loss_mask: 0.3619  decode.d2.loss_dice: 0.3464  decode.d3.loss_cls: 0.9450  decode.d3.loss_mask: 0.3739  decode.d3.loss_dice: 0.3552  decode.d4.loss_cls: 0.9213  decode.d4.loss_mask: 0.3586  decode.d4.loss_dice: 0.3600  decode.d5.loss_cls: 0.9219  decode.d5.loss_mask: 0.3679  decode.d5.loss_dice: 0.3453  decode.d6.loss_cls: 0.9893  decode.d6.loss_mask: 0.3542  decode.d6.loss_dice: 0.3675  decode.d7.loss_cls: 1.0358  decode.d7.loss_mask: 0.3578  decode.d7.loss_dice: 0.3435  decode.d8.loss_cls: 1.0172  decode.d8.loss_mask: 0.3554  decode.d8.loss_dice: 0.3556
09/30 09:25:08 - mmengine - INFO - Iter(train) [  3050/320000]  base_lr: 9.9142e-05 lr: 9.9142e-06  eta: 1 day, 14:15:59  time: 0.4320  data_time: 0.0089  memory: 5161  grad_norm: 316.1399  loss: 16.8922  decode.loss_cls: 0.6422  decode.loss_mask: 0.5337  decode.loss_dice: 0.3988  decode.d0.loss_cls: 1.4142  decode.d0.loss_mask: 0.5286  decode.d0.loss_dice: 0.4227  decode.d1.loss_cls: 0.8232  decode.d1.loss_mask: 0.5318  decode.d1.loss_dice: 0.3946  decode.d2.loss_cls: 0.7816  decode.d2.loss_mask: 0.5366  decode.d2.loss_dice: 0.4202  decode.d3.loss_cls: 0.7277  decode.d3.loss_mask: 0.5250  decode.d3.loss_dice: 0.4243  decode.d4.loss_cls: 0.6240  decode.d4.loss_mask: 0.5332  decode.d4.loss_dice: 0.4012  decode.d5.loss_cls: 0.5945  decode.d5.loss_mask: 0.5071  decode.d5.loss_dice: 0.3921  decode.d6.loss_cls: 0.6131  decode.d6.loss_mask: 0.5588  decode.d6.loss_dice: 0.3697  decode.d7.loss_cls: 0.6284  decode.d7.loss_mask: 0.5774  decode.d7.loss_dice: 0.4091  decode.d8.loss_cls: 0.6025  decode.d8.loss_mask: 0.5734  decode.d8.loss_dice: 0.4028
09/30 09:25:30 - mmengine - INFO - Iter(train) [  3100/320000]  base_lr: 9.9128e-05 lr: 9.9128e-06  eta: 1 day, 14:15:20  time: 0.4316  data_time: 0.0088  memory: 5161  grad_norm: 87.3908  loss: 11.0867  decode.loss_cls: 0.5260  decode.loss_mask: 0.2494  decode.loss_dice: 0.2573  decode.d0.loss_cls: 1.2464  decode.d0.loss_mask: 0.2376  decode.d0.loss_dice: 0.2527  decode.d1.loss_cls: 0.6898  decode.d1.loss_mask: 0.2476  decode.d1.loss_dice: 0.2756  decode.d2.loss_cls: 0.5355  decode.d2.loss_mask: 0.2379  decode.d2.loss_dice: 0.2248  decode.d3.loss_cls: 0.5088  decode.d3.loss_mask: 0.2397  decode.d3.loss_dice: 0.2706  decode.d4.loss_cls: 0.5215  decode.d4.loss_mask: 0.2404  decode.d4.loss_dice: 0.2458  decode.d5.loss_cls: 0.5219  decode.d5.loss_mask: 0.2377  decode.d5.loss_dice: 0.2706  decode.d6.loss_cls: 0.5321  decode.d6.loss_mask: 0.2360  decode.d6.loss_dice: 0.2465  decode.d7.loss_cls: 0.5128  decode.d7.loss_mask: 0.2442  decode.d7.loss_dice: 0.2587  decode.d8.loss_cls: 0.5139  decode.d8.loss_mask: 0.2496  decode.d8.loss_dice: 0.2555
09/30 09:25:51 - mmengine - INFO - Iter(train) [  3150/320000]  base_lr: 9.9114e-05 lr: 9.9114e-06  eta: 1 day, 14:14:38  time: 0.4305  data_time: 0.0090  memory: 5147  grad_norm: 89.2645  loss: 16.6795  decode.loss_cls: 0.7460  decode.loss_mask: 0.4082  decode.loss_dice: 0.3736  decode.d0.loss_cls: 1.4156  decode.d0.loss_mask: 0.4303  decode.d0.loss_dice: 0.4602  decode.d1.loss_cls: 1.0134  decode.d1.loss_mask: 0.4127  decode.d1.loss_dice: 0.3921  decode.d2.loss_cls: 0.8164  decode.d2.loss_mask: 0.3940  decode.d2.loss_dice: 0.3866  decode.d3.loss_cls: 0.7341  decode.d3.loss_mask: 0.3953  decode.d3.loss_dice: 0.3717  decode.d4.loss_cls: 0.7789  decode.d4.loss_mask: 0.4077  decode.d4.loss_dice: 0.3921  decode.d5.loss_cls: 0.7596  decode.d5.loss_mask: 0.4081  decode.d5.loss_dice: 0.3726  decode.d6.loss_cls: 0.7973  decode.d6.loss_mask: 0.3979  decode.d6.loss_dice: 0.3567  decode.d7.loss_cls: 0.8329  decode.d7.loss_mask: 0.4111  decode.d7.loss_dice: 0.3855  decode.d8.loss_cls: 0.8418  decode.d8.loss_mask: 0.4020  decode.d8.loss_dice: 0.3851
09/30 09:26:13 - mmengine - INFO - Iter(train) [  3200/320000]  base_lr: 9.9100e-05 lr: 9.9100e-06  eta: 1 day, 14:14:00  time: 0.4308  data_time: 0.0088  memory: 5161  grad_norm: 130.2937  loss: 15.5433  decode.loss_cls: 0.8709  decode.loss_mask: 0.2886  decode.loss_dice: 0.2583  decode.d0.loss_cls: 1.6208  decode.d0.loss_mask: 0.3380  decode.d0.loss_dice: 0.3362  decode.d1.loss_cls: 0.9817  decode.d1.loss_mask: 0.3355  decode.d1.loss_dice: 0.2822  decode.d2.loss_cls: 0.9259  decode.d2.loss_mask: 0.3200  decode.d2.loss_dice: 0.2573  decode.d3.loss_cls: 0.9158  decode.d3.loss_mask: 0.3111  decode.d3.loss_dice: 0.2548  decode.d4.loss_cls: 0.9010  decode.d4.loss_mask: 0.3151  decode.d4.loss_dice: 0.2478  decode.d5.loss_cls: 0.9542  decode.d5.loss_mask: 0.2911  decode.d5.loss_dice: 0.2558  decode.d6.loss_cls: 0.8775  decode.d6.loss_mask: 0.3066  decode.d6.loss_dice: 0.2879  decode.d7.loss_cls: 0.8639  decode.d7.loss_mask: 0.3050  decode.d7.loss_dice: 0.2570  decode.d8.loss_cls: 0.8462  decode.d8.loss_mask: 0.2962  decode.d8.loss_dice: 0.2409
09/30 09:26:35 - mmengine - INFO - Iter(train) [  3250/320000]  base_lr: 9.9086e-05 lr: 9.9086e-06  eta: 1 day, 14:13:22  time: 0.4318  data_time: 0.0089  memory: 5180  grad_norm: 107.9511  loss: 13.8945  decode.loss_cls: 0.7342  decode.loss_mask: 0.3287  decode.loss_dice: 0.3079  decode.d0.loss_cls: 1.4478  decode.d0.loss_mask: 0.3412  decode.d0.loss_dice: 0.3443  decode.d1.loss_cls: 0.7270  decode.d1.loss_mask: 0.3412  decode.d1.loss_dice: 0.3052  decode.d2.loss_cls: 0.7086  decode.d2.loss_mask: 0.3335  decode.d2.loss_dice: 0.2878  decode.d3.loss_cls: 0.6249  decode.d3.loss_mask: 0.3215  decode.d3.loss_dice: 0.3133  decode.d4.loss_cls: 0.6490  decode.d4.loss_mask: 0.3231  decode.d4.loss_dice: 0.2820  decode.d5.loss_cls: 0.6524  decode.d5.loss_mask: 0.3386  decode.d5.loss_dice: 0.3171  decode.d6.loss_cls: 0.6114  decode.d6.loss_mask: 0.3421  decode.d6.loss_dice: 0.3180  decode.d7.loss_cls: 0.6474  decode.d7.loss_mask: 0.3338  decode.d7.loss_dice: 0.2656  decode.d8.loss_cls: 0.7281  decode.d8.loss_mask: 0.3287  decode.d8.loss_dice: 0.2900
09/30 09:26:56 - mmengine - INFO - Iter(train) [  3300/320000]  base_lr: 9.9072e-05 lr: 9.9072e-06  eta: 1 day, 14:12:46  time: 0.4316  data_time: 0.0088  memory: 5161  grad_norm: 139.8877  loss: 17.3573  decode.loss_cls: 0.8380  decode.loss_mask: 0.3975  decode.loss_dice: 0.3505  decode.d0.loss_cls: 1.3922  decode.d0.loss_mask: 0.4270  decode.d0.loss_dice: 0.4015  decode.d1.loss_cls: 0.9437  decode.d1.loss_mask: 0.3926  decode.d1.loss_dice: 0.3829  decode.d2.loss_cls: 0.9061  decode.d2.loss_mask: 0.4195  decode.d2.loss_dice: 0.4115  decode.d3.loss_cls: 0.9110  decode.d3.loss_mask: 0.4258  decode.d3.loss_dice: 0.4045  decode.d4.loss_cls: 0.9035  decode.d4.loss_mask: 0.4367  decode.d4.loss_dice: 0.3526  decode.d5.loss_cls: 0.9299  decode.d5.loss_mask: 0.4797  decode.d5.loss_dice: 0.3344  decode.d6.loss_cls: 0.9438  decode.d6.loss_mask: 0.4589  decode.d6.loss_dice: 0.3308  decode.d7.loss_cls: 0.8741  decode.d7.loss_mask: 0.4248  decode.d7.loss_dice: 0.3235  decode.d8.loss_cls: 0.8606  decode.d8.loss_mask: 0.4001  decode.d8.loss_dice: 0.2995
09/30 09:27:18 - mmengine - INFO - Iter(train) [  3350/320000]  base_lr: 9.9058e-05 lr: 9.9058e-06  eta: 1 day, 14:12:10  time: 0.4314  data_time: 0.0087  memory: 5180  grad_norm: 138.9115  loss: 15.0480  decode.loss_cls: 0.6530  decode.loss_mask: 0.3941  decode.loss_dice: 0.3195  decode.d0.loss_cls: 1.3727  decode.d0.loss_mask: 0.3829  decode.d0.loss_dice: 0.3621  decode.d1.loss_cls: 0.8448  decode.d1.loss_mask: 0.3320  decode.d1.loss_dice: 0.3159  decode.d2.loss_cls: 0.7610  decode.d2.loss_mask: 0.3230  decode.d2.loss_dice: 0.3006  decode.d3.loss_cls: 0.7881  decode.d3.loss_mask: 0.3329  decode.d3.loss_dice: 0.3257  decode.d4.loss_cls: 0.8004  decode.d4.loss_mask: 0.3682  decode.d4.loss_dice: 0.3666  decode.d5.loss_cls: 0.7763  decode.d5.loss_mask: 0.3447  decode.d5.loss_dice: 0.3179  decode.d6.loss_cls: 0.7840  decode.d6.loss_mask: 0.3404  decode.d6.loss_dice: 0.3395  decode.d7.loss_cls: 0.7467  decode.d7.loss_mask: 0.3429  decode.d7.loss_dice: 0.3255  decode.d8.loss_cls: 0.7259  decode.d8.loss_mask: 0.3509  decode.d8.loss_dice: 0.3098
09/30 09:27:39 - mmengine - INFO - Iter(train) [  3400/320000]  base_lr: 9.9044e-05 lr: 9.9044e-06  eta: 1 day, 14:11:35  time: 0.4312  data_time: 0.0088  memory: 5180  grad_norm: 96.5437  loss: 12.5414  decode.loss_cls: 0.5149  decode.loss_mask: 0.2982  decode.loss_dice: 0.3202  decode.d0.loss_cls: 1.2454  decode.d0.loss_mask: 0.2763  decode.d0.loss_dice: 0.3373  decode.d1.loss_cls: 0.7064  decode.d1.loss_mask: 0.2803  decode.d1.loss_dice: 0.3136  decode.d2.loss_cls: 0.6214  decode.d2.loss_mask: 0.2830  decode.d2.loss_dice: 0.2975  decode.d3.loss_cls: 0.6068  decode.d3.loss_mask: 0.2713  decode.d3.loss_dice: 0.3087  decode.d4.loss_cls: 0.6933  decode.d4.loss_mask: 0.2813  decode.d4.loss_dice: 0.3032  decode.d5.loss_cls: 0.5662  decode.d5.loss_mask: 0.2820  decode.d5.loss_dice: 0.3080  decode.d6.loss_cls: 0.5434  decode.d6.loss_mask: 0.2761  decode.d6.loss_dice: 0.3076  decode.d7.loss_cls: 0.5530  decode.d7.loss_mask: 0.2828  decode.d7.loss_dice: 0.3133  decode.d8.loss_cls: 0.5190  decode.d8.loss_mask: 0.2906  decode.d8.loss_dice: 0.3402
09/30 09:28:01 - mmengine - INFO - Iter(train) [  3450/320000]  base_lr: 9.9029e-05 lr: 9.9029e-06  eta: 1 day, 14:11:00  time: 0.4323  data_time: 0.0088  memory: 5161  grad_norm: 127.3647  loss: 15.5741  decode.loss_cls: 0.7205  decode.loss_mask: 0.2977  decode.loss_dice: 0.4515  decode.d0.loss_cls: 1.2912  decode.d0.loss_mask: 0.3463  decode.d0.loss_dice: 0.5053  decode.d1.loss_cls: 0.7852  decode.d1.loss_mask: 0.2964  decode.d1.loss_dice: 0.4235  decode.d2.loss_cls: 0.6182  decode.d2.loss_mask: 0.3168  decode.d2.loss_dice: 0.4984  decode.d3.loss_cls: 0.7016  decode.d3.loss_mask: 0.2907  decode.d3.loss_dice: 0.4662  decode.d4.loss_cls: 0.6978  decode.d4.loss_mask: 0.3100  decode.d4.loss_dice: 0.4590  decode.d5.loss_cls: 0.7447  decode.d5.loss_mask: 0.3238  decode.d5.loss_dice: 0.4666  decode.d6.loss_cls: 0.7501  decode.d6.loss_mask: 0.3189  decode.d6.loss_dice: 0.4858  decode.d7.loss_cls: 0.6632  decode.d7.loss_mask: 0.3217  decode.d7.loss_dice: 0.4477  decode.d8.loss_cls: 0.7076  decode.d8.loss_mask: 0.3424  decode.d8.loss_dice: 0.5252
09/30 09:28:22 - mmengine - INFO - Iter(train) [  3500/320000]  base_lr: 9.9015e-05 lr: 9.9015e-06  eta: 1 day, 14:10:25  time: 0.4306  data_time: 0.0085  memory: 5179  grad_norm: 225.0780  loss: 17.9127  decode.loss_cls: 0.8783  decode.loss_mask: 0.3800  decode.loss_dice: 0.4332  decode.d0.loss_cls: 1.6039  decode.d0.loss_mask: 0.3688  decode.d0.loss_dice: 0.4646  decode.d1.loss_cls: 1.0927  decode.d1.loss_mask: 0.3322  decode.d1.loss_dice: 0.4300  decode.d2.loss_cls: 0.8746  decode.d2.loss_mask: 0.3453  decode.d2.loss_dice: 0.4101  decode.d3.loss_cls: 0.9277  decode.d3.loss_mask: 0.4005  decode.d3.loss_dice: 0.4045  decode.d4.loss_cls: 0.7931  decode.d4.loss_mask: 0.4031  decode.d4.loss_dice: 0.4479  decode.d5.loss_cls: 0.8293  decode.d5.loss_mask: 0.5195  decode.d5.loss_dice: 0.4444  decode.d6.loss_cls: 0.8997  decode.d6.loss_mask: 0.3794  decode.d6.loss_dice: 0.4104  decode.d7.loss_cls: 0.9472  decode.d7.loss_mask: 0.3607  decode.d7.loss_dice: 0.4138  decode.d8.loss_cls: 0.8778  decode.d8.loss_mask: 0.3846  decode.d8.loss_dice: 0.4555
09/30 09:28:44 - mmengine - INFO - Iter(train) [  3550/320000]  base_lr: 9.9001e-05 lr: 9.9001e-06  eta: 1 day, 14:09:52  time: 0.4314  data_time: 0.0087  memory: 5161  grad_norm: 293.8931  loss: 19.4691  decode.loss_cls: 0.9064  decode.loss_mask: 0.6606  decode.loss_dice: 0.4489  decode.d0.loss_cls: 1.3521  decode.d0.loss_mask: 0.6340  decode.d0.loss_dice: 0.5417  decode.d1.loss_cls: 1.0083  decode.d1.loss_mask: 0.5366  decode.d1.loss_dice: 0.4579  decode.d2.loss_cls: 0.9261  decode.d2.loss_mask: 0.5281  decode.d2.loss_dice: 0.4646  decode.d3.loss_cls: 0.8622  decode.d3.loss_mask: 0.5256  decode.d3.loss_dice: 0.4150  decode.d4.loss_cls: 0.8388  decode.d4.loss_mask: 0.5530  decode.d4.loss_dice: 0.3840  decode.d5.loss_cls: 0.8530  decode.d5.loss_mask: 0.6273  decode.d5.loss_dice: 0.3909  decode.d6.loss_cls: 0.8679  decode.d6.loss_mask: 0.5270  decode.d6.loss_dice: 0.3836  decode.d7.loss_cls: 0.8919  decode.d7.loss_mask: 0.5343  decode.d7.loss_dice: 0.3950  decode.d8.loss_cls: 0.9127  decode.d8.loss_mask: 0.5908  decode.d8.loss_dice: 0.4511
09/30 09:29:06 - mmengine - INFO - Iter(train) [  3600/320000]  base_lr: 9.8987e-05 lr: 9.8987e-06  eta: 1 day, 14:09:17  time: 0.4314  data_time: 0.0089  memory: 5180  grad_norm: 96.4902  loss: 11.6897  decode.loss_cls: 0.4030  decode.loss_mask: 0.3874  decode.loss_dice: 0.2997  decode.d0.loss_cls: 0.9761  decode.d0.loss_mask: 0.3832  decode.d0.loss_dice: 0.2763  decode.d1.loss_cls: 0.5163  decode.d1.loss_mask: 0.3832  decode.d1.loss_dice: 0.2745  decode.d2.loss_cls: 0.4570  decode.d2.loss_mask: 0.3919  decode.d2.loss_dice: 0.2836  decode.d3.loss_cls: 0.4663  decode.d3.loss_mask: 0.3868  decode.d3.loss_dice: 0.2738  decode.d4.loss_cls: 0.4622  decode.d4.loss_mask: 0.3799  decode.d4.loss_dice: 0.2742  decode.d5.loss_cls: 0.4182  decode.d5.loss_mask: 0.3857  decode.d5.loss_dice: 0.2765  decode.d6.loss_cls: 0.4058  decode.d6.loss_mask: 0.3875  decode.d6.loss_dice: 0.2903  decode.d7.loss_cls: 0.4446  decode.d7.loss_mask: 0.3823  decode.d7.loss_dice: 0.2878  decode.d8.loss_cls: 0.4574  decode.d8.loss_mask: 0.3866  decode.d8.loss_dice: 0.2917
09/30 09:29:27 - mmengine - INFO - Iter(train) [  3650/320000]  base_lr: 9.8973e-05 lr: 9.8973e-06  eta: 1 day, 14:08:45  time: 0.4317  data_time: 0.0087  memory: 5160  grad_norm: 263.3743  loss: 14.7629  decode.loss_cls: 0.5398  decode.loss_mask: 0.7997  decode.loss_dice: 0.4127  decode.d0.loss_cls: 1.0978  decode.d0.loss_mask: 0.4309  decode.d0.loss_dice: 0.4231  decode.d1.loss_cls: 0.5908  decode.d1.loss_mask: 0.4047  decode.d1.loss_dice: 0.3940  decode.d2.loss_cls: 0.5127  decode.d2.loss_mask: 0.4115  decode.d2.loss_dice: 0.3890  decode.d3.loss_cls: 0.5151  decode.d3.loss_mask: 0.4013  decode.d3.loss_dice: 0.3942  decode.d4.loss_cls: 0.5068  decode.d4.loss_mask: 0.4064  decode.d4.loss_dice: 0.3670  decode.d5.loss_cls: 0.4961  decode.d5.loss_mask: 0.4411  decode.d5.loss_dice: 0.3907  decode.d6.loss_cls: 0.5105  decode.d6.loss_mask: 0.4306  decode.d6.loss_dice: 0.4001  decode.d7.loss_cls: 0.5971  decode.d7.loss_mask: 0.6008  decode.d7.loss_dice: 0.4298  decode.d8.loss_cls: 0.5700  decode.d8.loss_mask: 0.4974  decode.d8.loss_dice: 0.4010
09/30 09:29:49 - mmengine - INFO - Iter(train) [  3700/320000]  base_lr: 9.8959e-05 lr: 9.8959e-06  eta: 1 day, 14:08:12  time: 0.4328  data_time: 0.0089  memory: 5180  grad_norm: 130.3208  loss: 9.8770  decode.loss_cls: 0.3187  decode.loss_mask: 0.2868  decode.loss_dice: 0.3185  decode.d0.loss_cls: 1.1873  decode.d0.loss_mask: 0.2537  decode.d0.loss_dice: 0.2468  decode.d1.loss_cls: 0.5258  decode.d1.loss_mask: 0.2546  decode.d1.loss_dice: 0.2435  decode.d2.loss_cls: 0.3835  decode.d2.loss_mask: 0.2570  decode.d2.loss_dice: 0.2539  decode.d3.loss_cls: 0.3454  decode.d3.loss_mask: 0.2624  decode.d3.loss_dice: 0.2529  decode.d4.loss_cls: 0.3849  decode.d4.loss_mask: 0.2781  decode.d4.loss_dice: 0.2629  decode.d5.loss_cls: 0.3985  decode.d5.loss_mask: 0.2563  decode.d5.loss_dice: 0.2426  decode.d6.loss_cls: 0.3724  decode.d6.loss_mask: 0.2687  decode.d6.loss_dice: 0.2499  decode.d7.loss_cls: 0.3942  decode.d7.loss_mask: 0.2644  decode.d7.loss_dice: 0.2597  decode.d8.loss_cls: 0.3363  decode.d8.loss_mask: 0.2597  decode.d8.loss_dice: 0.2578
09/30 09:30:10 - mmengine - INFO - Iter(train) [  3750/320000]  base_lr: 9.8945e-05 lr: 9.8945e-06  eta: 1 day, 14:07:40  time: 0.4306  data_time: 0.0085  memory: 5146  grad_norm: 117.9622  loss: 11.5938  decode.loss_cls: 0.5635  decode.loss_mask: 0.3056  decode.loss_dice: 0.2752  decode.d0.loss_cls: 1.1736  decode.d0.loss_mask: 0.3361  decode.d0.loss_dice: 0.2628  decode.d1.loss_cls: 0.5982  decode.d1.loss_mask: 0.3547  decode.d1.loss_dice: 0.2716  decode.d2.loss_cls: 0.4888  decode.d2.loss_mask: 0.3100  decode.d2.loss_dice: 0.2707  decode.d3.loss_cls: 0.4656  decode.d3.loss_mask: 0.3008  decode.d3.loss_dice: 0.2823  decode.d4.loss_cls: 0.4943  decode.d4.loss_mask: 0.3120  decode.d4.loss_dice: 0.2830  decode.d5.loss_cls: 0.4696  decode.d5.loss_mask: 0.3119  decode.d5.loss_dice: 0.2715  decode.d6.loss_cls: 0.4721  decode.d6.loss_mask: 0.3062  decode.d6.loss_dice: 0.2765  decode.d7.loss_cls: 0.4788  decode.d7.loss_mask: 0.2917  decode.d7.loss_dice: 0.2492  decode.d8.loss_cls: 0.5442  decode.d8.loss_mask: 0.3136  decode.d8.loss_dice: 0.2599
09/30 09:30:32 - mmengine - INFO - Iter(train) [  3800/320000]  base_lr: 9.8931e-05 lr: 9.8931e-06  eta: 1 day, 14:07:10  time: 0.4322  data_time: 0.0088  memory: 5145  grad_norm: 173.6175  loss: 15.0440  decode.loss_cls: 0.6458  decode.loss_mask: 0.3969  decode.loss_dice: 0.4135  decode.d0.loss_cls: 1.3238  decode.d0.loss_mask: 0.3523  decode.d0.loss_dice: 0.3987  decode.d1.loss_cls: 0.7500  decode.d1.loss_mask: 0.3285  decode.d1.loss_dice: 0.3705  decode.d2.loss_cls: 0.7301  decode.d2.loss_mask: 0.3388  decode.d2.loss_dice: 0.3741  decode.d3.loss_cls: 0.7337  decode.d3.loss_mask: 0.3136  decode.d3.loss_dice: 0.3561  decode.d4.loss_cls: 0.7041  decode.d4.loss_mask: 0.3476  decode.d4.loss_dice: 0.3594  decode.d5.loss_cls: 0.6903  decode.d5.loss_mask: 0.3231  decode.d5.loss_dice: 0.3559  decode.d6.loss_cls: 0.7463  decode.d6.loss_mask: 0.3379  decode.d6.loss_dice: 0.3903  decode.d7.loss_cls: 0.7324  decode.d7.loss_mask: 0.3273  decode.d7.loss_dice: 0.3990  decode.d8.loss_cls: 0.7663  decode.d8.loss_mask: 0.3413  decode.d8.loss_dice: 0.3963
09/30 09:30:53 - mmengine - INFO - Iter(train) [  3850/320000]  base_lr: 9.8917e-05 lr: 9.8917e-06  eta: 1 day, 14:06:37  time: 0.4315  data_time: 0.0088  memory: 5145  grad_norm: 107.2049  loss: 11.0639  decode.loss_cls: 0.4776  decode.loss_mask: 0.3189  decode.loss_dice: 0.2237  decode.d0.loss_cls: 1.1891  decode.d0.loss_mask: 0.3347  decode.d0.loss_dice: 0.2872  decode.d1.loss_cls: 0.5776  decode.d1.loss_mask: 0.3027  decode.d1.loss_dice: 0.2328  decode.d2.loss_cls: 0.4842  decode.d2.loss_mask: 0.3244  decode.d2.loss_dice: 0.2621  decode.d3.loss_cls: 0.4689  decode.d3.loss_mask: 0.3218  decode.d3.loss_dice: 0.2362  decode.d4.loss_cls: 0.4648  decode.d4.loss_mask: 0.3210  decode.d4.loss_dice: 0.2277  decode.d5.loss_cls: 0.4503  decode.d5.loss_mask: 0.3158  decode.d5.loss_dice: 0.2264  decode.d6.loss_cls: 0.3987  decode.d6.loss_mask: 0.3179  decode.d6.loss_dice: 0.2356  decode.d7.loss_cls: 0.4266  decode.d7.loss_mask: 0.3255  decode.d7.loss_dice: 0.2358  decode.d8.loss_cls: 0.5030  decode.d8.loss_mask: 0.3311  decode.d8.loss_dice: 0.2415
09/30 09:31:15 - mmengine - INFO - Iter(train) [  3900/320000]  base_lr: 9.8903e-05 lr: 9.8903e-06  eta: 1 day, 14:06:06  time: 0.4313  data_time: 0.0087  memory: 5160  grad_norm: 86.6982  loss: 12.7875  decode.loss_cls: 0.6335  decode.loss_mask: 0.2677  decode.loss_dice: 0.2526  decode.d0.loss_cls: 1.3433  decode.d0.loss_mask: 0.2817  decode.d0.loss_dice: 0.2976  decode.d1.loss_cls: 0.6932  decode.d1.loss_mask: 0.2780  decode.d1.loss_dice: 0.2520  decode.d2.loss_cls: 0.6616  decode.d2.loss_mask: 0.2779  decode.d2.loss_dice: 0.2560  decode.d3.loss_cls: 0.6962  decode.d3.loss_mask: 0.2674  decode.d3.loss_dice: 0.2416  decode.d4.loss_cls: 0.6869  decode.d4.loss_mask: 0.2712  decode.d4.loss_dice: 0.2562  decode.d5.loss_cls: 0.6896  decode.d5.loss_mask: 0.2706  decode.d5.loss_dice: 0.2407  decode.d6.loss_cls: 0.7562  decode.d6.loss_mask: 0.2715  decode.d6.loss_dice: 0.2387  decode.d7.loss_cls: 0.6783  decode.d7.loss_mask: 0.2711  decode.d7.loss_dice: 0.2475  decode.d8.loss_cls: 0.7160  decode.d8.loss_mask: 0.2646  decode.d8.loss_dice: 0.2278
09/30 09:31:37 - mmengine - INFO - Iter(train) [  3950/320000]  base_lr: 9.8889e-05 lr: 9.8889e-06  eta: 1 day, 14:05:36  time: 0.4323  data_time: 0.0090  memory: 5161  grad_norm: 144.9527  loss: 14.1385  decode.loss_cls: 0.5753  decode.loss_mask: 0.3497  decode.loss_dice: 0.2889  decode.d0.loss_cls: 1.2917  decode.d0.loss_mask: 0.3756  decode.d0.loss_dice: 0.3343  decode.d1.loss_cls: 0.7437  decode.d1.loss_mask: 0.3453  decode.d1.loss_dice: 0.2936  decode.d2.loss_cls: 0.7144  decode.d2.loss_mask: 0.3667  decode.d2.loss_dice: 0.2990  decode.d3.loss_cls: 0.6963  decode.d3.loss_mask: 0.3438  decode.d3.loss_dice: 0.2938  decode.d4.loss_cls: 0.6158  decode.d4.loss_mask: 0.3636  decode.d4.loss_dice: 0.3215  decode.d5.loss_cls: 0.6363  decode.d5.loss_mask: 0.4379  decode.d5.loss_dice: 0.3308  decode.d6.loss_cls: 0.5831  decode.d6.loss_mask: 0.4572  decode.d6.loss_dice: 0.3259  decode.d7.loss_cls: 0.5825  decode.d7.loss_mask: 0.4527  decode.d7.loss_dice: 0.3457  decode.d8.loss_cls: 0.5827  decode.d8.loss_mask: 0.4426  decode.d8.loss_dice: 0.3483
09/30 09:31:58 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 09:31:58 - mmengine - INFO - Iter(train) [  4000/320000]  base_lr: 9.8875e-05 lr: 9.8875e-06  eta: 1 day, 14:05:05  time: 0.4317  data_time: 0.0088  memory: 5161  grad_norm: 150.9489  loss: 11.9810  decode.loss_cls: 0.5256  decode.loss_mask: 0.3056  decode.loss_dice: 0.2553  decode.d0.loss_cls: 1.1023  decode.d0.loss_mask: 0.2896  decode.d0.loss_dice: 0.2598  decode.d1.loss_cls: 0.6819  decode.d1.loss_mask: 0.3227  decode.d1.loss_dice: 0.2284  decode.d2.loss_cls: 0.6554  decode.d2.loss_mask: 0.2762  decode.d2.loss_dice: 0.2274  decode.d3.loss_cls: 0.5709  decode.d3.loss_mask: 0.2803  decode.d3.loss_dice: 0.2342  decode.d4.loss_cls: 0.6337  decode.d4.loss_mask: 0.2907  decode.d4.loss_dice: 0.2468  decode.d5.loss_cls: 0.5823  decode.d5.loss_mask: 0.2836  decode.d5.loss_dice: 0.2513  decode.d6.loss_cls: 0.5973  decode.d6.loss_mask: 0.2950  decode.d6.loss_dice: 0.2359  decode.d7.loss_cls: 0.6448  decode.d7.loss_mask: 0.3275  decode.d7.loss_dice: 0.2415  decode.d8.loss_cls: 0.5556  decode.d8.loss_mask: 0.3254  decode.d8.loss_dice: 0.2540
09/30 09:32:20 - mmengine - INFO - Iter(train) [  4050/320000]  base_lr: 9.8860e-05 lr: 9.8860e-06  eta: 1 day, 14:04:34  time: 0.4317  data_time: 0.0087  memory: 5160  grad_norm: 146.5712  loss: 12.9819  decode.loss_cls: 0.4939  decode.loss_mask: 0.3469  decode.loss_dice: 0.3200  decode.d0.loss_cls: 1.0419  decode.d0.loss_mask: 0.3727  decode.d0.loss_dice: 0.3768  decode.d1.loss_cls: 0.6235  decode.d1.loss_mask: 0.3634  decode.d1.loss_dice: 0.3523  decode.d2.loss_cls: 0.5738  decode.d2.loss_mask: 0.3484  decode.d2.loss_dice: 0.3476  decode.d3.loss_cls: 0.5186  decode.d3.loss_mask: 0.3483  decode.d3.loss_dice: 0.3766  decode.d4.loss_cls: 0.5056  decode.d4.loss_mask: 0.3541  decode.d4.loss_dice: 0.3786  decode.d5.loss_cls: 0.5269  decode.d5.loss_mask: 0.3809  decode.d5.loss_dice: 0.3692  decode.d6.loss_cls: 0.4677  decode.d6.loss_mask: 0.3533  decode.d6.loss_dice: 0.3237  decode.d7.loss_cls: 0.5047  decode.d7.loss_mask: 0.3784  decode.d7.loss_dice: 0.3993  decode.d8.loss_cls: 0.5148  decode.d8.loss_mask: 0.3542  decode.d8.loss_dice: 0.3659
09/30 09:32:41 - mmengine - INFO - Iter(train) [  4100/320000]  base_lr: 9.8846e-05 lr: 9.8846e-06  eta: 1 day, 14:04:04  time: 0.4327  data_time: 0.0087  memory: 5180  grad_norm: 118.5958  loss: 11.3197  decode.loss_cls: 0.4937  decode.loss_mask: 0.2836  decode.loss_dice: 0.2685  decode.d0.loss_cls: 1.3039  decode.d0.loss_mask: 0.2818  decode.d0.loss_dice: 0.2696  decode.d1.loss_cls: 0.5789  decode.d1.loss_mask: 0.2711  decode.d1.loss_dice: 0.2308  decode.d2.loss_cls: 0.5278  decode.d2.loss_mask: 0.2756  decode.d2.loss_dice: 0.2317  decode.d3.loss_cls: 0.5110  decode.d3.loss_mask: 0.2945  decode.d3.loss_dice: 0.2508  decode.d4.loss_cls: 0.5339  decode.d4.loss_mask: 0.2898  decode.d4.loss_dice: 0.2385  decode.d5.loss_cls: 0.5353  decode.d5.loss_mask: 0.2736  decode.d5.loss_dice: 0.2230  decode.d6.loss_cls: 0.5344  decode.d6.loss_mask: 0.2848  decode.d6.loss_dice: 0.2285  decode.d7.loss_cls: 0.5476  decode.d7.loss_mask: 0.2795  decode.d7.loss_dice: 0.2405  decode.d8.loss_cls: 0.5352  decode.d8.loss_mask: 0.2736  decode.d8.loss_dice: 0.2281
09/30 09:33:03 - mmengine - INFO - Iter(train) [  4150/320000]  base_lr: 9.8832e-05 lr: 9.8832e-06  eta: 1 day, 14:03:32  time: 0.4316  data_time: 0.0087  memory: 5180  grad_norm: 103.5503  loss: 12.4690  decode.loss_cls: 0.6532  decode.loss_mask: 0.2548  decode.loss_dice: 0.3369  decode.d0.loss_cls: 1.0265  decode.d0.loss_mask: 0.2737  decode.d0.loss_dice: 0.3961  decode.d1.loss_cls: 0.6181  decode.d1.loss_mask: 0.2603  decode.d1.loss_dice: 0.3239  decode.d2.loss_cls: 0.5644  decode.d2.loss_mask: 0.2651  decode.d2.loss_dice: 0.3396  decode.d3.loss_cls: 0.6033  decode.d3.loss_mask: 0.2521  decode.d3.loss_dice: 0.3157  decode.d4.loss_cls: 0.6108  decode.d4.loss_mask: 0.2583  decode.d4.loss_dice: 0.3458  decode.d5.loss_cls: 0.6475  decode.d5.loss_mask: 0.2539  decode.d5.loss_dice: 0.3228  decode.d6.loss_cls: 0.6055  decode.d6.loss_mask: 0.2606  decode.d6.loss_dice: 0.3073  decode.d7.loss_cls: 0.5982  decode.d7.loss_mask: 0.2514  decode.d7.loss_dice: 0.3168  decode.d8.loss_cls: 0.6000  decode.d8.loss_mask: 0.2477  decode.d8.loss_dice: 0.3587
09/30 09:33:25 - mmengine - INFO - Iter(train) [  4200/320000]  base_lr: 9.8818e-05 lr: 9.8818e-06  eta: 1 day, 14:03:02  time: 0.4325  data_time: 0.0089  memory: 5161  grad_norm: 89.2565  loss: 11.5104  decode.loss_cls: 0.4226  decode.loss_mask: 0.3638  decode.loss_dice: 0.2663  decode.d0.loss_cls: 1.1152  decode.d0.loss_mask: 0.3090  decode.d0.loss_dice: 0.2633  decode.d1.loss_cls: 0.5358  decode.d1.loss_mask: 0.3302  decode.d1.loss_dice: 0.2672  decode.d2.loss_cls: 0.4977  decode.d2.loss_mask: 0.3613  decode.d2.loss_dice: 0.2622  decode.d3.loss_cls: 0.4078  decode.d3.loss_mask: 0.3725  decode.d3.loss_dice: 0.2689  decode.d4.loss_cls: 0.4275  decode.d4.loss_mask: 0.3467  decode.d4.loss_dice: 0.2518  decode.d5.loss_cls: 0.5174  decode.d5.loss_mask: 0.3099  decode.d5.loss_dice: 0.2615  decode.d6.loss_cls: 0.4571  decode.d6.loss_mask: 0.3442  decode.d6.loss_dice: 0.2905  decode.d7.loss_cls: 0.4826  decode.d7.loss_mask: 0.3964  decode.d7.loss_dice: 0.2868  decode.d8.loss_cls: 0.4437  decode.d8.loss_mask: 0.3881  decode.d8.loss_dice: 0.2622
09/30 09:33:46 - mmengine - INFO - Iter(train) [  4250/320000]  base_lr: 9.8804e-05 lr: 9.8804e-06  eta: 1 day, 14:02:33  time: 0.4302  data_time: 0.0085  memory: 5146  grad_norm: 285.4977  loss: 17.8409  decode.loss_cls: 0.7993  decode.loss_mask: 0.4586  decode.loss_dice: 0.4515  decode.d0.loss_cls: 1.5109  decode.d0.loss_mask: 0.4643  decode.d0.loss_dice: 0.5679  decode.d1.loss_cls: 0.9845  decode.d1.loss_mask: 0.4684  decode.d1.loss_dice: 0.4381  decode.d2.loss_cls: 0.8226  decode.d2.loss_mask: 0.4638  decode.d2.loss_dice: 0.4543  decode.d3.loss_cls: 0.8072  decode.d3.loss_mask: 0.4108  decode.d3.loss_dice: 0.4723  decode.d4.loss_cls: 0.7495  decode.d4.loss_mask: 0.4095  decode.d4.loss_dice: 0.4441  decode.d5.loss_cls: 0.7468  decode.d5.loss_mask: 0.4310  decode.d5.loss_dice: 0.4369  decode.d6.loss_cls: 0.7493  decode.d6.loss_mask: 0.4627  decode.d6.loss_dice: 0.5061  decode.d7.loss_cls: 0.7246  decode.d7.loss_mask: 0.4194  decode.d7.loss_dice: 0.4653  decode.d8.loss_cls: 0.7996  decode.d8.loss_mask: 0.4180  decode.d8.loss_dice: 0.5036
09/30 09:34:08 - mmengine - INFO - Iter(train) [  4300/320000]  base_lr: 9.8790e-05 lr: 9.8790e-06  eta: 1 day, 14:02:04  time: 0.4317  data_time: 0.0087  memory: 5146  grad_norm: 100.4425  loss: 11.8533  decode.loss_cls: 0.4472  decode.loss_mask: 0.2885  decode.loss_dice: 0.3668  decode.d0.loss_cls: 1.2793  decode.d0.loss_mask: 0.2922  decode.d0.loss_dice: 0.3645  decode.d1.loss_cls: 0.5580  decode.d1.loss_mask: 0.2825  decode.d1.loss_dice: 0.3518  decode.d2.loss_cls: 0.4209  decode.d2.loss_mask: 0.3050  decode.d2.loss_dice: 0.3536  decode.d3.loss_cls: 0.4190  decode.d3.loss_mask: 0.3006  decode.d3.loss_dice: 0.3743  decode.d4.loss_cls: 0.4550  decode.d4.loss_mask: 0.2736  decode.d4.loss_dice: 0.3588  decode.d5.loss_cls: 0.4573  decode.d5.loss_mask: 0.2882  decode.d5.loss_dice: 0.3694  decode.d6.loss_cls: 0.3943  decode.d6.loss_mask: 0.2924  decode.d6.loss_dice: 0.3621  decode.d7.loss_cls: 0.4089  decode.d7.loss_mask: 0.3040  decode.d7.loss_dice: 0.3540  decode.d8.loss_cls: 0.4417  decode.d8.loss_mask: 0.3175  decode.d8.loss_dice: 0.3720
09/30 09:34:29 - mmengine - INFO - Iter(train) [  4350/320000]  base_lr: 9.8776e-05 lr: 9.8776e-06  eta: 1 day, 14:01:34  time: 0.4316  data_time: 0.0086  memory: 5160  grad_norm: 212.7691  loss: 9.0970  decode.loss_cls: 0.3414  decode.loss_mask: 0.2422  decode.loss_dice: 0.2363  decode.d0.loss_cls: 1.2261  decode.d0.loss_mask: 0.2454  decode.d0.loss_dice: 0.2620  decode.d1.loss_cls: 0.4180  decode.d1.loss_mask: 0.2414  decode.d1.loss_dice: 0.2107  decode.d2.loss_cls: 0.4079  decode.d2.loss_mask: 0.2433  decode.d2.loss_dice: 0.2320  decode.d3.loss_cls: 0.3516  decode.d3.loss_mask: 0.2395  decode.d3.loss_dice: 0.2102  decode.d4.loss_cls: 0.3765  decode.d4.loss_mask: 0.2338  decode.d4.loss_dice: 0.1995  decode.d5.loss_cls: 0.3569  decode.d5.loss_mask: 0.2697  decode.d5.loss_dice: 0.2176  decode.d6.loss_cls: 0.3361  decode.d6.loss_mask: 0.2509  decode.d6.loss_dice: 0.2035  decode.d7.loss_cls: 0.3133  decode.d7.loss_mask: 0.2470  decode.d7.loss_dice: 0.2079  decode.d8.loss_cls: 0.3093  decode.d8.loss_mask: 0.2523  decode.d8.loss_dice: 0.2148
09/30 09:34:51 - mmengine - INFO - Iter(train) [  4400/320000]  base_lr: 9.8762e-05 lr: 9.8762e-06  eta: 1 day, 14:01:06  time: 0.4325  data_time: 0.0089  memory: 5180  grad_norm: 69.7470  loss: 10.8888  decode.loss_cls: 0.3803  decode.loss_mask: 0.2897  decode.loss_dice: 0.3139  decode.d0.loss_cls: 1.1954  decode.d0.loss_mask: 0.2562  decode.d0.loss_dice: 0.3078  decode.d1.loss_cls: 0.5038  decode.d1.loss_mask: 0.2570  decode.d1.loss_dice: 0.3127  decode.d2.loss_cls: 0.3588  decode.d2.loss_mask: 0.2687  decode.d2.loss_dice: 0.3375  decode.d3.loss_cls: 0.4321  decode.d3.loss_mask: 0.2636  decode.d3.loss_dice: 0.3438  decode.d4.loss_cls: 0.4353  decode.d4.loss_mask: 0.2708  decode.d4.loss_dice: 0.3461  decode.d5.loss_cls: 0.4167  decode.d5.loss_mask: 0.2929  decode.d5.loss_dice: 0.3220  decode.d6.loss_cls: 0.3967  decode.d6.loss_mask: 0.2872  decode.d6.loss_dice: 0.3213  decode.d7.loss_cls: 0.4032  decode.d7.loss_mask: 0.2986  decode.d7.loss_dice: 0.3195  decode.d8.loss_cls: 0.3599  decode.d8.loss_mask: 0.2781  decode.d8.loss_dice: 0.3192
09/30 09:35:12 - mmengine - INFO - Iter(train) [  4450/320000]  base_lr: 9.8748e-05 lr: 9.8748e-06  eta: 1 day, 14:00:37  time: 0.4306  data_time: 0.0086  memory: 5180  grad_norm: 93.2541  loss: 13.6291  decode.loss_cls: 0.5445  decode.loss_mask: 0.4421  decode.loss_dice: 0.3492  decode.d0.loss_cls: 1.0814  decode.d0.loss_mask: 0.3390  decode.d0.loss_dice: 0.3459  decode.d1.loss_cls: 0.6959  decode.d1.loss_mask: 0.3447  decode.d1.loss_dice: 0.3399  decode.d2.loss_cls: 0.6994  decode.d2.loss_mask: 0.3574  decode.d2.loss_dice: 0.3443  decode.d3.loss_cls: 0.5862  decode.d3.loss_mask: 0.3510  decode.d3.loss_dice: 0.3421  decode.d4.loss_cls: 0.5901  decode.d4.loss_mask: 0.3590  decode.d4.loss_dice: 0.3498  decode.d5.loss_cls: 0.6296  decode.d5.loss_mask: 0.3818  decode.d5.loss_dice: 0.3386  decode.d6.loss_cls: 0.5379  decode.d6.loss_mask: 0.3582  decode.d6.loss_dice: 0.3327  decode.d7.loss_cls: 0.5691  decode.d7.loss_mask: 0.3751  decode.d7.loss_dice: 0.3475  decode.d8.loss_cls: 0.5857  decode.d8.loss_mask: 0.3806  decode.d8.loss_dice: 0.3305
09/30 09:35:34 - mmengine - INFO - Iter(train) [  4500/320000]  base_lr: 9.8734e-05 lr: 9.8734e-06  eta: 1 day, 14:00:08  time: 0.4314  data_time: 0.0085  memory: 5180  grad_norm: 147.3155  loss: 12.9305  decode.loss_cls: 0.4486  decode.loss_mask: 0.4694  decode.loss_dice: 0.3700  decode.d0.loss_cls: 1.1313  decode.d0.loss_mask: 0.3326  decode.d0.loss_dice: 0.3327  decode.d1.loss_cls: 0.5528  decode.d1.loss_mask: 0.4004  decode.d1.loss_dice: 0.3261  decode.d2.loss_cls: 0.5404  decode.d2.loss_mask: 0.3714  decode.d2.loss_dice: 0.3233  decode.d3.loss_cls: 0.5308  decode.d3.loss_mask: 0.3753  decode.d3.loss_dice: 0.3241  decode.d4.loss_cls: 0.5179  decode.d4.loss_mask: 0.4340  decode.d4.loss_dice: 0.3281  decode.d5.loss_cls: 0.4968  decode.d5.loss_mask: 0.3300  decode.d5.loss_dice: 0.3479  decode.d6.loss_cls: 0.4770  decode.d6.loss_mask: 0.4529  decode.d6.loss_dice: 0.3345  decode.d7.loss_cls: 0.5037  decode.d7.loss_mask: 0.3145  decode.d7.loss_dice: 0.3562  decode.d8.loss_cls: 0.4968  decode.d8.loss_mask: 0.3861  decode.d8.loss_dice: 0.3248
09/30 09:35:56 - mmengine - INFO - Iter(train) [  4550/320000]  base_lr: 9.8720e-05 lr: 9.8720e-06  eta: 1 day, 13:59:52  time: 0.4314  data_time: 0.0087  memory: 5180  grad_norm: 169.9313  loss: 17.3008  decode.loss_cls: 0.8106  decode.loss_mask: 0.5367  decode.loss_dice: 0.3637  decode.d0.loss_cls: 1.4537  decode.d0.loss_mask: 0.4091  decode.d0.loss_dice: 0.3749  decode.d1.loss_cls: 0.9833  decode.d1.loss_mask: 0.4124  decode.d1.loss_dice: 0.3539  decode.d2.loss_cls: 0.7967  decode.d2.loss_mask: 0.4481  decode.d2.loss_dice: 0.3780  decode.d3.loss_cls: 0.7859  decode.d3.loss_mask: 0.5348  decode.d3.loss_dice: 0.4098  decode.d4.loss_cls: 0.8514  decode.d4.loss_mask: 0.4445  decode.d4.loss_dice: 0.3599  decode.d5.loss_cls: 0.7886  decode.d5.loss_mask: 0.4284  decode.d5.loss_dice: 0.3631  decode.d6.loss_cls: 0.7462  decode.d6.loss_mask: 0.4581  decode.d6.loss_dice: 0.4142  decode.d7.loss_cls: 0.6714  decode.d7.loss_mask: 0.6002  decode.d7.loss_dice: 0.3991  decode.d8.loss_cls: 0.7961  decode.d8.loss_mask: 0.5432  decode.d8.loss_dice: 0.3845
09/30 09:36:17 - mmengine - INFO - Iter(train) [  4600/320000]  base_lr: 9.8706e-05 lr: 9.8706e-06  eta: 1 day, 13:59:23  time: 0.4327  data_time: 0.0089  memory: 5161  grad_norm: 147.1791  loss: 13.6418  decode.loss_cls: 0.6005  decode.loss_mask: 0.3206  decode.loss_dice: 0.3054  decode.d0.loss_cls: 1.2173  decode.d0.loss_mask: 0.3189  decode.d0.loss_dice: 0.3323  decode.d1.loss_cls: 0.7340  decode.d1.loss_mask: 0.3192  decode.d1.loss_dice: 0.3097  decode.d2.loss_cls: 0.6712  decode.d2.loss_mask: 0.3191  decode.d2.loss_dice: 0.2938  decode.d3.loss_cls: 0.6389  decode.d3.loss_mask: 0.3113  decode.d3.loss_dice: 0.2975  decode.d4.loss_cls: 0.7215  decode.d4.loss_mask: 0.3274  decode.d4.loss_dice: 0.3284  decode.d5.loss_cls: 0.6952  decode.d5.loss_mask: 0.3513  decode.d5.loss_dice: 0.3303  decode.d6.loss_cls: 0.6273  decode.d6.loss_mask: 0.3344  decode.d6.loss_dice: 0.3366  decode.d7.loss_cls: 0.6543  decode.d7.loss_mask: 0.3286  decode.d7.loss_dice: 0.3200  decode.d8.loss_cls: 0.6717  decode.d8.loss_mask: 0.3210  decode.d8.loss_dice: 0.3039
09/30 09:36:39 - mmengine - INFO - Iter(train) [  4650/320000]  base_lr: 9.8692e-05 lr: 9.8692e-06  eta: 1 day, 13:58:53  time: 0.4301  data_time: 0.0083  memory: 5180  grad_norm: 110.8209  loss: 10.7631  decode.loss_cls: 0.4266  decode.loss_mask: 0.2497  decode.loss_dice: 0.2715  decode.d0.loss_cls: 1.1577  decode.d0.loss_mask: 0.2629  decode.d0.loss_dice: 0.2829  decode.d1.loss_cls: 0.5886  decode.d1.loss_mask: 0.2642  decode.d1.loss_dice: 0.2835  decode.d2.loss_cls: 0.4743  decode.d2.loss_mask: 0.2463  decode.d2.loss_dice: 0.2777  decode.d3.loss_cls: 0.4290  decode.d3.loss_mask: 0.2474  decode.d3.loss_dice: 0.2784  decode.d4.loss_cls: 0.4407  decode.d4.loss_mask: 0.2462  decode.d4.loss_dice: 0.2916  decode.d5.loss_cls: 0.4955  decode.d5.loss_mask: 0.2389  decode.d5.loss_dice: 0.2667  decode.d6.loss_cls: 0.4804  decode.d6.loss_mask: 0.2763  decode.d6.loss_dice: 0.2986  decode.d7.loss_cls: 0.4413  decode.d7.loss_mask: 0.2635  decode.d7.loss_dice: 0.2889  decode.d8.loss_cls: 0.4706  decode.d8.loss_mask: 0.2551  decode.d8.loss_dice: 0.2680
09/30 09:37:00 - mmengine - INFO - Iter(train) [  4700/320000]  base_lr: 9.8677e-05 lr: 9.8677e-06  eta: 1 day, 13:58:24  time: 0.4311  data_time: 0.0084  memory: 5161  grad_norm: 105.2091  loss: 10.5072  decode.loss_cls: 0.4090  decode.loss_mask: 0.3355  decode.loss_dice: 0.2397  decode.d0.loss_cls: 1.1678  decode.d0.loss_mask: 0.3551  decode.d0.loss_dice: 0.2762  decode.d1.loss_cls: 0.4744  decode.d1.loss_mask: 0.3456  decode.d1.loss_dice: 0.2272  decode.d2.loss_cls: 0.3813  decode.d2.loss_mask: 0.3401  decode.d2.loss_dice: 0.2288  decode.d3.loss_cls: 0.3851  decode.d3.loss_mask: 0.3412  decode.d3.loss_dice: 0.2255  decode.d4.loss_cls: 0.3776  decode.d4.loss_mask: 0.3418  decode.d4.loss_dice: 0.2169  decode.d5.loss_cls: 0.4004  decode.d5.loss_mask: 0.3319  decode.d5.loss_dice: 0.2210  decode.d6.loss_cls: 0.3899  decode.d6.loss_mask: 0.3382  decode.d6.loss_dice: 0.2326  decode.d7.loss_cls: 0.3814  decode.d7.loss_mask: 0.3346  decode.d7.loss_dice: 0.2285  decode.d8.loss_cls: 0.3837  decode.d8.loss_mask: 0.3553  decode.d8.loss_dice: 0.2410
09/30 09:37:22 - mmengine - INFO - Iter(train) [  4750/320000]  base_lr: 9.8663e-05 lr: 9.8663e-06  eta: 1 day, 13:57:53  time: 0.4302  data_time: 0.0088  memory: 5180  grad_norm: 113.8551  loss: 10.9800  decode.loss_cls: 0.4405  decode.loss_mask: 0.2927  decode.loss_dice: 0.2595  decode.d0.loss_cls: 1.1307  decode.d0.loss_mask: 0.3019  decode.d0.loss_dice: 0.3162  decode.d1.loss_cls: 0.6028  decode.d1.loss_mask: 0.2999  decode.d1.loss_dice: 0.2976  decode.d2.loss_cls: 0.4510  decode.d2.loss_mask: 0.2929  decode.d2.loss_dice: 0.2833  decode.d3.loss_cls: 0.4537  decode.d3.loss_mask: 0.2919  decode.d3.loss_dice: 0.2908  decode.d4.loss_cls: 0.4378  decode.d4.loss_mask: 0.2907  decode.d4.loss_dice: 0.2908  decode.d5.loss_cls: 0.4120  decode.d5.loss_mask: 0.2974  decode.d5.loss_dice: 0.2817  decode.d6.loss_cls: 0.4464  decode.d6.loss_mask: 0.2906  decode.d6.loss_dice: 0.2714  decode.d7.loss_cls: 0.4347  decode.d7.loss_mask: 0.2896  decode.d7.loss_dice: 0.2526  decode.d8.loss_cls: 0.4324  decode.d8.loss_mask: 0.2928  decode.d8.loss_dice: 0.2538
09/30 09:37:44 - mmengine - INFO - Iter(train) [  4800/320000]  base_lr: 9.8649e-05 lr: 9.8649e-06  eta: 1 day, 13:57:25  time: 0.4320  data_time: 0.0088  memory: 5180  grad_norm: 166.5702  loss: 10.6259  decode.loss_cls: 0.3872  decode.loss_mask: 0.3172  decode.loss_dice: 0.2426  decode.d0.loss_cls: 1.0243  decode.d0.loss_mask: 0.3529  decode.d0.loss_dice: 0.2819  decode.d1.loss_cls: 0.4289  decode.d1.loss_mask: 0.3678  decode.d1.loss_dice: 0.3032  decode.d2.loss_cls: 0.3448  decode.d2.loss_mask: 0.3245  decode.d2.loss_dice: 0.2427  decode.d3.loss_cls: 0.3826  decode.d3.loss_mask: 0.3243  decode.d3.loss_dice: 0.2387  decode.d4.loss_cls: 0.4114  decode.d4.loss_mask: 0.3925  decode.d4.loss_dice: 0.2510  decode.d5.loss_cls: 0.3558  decode.d5.loss_mask: 0.3904  decode.d5.loss_dice: 0.2674  decode.d6.loss_cls: 0.3772  decode.d6.loss_mask: 0.3850  decode.d6.loss_dice: 0.2564  decode.d7.loss_cls: 0.4441  decode.d7.loss_mask: 0.3139  decode.d7.loss_dice: 0.2506  decode.d8.loss_cls: 0.4032  decode.d8.loss_mask: 0.3170  decode.d8.loss_dice: 0.2462
09/30 09:38:05 - mmengine - INFO - Iter(train) [  4850/320000]  base_lr: 9.8635e-05 lr: 9.8635e-06  eta: 1 day, 13:56:53  time: 0.4308  data_time: 0.0089  memory: 5161  grad_norm: 102.4427  loss: 9.9652  decode.loss_cls: 0.3192  decode.loss_mask: 0.2992  decode.loss_dice: 0.2814  decode.d0.loss_cls: 1.0064  decode.d0.loss_mask: 0.3087  decode.d0.loss_dice: 0.3122  decode.d1.loss_cls: 0.3850  decode.d1.loss_mask: 0.3173  decode.d1.loss_dice: 0.2896  decode.d2.loss_cls: 0.3744  decode.d2.loss_mask: 0.3092  decode.d2.loss_dice: 0.3013  decode.d3.loss_cls: 0.2674  decode.d3.loss_mask: 0.3016  decode.d3.loss_dice: 0.2830  decode.d4.loss_cls: 0.3501  decode.d4.loss_mask: 0.2968  decode.d4.loss_dice: 0.2857  decode.d5.loss_cls: 0.3305  decode.d5.loss_mask: 0.2937  decode.d5.loss_dice: 0.2843  decode.d6.loss_cls: 0.2971  decode.d6.loss_mask: 0.3012  decode.d6.loss_dice: 0.2925  decode.d7.loss_cls: 0.3462  decode.d7.loss_mask: 0.3030  decode.d7.loss_dice: 0.3009  decode.d8.loss_cls: 0.3367  decode.d8.loss_mask: 0.2998  decode.d8.loss_dice: 0.2908
09/30 09:38:27 - mmengine - INFO - Iter(train) [  4900/320000]  base_lr: 9.8621e-05 lr: 9.8621e-06  eta: 1 day, 13:56:25  time: 0.4314  data_time: 0.0086  memory: 5161  grad_norm: 224.9407  loss: 13.4748  decode.loss_cls: 0.4737  decode.loss_mask: 0.4341  decode.loss_dice: 0.4082  decode.d0.loss_cls: 1.1650  decode.d0.loss_mask: 0.4358  decode.d0.loss_dice: 0.4799  decode.d1.loss_cls: 0.5377  decode.d1.loss_mask: 0.3615  decode.d1.loss_dice: 0.4138  decode.d2.loss_cls: 0.4907  decode.d2.loss_mask: 0.3613  decode.d2.loss_dice: 0.3823  decode.d3.loss_cls: 0.4764  decode.d3.loss_mask: 0.3672  decode.d3.loss_dice: 0.3865  decode.d4.loss_cls: 0.4548  decode.d4.loss_mask: 0.4223  decode.d4.loss_dice: 0.4021  decode.d5.loss_cls: 0.4392  decode.d5.loss_mask: 0.3913  decode.d5.loss_dice: 0.4094  decode.d6.loss_cls: 0.4287  decode.d6.loss_mask: 0.3944  decode.d6.loss_dice: 0.4018  decode.d7.loss_cls: 0.4610  decode.d7.loss_mask: 0.3838  decode.d7.loss_dice: 0.3866  decode.d8.loss_cls: 0.4459  decode.d8.loss_mask: 0.4604  decode.d8.loss_dice: 0.4191
09/30 09:38:48 - mmengine - INFO - Iter(train) [  4950/320000]  base_lr: 9.8607e-05 lr: 9.8607e-06  eta: 1 day, 13:55:57  time: 0.4313  data_time: 0.0087  memory: 5160  grad_norm: 139.3830  loss: 14.7730  decode.loss_cls: 0.5334  decode.loss_mask: 0.4901  decode.loss_dice: 0.3921  decode.d0.loss_cls: 1.1694  decode.d0.loss_mask: 0.3939  decode.d0.loss_dice: 0.4120  decode.d1.loss_cls: 0.6597  decode.d1.loss_mask: 0.3974  decode.d1.loss_dice: 0.3637  decode.d2.loss_cls: 0.6499  decode.d2.loss_mask: 0.4001  decode.d2.loss_dice: 0.3479  decode.d3.loss_cls: 0.6306  decode.d3.loss_mask: 0.4059  decode.d3.loss_dice: 0.3584  decode.d4.loss_cls: 0.6249  decode.d4.loss_mask: 0.4278  decode.d4.loss_dice: 0.3727  decode.d5.loss_cls: 0.5511  decode.d5.loss_mask: 0.4559  decode.d5.loss_dice: 0.3960  decode.d6.loss_cls: 0.6495  decode.d6.loss_mask: 0.4088  decode.d6.loss_dice: 0.3812  decode.d7.loss_cls: 0.5447  decode.d7.loss_mask: 0.5143  decode.d7.loss_dice: 0.3968  decode.d8.loss_cls: 0.5092  decode.d8.loss_mask: 0.5267  decode.d8.loss_dice: 0.4090
09/30 09:39:10 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 09:39:10 - mmengine - INFO - Iter(train) [  5000/320000]  base_lr: 9.8593e-05 lr: 9.8593e-06  eta: 1 day, 13:55:27  time: 0.4311  data_time: 0.0085  memory: 5161  grad_norm: 118.3962  loss: 10.9059  decode.loss_cls: 0.3542  decode.loss_mask: 0.3422  decode.loss_dice: 0.3059  decode.d0.loss_cls: 1.0821  decode.d0.loss_mask: 0.3562  decode.d0.loss_dice: 0.3422  decode.d1.loss_cls: 0.4252  decode.d1.loss_mask: 0.3395  decode.d1.loss_dice: 0.3122  decode.d2.loss_cls: 0.3786  decode.d2.loss_mask: 0.3413  decode.d2.loss_dice: 0.3071  decode.d3.loss_cls: 0.3692  decode.d3.loss_mask: 0.3390  decode.d3.loss_dice: 0.3171  decode.d4.loss_cls: 0.3555  decode.d4.loss_mask: 0.3358  decode.d4.loss_dice: 0.3150  decode.d5.loss_cls: 0.3488  decode.d5.loss_mask: 0.3343  decode.d5.loss_dice: 0.3139  decode.d6.loss_cls: 0.3788  decode.d6.loss_mask: 0.3333  decode.d6.loss_dice: 0.3169  decode.d7.loss_cls: 0.3228  decode.d7.loss_mask: 0.3393  decode.d7.loss_dice: 0.3096  decode.d8.loss_cls: 0.3492  decode.d8.loss_mask: 0.3334  decode.d8.loss_dice: 0.3073
09/30 09:39:31 - mmengine - INFO - Iter(train) [  5050/320000]  base_lr: 9.8579e-05 lr: 9.8579e-06  eta: 1 day, 13:54:58  time: 0.4303  data_time: 0.0086  memory: 5145  grad_norm: 208.3550  loss: 15.0646  decode.loss_cls: 0.5696  decode.loss_mask: 0.4816  decode.loss_dice: 0.3181  decode.d0.loss_cls: 1.2870  decode.d0.loss_mask: 0.5219  decode.d0.loss_dice: 0.3856  decode.d1.loss_cls: 0.6310  decode.d1.loss_mask: 0.4996  decode.d1.loss_dice: 0.3401  decode.d2.loss_cls: 0.6770  decode.d2.loss_mask: 0.4661  decode.d2.loss_dice: 0.3178  decode.d3.loss_cls: 0.6241  decode.d3.loss_mask: 0.4739  decode.d3.loss_dice: 0.3213  decode.d4.loss_cls: 0.5972  decode.d4.loss_mask: 0.4874  decode.d4.loss_dice: 0.3300  decode.d5.loss_cls: 0.5773  decode.d5.loss_mask: 0.4944  decode.d5.loss_dice: 0.3272  decode.d6.loss_cls: 0.5801  decode.d6.loss_mask: 0.4993  decode.d6.loss_dice: 0.3373  decode.d7.loss_cls: 0.6359  decode.d7.loss_mask: 0.5044  decode.d7.loss_dice: 0.3406  decode.d8.loss_cls: 0.5925  decode.d8.loss_mask: 0.5109  decode.d8.loss_dice: 0.3355
09/30 09:39:53 - mmengine - INFO - Iter(train) [  5100/320000]  base_lr: 9.8565e-05 lr: 9.8565e-06  eta: 1 day, 13:54:28  time: 0.4311  data_time: 0.0086  memory: 5180  grad_norm: 150.9204  loss: 11.9668  decode.loss_cls: 0.3814  decode.loss_mask: 0.3749  decode.loss_dice: 0.3543  decode.d0.loss_cls: 1.0692  decode.d0.loss_mask: 0.3837  decode.d0.loss_dice: 0.3634  decode.d1.loss_cls: 0.4281  decode.d1.loss_mask: 0.3597  decode.d1.loss_dice: 0.3746  decode.d2.loss_cls: 0.4229  decode.d2.loss_mask: 0.3649  decode.d2.loss_dice: 0.3562  decode.d3.loss_cls: 0.4621  decode.d3.loss_mask: 0.3613  decode.d3.loss_dice: 0.3465  decode.d4.loss_cls: 0.3929  decode.d4.loss_mask: 0.3583  decode.d4.loss_dice: 0.3369  decode.d5.loss_cls: 0.4614  decode.d5.loss_mask: 0.3475  decode.d5.loss_dice: 0.3303  decode.d6.loss_cls: 0.4274  decode.d6.loss_mask: 0.3704  decode.d6.loss_dice: 0.3608  decode.d7.loss_cls: 0.3845  decode.d7.loss_mask: 0.3788  decode.d7.loss_dice: 0.3434  decode.d8.loss_cls: 0.3688  decode.d8.loss_mask: 0.3537  decode.d8.loss_dice: 0.3486
09/30 09:40:14 - mmengine - INFO - Iter(train) [  5150/320000]  base_lr: 9.8551e-05 lr: 9.8551e-06  eta: 1 day, 13:53:59  time: 0.4307  data_time: 0.0084  memory: 5161  grad_norm: 128.8098  loss: 15.9376  decode.loss_cls: 0.6766  decode.loss_mask: 0.4227  decode.loss_dice: 0.4177  decode.d0.loss_cls: 1.2721  decode.d0.loss_mask: 0.4362  decode.d0.loss_dice: 0.4886  decode.d1.loss_cls: 0.8058  decode.d1.loss_mask: 0.4146  decode.d1.loss_dice: 0.4098  decode.d2.loss_cls: 0.7978  decode.d2.loss_mask: 0.4142  decode.d2.loss_dice: 0.3735  decode.d3.loss_cls: 0.7456  decode.d3.loss_mask: 0.4012  decode.d3.loss_dice: 0.3915  decode.d4.loss_cls: 0.6688  decode.d4.loss_mask: 0.4106  decode.d4.loss_dice: 0.3614  decode.d5.loss_cls: 0.6716  decode.d5.loss_mask: 0.3804  decode.d5.loss_dice: 0.4555  decode.d6.loss_cls: 0.6770  decode.d6.loss_mask: 0.4563  decode.d6.loss_dice: 0.4054  decode.d7.loss_cls: 0.5744  decode.d7.loss_mask: 0.4388  decode.d7.loss_dice: 0.4527  decode.d8.loss_cls: 0.6498  decode.d8.loss_mask: 0.4223  decode.d8.loss_dice: 0.4448
09/30 09:40:36 - mmengine - INFO - Iter(train) [  5200/320000]  base_lr: 9.8537e-05 lr: 9.8537e-06  eta: 1 day, 13:53:32  time: 0.4314  data_time: 0.0086  memory: 5147  grad_norm: 63.2489  loss: 7.1084  decode.loss_cls: 0.2002  decode.loss_mask: 0.2478  decode.loss_dice: 0.1867  decode.d0.loss_cls: 0.9473  decode.d0.loss_mask: 0.2509  decode.d0.loss_dice: 0.2199  decode.d1.loss_cls: 0.2670  decode.d1.loss_mask: 0.2494  decode.d1.loss_dice: 0.1910  decode.d2.loss_cls: 0.2319  decode.d2.loss_mask: 0.2395  decode.d2.loss_dice: 0.1868  decode.d3.loss_cls: 0.1895  decode.d3.loss_mask: 0.2477  decode.d3.loss_dice: 0.1960  decode.d4.loss_cls: 0.1849  decode.d4.loss_mask: 0.2448  decode.d4.loss_dice: 0.1893  decode.d5.loss_cls: 0.1681  decode.d5.loss_mask: 0.2446  decode.d5.loss_dice: 0.1833  decode.d6.loss_cls: 0.1866  decode.d6.loss_mask: 0.2425  decode.d6.loss_dice: 0.1855  decode.d7.loss_cls: 0.1763  decode.d7.loss_mask: 0.2462  decode.d7.loss_dice: 0.1958  decode.d8.loss_cls: 0.1748  decode.d8.loss_mask: 0.2457  decode.d8.loss_dice: 0.1884
09/30 09:40:58 - mmengine - INFO - Iter(train) [  5250/320000]  base_lr: 9.8522e-05 lr: 9.8522e-06  eta: 1 day, 13:53:01  time: 0.4307  data_time: 0.0083  memory: 5180  grad_norm: 76.5303  loss: 8.8515  decode.loss_cls: 0.3061  decode.loss_mask: 0.2446  decode.loss_dice: 0.2371  decode.d0.loss_cls: 0.9918  decode.d0.loss_mask: 0.2522  decode.d0.loss_dice: 0.2381  decode.d1.loss_cls: 0.4944  decode.d1.loss_mask: 0.2424  decode.d1.loss_dice: 0.2317  decode.d2.loss_cls: 0.4302  decode.d2.loss_mask: 0.2438  decode.d2.loss_dice: 0.2183  decode.d3.loss_cls: 0.3673  decode.d3.loss_mask: 0.2409  decode.d3.loss_dice: 0.2220  decode.d4.loss_cls: 0.3326  decode.d4.loss_mask: 0.2416  decode.d4.loss_dice: 0.2286  decode.d5.loss_cls: 0.2983  decode.d5.loss_mask: 0.2596  decode.d5.loss_dice: 0.2313  decode.d6.loss_cls: 0.2997  decode.d6.loss_mask: 0.2465  decode.d6.loss_dice: 0.2247  decode.d7.loss_cls: 0.2842  decode.d7.loss_mask: 0.2446  decode.d7.loss_dice: 0.2324  decode.d8.loss_cls: 0.3081  decode.d8.loss_mask: 0.2428  decode.d8.loss_dice: 0.2156
09/30 09:41:19 - mmengine - INFO - Iter(train) [  5300/320000]  base_lr: 9.8508e-05 lr: 9.8508e-06  eta: 1 day, 13:52:34  time: 0.4319  data_time: 0.0086  memory: 5180  grad_norm: 130.1269  loss: 13.9682  decode.loss_cls: 0.5096  decode.loss_mask: 0.4093  decode.loss_dice: 0.3305  decode.d0.loss_cls: 1.2532  decode.d0.loss_mask: 0.4347  decode.d0.loss_dice: 0.4240  decode.d1.loss_cls: 0.6684  decode.d1.loss_mask: 0.4163  decode.d1.loss_dice: 0.3522  decode.d2.loss_cls: 0.6136  decode.d2.loss_mask: 0.4109  decode.d2.loss_dice: 0.3201  decode.d3.loss_cls: 0.5479  decode.d3.loss_mask: 0.4119  decode.d3.loss_dice: 0.3348  decode.d4.loss_cls: 0.5327  decode.d4.loss_mask: 0.4547  decode.d4.loss_dice: 0.3341  decode.d5.loss_cls: 0.5741  decode.d5.loss_mask: 0.4287  decode.d5.loss_dice: 0.3577  decode.d6.loss_cls: 0.5360  decode.d6.loss_mask: 0.4241  decode.d6.loss_dice: 0.3256  decode.d7.loss_cls: 0.4907  decode.d7.loss_mask: 0.4156  decode.d7.loss_dice: 0.3345  decode.d8.loss_cls: 0.5519  decode.d8.loss_mask: 0.4276  decode.d8.loss_dice: 0.3427
09/30 09:41:41 - mmengine - INFO - Iter(train) [  5350/320000]  base_lr: 9.8494e-05 lr: 9.8494e-06  eta: 1 day, 13:52:05  time: 0.4314  data_time: 0.0088  memory: 5161  grad_norm: 137.6695  loss: 16.4424  decode.loss_cls: 0.6712  decode.loss_mask: 0.3681  decode.loss_dice: 0.4940  decode.d0.loss_cls: 1.4114  decode.d0.loss_mask: 0.3798  decode.d0.loss_dice: 0.6013  decode.d1.loss_cls: 0.6443  decode.d1.loss_mask: 0.3680  decode.d1.loss_dice: 0.5164  decode.d2.loss_cls: 0.6841  decode.d2.loss_mask: 0.3621  decode.d2.loss_dice: 0.5018  decode.d3.loss_cls: 0.6874  decode.d3.loss_mask: 0.3563  decode.d3.loss_dice: 0.5311  decode.d4.loss_cls: 0.6928  decode.d4.loss_mask: 0.3848  decode.d4.loss_dice: 0.5302  decode.d5.loss_cls: 0.6937  decode.d5.loss_mask: 0.3709  decode.d5.loss_dice: 0.5287  decode.d6.loss_cls: 0.6753  decode.d6.loss_mask: 0.3927  decode.d6.loss_dice: 0.5456  decode.d7.loss_cls: 0.6033  decode.d7.loss_mask: 0.4056  decode.d7.loss_dice: 0.5454  decode.d8.loss_cls: 0.5768  decode.d8.loss_mask: 0.3934  decode.d8.loss_dice: 0.5259
09/30 09:42:02 - mmengine - INFO - Iter(train) [  5400/320000]  base_lr: 9.8480e-05 lr: 9.8480e-06  eta: 1 day, 13:51:39  time: 0.4313  data_time: 0.0084  memory: 5180  grad_norm: 134.3266  loss: 10.8005  decode.loss_cls: 0.4069  decode.loss_mask: 0.2835  decode.loss_dice: 0.2663  decode.d0.loss_cls: 1.2978  decode.d0.loss_mask: 0.2692  decode.d0.loss_dice: 0.2976  decode.d1.loss_cls: 0.5004  decode.d1.loss_mask: 0.2628  decode.d1.loss_dice: 0.3045  decode.d2.loss_cls: 0.4354  decode.d2.loss_mask: 0.2551  decode.d2.loss_dice: 0.2910  decode.d3.loss_cls: 0.3983  decode.d3.loss_mask: 0.2743  decode.d3.loss_dice: 0.2908  decode.d4.loss_cls: 0.4669  decode.d4.loss_mask: 0.2875  decode.d4.loss_dice: 0.2894  decode.d5.loss_cls: 0.5403  decode.d5.loss_mask: 0.2670  decode.d5.loss_dice: 0.2458  decode.d6.loss_cls: 0.4316  decode.d6.loss_mask: 0.2670  decode.d6.loss_dice: 0.2666  decode.d7.loss_cls: 0.3978  decode.d7.loss_mask: 0.2961  decode.d7.loss_dice: 0.2461  decode.d8.loss_cls: 0.4289  decode.d8.loss_mask: 0.2888  decode.d8.loss_dice: 0.2467
09/30 09:42:24 - mmengine - INFO - Iter(train) [  5450/320000]  base_lr: 9.8466e-05 lr: 9.8466e-06  eta: 1 day, 13:51:14  time: 0.4316  data_time: 0.0084  memory: 5161  grad_norm: 202.4819  loss: 10.7974  decode.loss_cls: 0.3326  decode.loss_mask: 0.2937  decode.loss_dice: 0.2727  decode.d0.loss_cls: 1.0718  decode.d0.loss_mask: 0.3058  decode.d0.loss_dice: 0.3343  decode.d1.loss_cls: 0.4939  decode.d1.loss_mask: 0.3450  decode.d1.loss_dice: 0.3083  decode.d2.loss_cls: 0.4505  decode.d2.loss_mask: 0.3190  decode.d2.loss_dice: 0.3191  decode.d3.loss_cls: 0.4068  decode.d3.loss_mask: 0.3240  decode.d3.loss_dice: 0.3415  decode.d4.loss_cls: 0.3675  decode.d4.loss_mask: 0.3037  decode.d4.loss_dice: 0.2876  decode.d5.loss_cls: 0.3912  decode.d5.loss_mask: 0.3102  decode.d5.loss_dice: 0.2927  decode.d6.loss_cls: 0.3795  decode.d6.loss_mask: 0.3141  decode.d6.loss_dice: 0.2959  decode.d7.loss_cls: 0.3359  decode.d7.loss_mask: 0.3307  decode.d7.loss_dice: 0.3072  decode.d8.loss_cls: 0.3192  decode.d8.loss_mask: 0.3488  decode.d8.loss_dice: 0.2945
09/30 09:42:45 - mmengine - INFO - Iter(train) [  5500/320000]  base_lr: 9.8452e-05 lr: 9.8452e-06  eta: 1 day, 13:50:48  time: 0.4324  data_time: 0.0089  memory: 5160  grad_norm: 246.3889  loss: 14.0435  decode.loss_cls: 0.7489  decode.loss_mask: 0.4087  decode.loss_dice: 0.2942  decode.d0.loss_cls: 1.1305  decode.d0.loss_mask: 0.3688  decode.d0.loss_dice: 0.3000  decode.d1.loss_cls: 0.8447  decode.d1.loss_mask: 0.3591  decode.d1.loss_dice: 0.2668  decode.d2.loss_cls: 0.7062  decode.d2.loss_mask: 0.3482  decode.d2.loss_dice: 0.2349  decode.d3.loss_cls: 0.7153  decode.d3.loss_mask: 0.3234  decode.d3.loss_dice: 0.2370  decode.d4.loss_cls: 0.7585  decode.d4.loss_mask: 0.3304  decode.d4.loss_dice: 0.2540  decode.d5.loss_cls: 0.6834  decode.d5.loss_mask: 0.3820  decode.d5.loss_dice: 0.2527  decode.d6.loss_cls: 0.7520  decode.d6.loss_mask: 0.3335  decode.d6.loss_dice: 0.2535  decode.d7.loss_cls: 0.7231  decode.d7.loss_mask: 0.3805  decode.d7.loss_dice: 0.2639  decode.d8.loss_cls: 0.7632  decode.d8.loss_mask: 0.3596  decode.d8.loss_dice: 0.2663
09/30 09:43:07 - mmengine - INFO - Iter(train) [  5550/320000]  base_lr: 9.8438e-05 lr: 9.8438e-06  eta: 1 day, 13:50:22  time: 0.4308  data_time: 0.0084  memory: 5147  grad_norm: 180.1155  loss: 13.1373  decode.loss_cls: 0.5869  decode.loss_mask: 0.3221  decode.loss_dice: 0.3383  decode.d0.loss_cls: 1.2573  decode.d0.loss_mask: 0.3166  decode.d0.loss_dice: 0.3650  decode.d1.loss_cls: 0.6401  decode.d1.loss_mask: 0.3181  decode.d1.loss_dice: 0.3328  decode.d2.loss_cls: 0.5747  decode.d2.loss_mask: 0.3120  decode.d2.loss_dice: 0.3083  decode.d3.loss_cls: 0.6036  decode.d3.loss_mask: 0.3163  decode.d3.loss_dice: 0.3230  decode.d4.loss_cls: 0.6036  decode.d4.loss_mask: 0.3128  decode.d4.loss_dice: 0.3179  decode.d5.loss_cls: 0.5976  decode.d5.loss_mask: 0.3130  decode.d5.loss_dice: 0.3286  decode.d6.loss_cls: 0.6096  decode.d6.loss_mask: 0.3142  decode.d6.loss_dice: 0.3319  decode.d7.loss_cls: 0.5951  decode.d7.loss_mask: 0.3140  decode.d7.loss_dice: 0.3277  decode.d8.loss_cls: 0.6203  decode.d8.loss_mask: 0.3151  decode.d8.loss_dice: 0.3208
09/30 09:43:29 - mmengine - INFO - Iter(train) [  5600/320000]  base_lr: 9.8424e-05 lr: 9.8424e-06  eta: 1 day, 13:49:57  time: 0.4321  data_time: 0.0086  memory: 5160  grad_norm: 131.6579  loss: 11.1711  decode.loss_cls: 0.3294  decode.loss_mask: 0.2862  decode.loss_dice: 0.2988  decode.d0.loss_cls: 1.1760  decode.d0.loss_mask: 0.3013  decode.d0.loss_dice: 0.3396  decode.d1.loss_cls: 0.5700  decode.d1.loss_mask: 0.2915  decode.d1.loss_dice: 0.3118  decode.d2.loss_cls: 0.4586  decode.d2.loss_mask: 0.2864  decode.d2.loss_dice: 0.3166  decode.d3.loss_cls: 0.4719  decode.d3.loss_mask: 0.2888  decode.d3.loss_dice: 0.3134  decode.d4.loss_cls: 0.4787  decode.d4.loss_mask: 0.2876  decode.d4.loss_dice: 0.2943  decode.d5.loss_cls: 0.4322  decode.d5.loss_mask: 0.2880  decode.d5.loss_dice: 0.2970  decode.d6.loss_cls: 0.4602  decode.d6.loss_mask: 0.2849  decode.d6.loss_dice: 0.3272  decode.d7.loss_cls: 0.4331  decode.d7.loss_mask: 0.2815  decode.d7.loss_dice: 0.3116  decode.d8.loss_cls: 0.3635  decode.d8.loss_mask: 0.2864  decode.d8.loss_dice: 0.3048
09/30 09:43:50 - mmengine - INFO - Iter(train) [  5650/320000]  base_lr: 9.8410e-05 lr: 9.8410e-06  eta: 1 day, 13:49:32  time: 0.4322  data_time: 0.0088  memory: 5147  grad_norm: 414.8088  loss: 17.7980  decode.loss_cls: 0.5932  decode.loss_mask: 0.5354  decode.loss_dice: 0.5892  decode.d0.loss_cls: 1.2954  decode.d0.loss_mask: 0.5297  decode.d0.loss_dice: 0.5447  decode.d1.loss_cls: 0.6323  decode.d1.loss_mask: 0.5287  decode.d1.loss_dice: 0.5099  decode.d2.loss_cls: 0.7236  decode.d2.loss_mask: 0.5071  decode.d2.loss_dice: 0.5055  decode.d3.loss_cls: 0.6601  decode.d3.loss_mask: 0.5429  decode.d3.loss_dice: 0.5116  decode.d4.loss_cls: 0.6180  decode.d4.loss_mask: 0.5895  decode.d4.loss_dice: 0.4803  decode.d5.loss_cls: 0.6300  decode.d5.loss_mask: 0.6035  decode.d5.loss_dice: 0.4832  decode.d6.loss_cls: 0.5762  decode.d6.loss_mask: 0.6340  decode.d6.loss_dice: 0.5478  decode.d7.loss_cls: 0.5402  decode.d7.loss_mask: 0.6144  decode.d7.loss_dice: 0.5396  decode.d8.loss_cls: 0.5772  decode.d8.loss_mask: 0.5869  decode.d8.loss_dice: 0.5679
09/30 09:44:12 - mmengine - INFO - Iter(train) [  5700/320000]  base_lr: 9.8396e-05 lr: 9.8396e-06  eta: 1 day, 13:49:05  time: 0.4308  data_time: 0.0083  memory: 5180  grad_norm: 136.4175  loss: 12.3492  decode.loss_cls: 0.5541  decode.loss_mask: 0.3322  decode.loss_dice: 0.3381  decode.d0.loss_cls: 1.0454  decode.d0.loss_mask: 0.3216  decode.d0.loss_dice: 0.3231  decode.d1.loss_cls: 0.5880  decode.d1.loss_mask: 0.3130  decode.d1.loss_dice: 0.3035  decode.d2.loss_cls: 0.5545  decode.d2.loss_mask: 0.3347  decode.d2.loss_dice: 0.3067  decode.d3.loss_cls: 0.5377  decode.d3.loss_mask: 0.3317  decode.d3.loss_dice: 0.3125  decode.d4.loss_cls: 0.5370  decode.d4.loss_mask: 0.3062  decode.d4.loss_dice: 0.3065  decode.d5.loss_cls: 0.4790  decode.d5.loss_mask: 0.3095  decode.d5.loss_dice: 0.3138  decode.d6.loss_cls: 0.5773  decode.d6.loss_mask: 0.3359  decode.d6.loss_dice: 0.3323  decode.d7.loss_cls: 0.5577  decode.d7.loss_mask: 0.3235  decode.d7.loss_dice: 0.3085  decode.d8.loss_cls: 0.5479  decode.d8.loss_mask: 0.3069  decode.d8.loss_dice: 0.3104
09/30 09:44:33 - mmengine - INFO - Iter(train) [  5750/320000]  base_lr: 9.8382e-05 lr: 9.8382e-06  eta: 1 day, 13:48:39  time: 0.4327  data_time: 0.0088  memory: 5180  grad_norm: 105.4045  loss: 11.2715  decode.loss_cls: 0.4459  decode.loss_mask: 0.2937  decode.loss_dice: 0.2645  decode.d0.loss_cls: 1.2879  decode.d0.loss_mask: 0.2770  decode.d0.loss_dice: 0.2431  decode.d1.loss_cls: 0.5240  decode.d1.loss_mask: 0.3114  decode.d1.loss_dice: 0.2576  decode.d2.loss_cls: 0.4737  decode.d2.loss_mask: 0.2923  decode.d2.loss_dice: 0.2457  decode.d3.loss_cls: 0.4344  decode.d3.loss_mask: 0.3466  decode.d3.loss_dice: 0.2436  decode.d4.loss_cls: 0.4892  decode.d4.loss_mask: 0.3136  decode.d4.loss_dice: 0.2406  decode.d5.loss_cls: 0.4368  decode.d5.loss_mask: 0.3348  decode.d5.loss_dice: 0.2465  decode.d6.loss_cls: 0.5410  decode.d6.loss_mask: 0.3122  decode.d6.loss_dice: 0.2326  decode.d7.loss_cls: 0.5341  decode.d7.loss_mask: 0.3256  decode.d7.loss_dice: 0.2616  decode.d8.loss_cls: 0.4970  decode.d8.loss_mask: 0.3202  decode.d8.loss_dice: 0.2443
09/30 09:44:55 - mmengine - INFO - Iter(train) [  5800/320000]  base_lr: 9.8368e-05 lr: 9.8368e-06  eta: 1 day, 13:48:11  time: 0.4303  data_time: 0.0085  memory: 5180  grad_norm: 76.6728  loss: 11.2290  decode.loss_cls: 0.3751  decode.loss_mask: 0.3765  decode.loss_dice: 0.3095  decode.d0.loss_cls: 1.1343  decode.d0.loss_mask: 0.3812  decode.d0.loss_dice: 0.3417  decode.d1.loss_cls: 0.4308  decode.d1.loss_mask: 0.3279  decode.d1.loss_dice: 0.2706  decode.d2.loss_cls: 0.4006  decode.d2.loss_mask: 0.3411  decode.d2.loss_dice: 0.2802  decode.d3.loss_cls: 0.4168  decode.d3.loss_mask: 0.3628  decode.d3.loss_dice: 0.3039  decode.d4.loss_cls: 0.3983  decode.d4.loss_mask: 0.3783  decode.d4.loss_dice: 0.2883  decode.d5.loss_cls: 0.3687  decode.d5.loss_mask: 0.3398  decode.d5.loss_dice: 0.2930  decode.d6.loss_cls: 0.3920  decode.d6.loss_mask: 0.3227  decode.d6.loss_dice: 0.2947  decode.d7.loss_cls: 0.3747  decode.d7.loss_mask: 0.3338  decode.d7.loss_dice: 0.2940  decode.d8.loss_cls: 0.4613  decode.d8.loss_mask: 0.3411  decode.d8.loss_dice: 0.2955
09/30 09:45:17 - mmengine - INFO - Iter(train) [  5850/320000]  base_lr: 9.8353e-05 lr: 9.8353e-06  eta: 1 day, 13:47:46  time: 0.4317  data_time: 0.0085  memory: 5161  grad_norm: 145.4820  loss: 15.1146  decode.loss_cls: 0.6805  decode.loss_mask: 0.3831  decode.loss_dice: 0.3490  decode.d0.loss_cls: 1.3751  decode.d0.loss_mask: 0.4107  decode.d0.loss_dice: 0.4035  decode.d1.loss_cls: 0.7370  decode.d1.loss_mask: 0.3865  decode.d1.loss_dice: 0.3640  decode.d2.loss_cls: 0.7127  decode.d2.loss_mask: 0.3864  decode.d2.loss_dice: 0.3529  decode.d3.loss_cls: 0.7183  decode.d3.loss_mask: 0.3958  decode.d3.loss_dice: 0.3608  decode.d4.loss_cls: 0.6652  decode.d4.loss_mask: 0.4170  decode.d4.loss_dice: 0.3779  decode.d5.loss_cls: 0.6760  decode.d5.loss_mask: 0.3915  decode.d5.loss_dice: 0.3605  decode.d6.loss_cls: 0.6260  decode.d6.loss_mask: 0.3904  decode.d6.loss_dice: 0.3645  decode.d7.loss_cls: 0.6544  decode.d7.loss_mask: 0.3954  decode.d7.loss_dice: 0.3362  decode.d8.loss_cls: 0.7065  decode.d8.loss_mask: 0.3922  decode.d8.loss_dice: 0.3444
09/30 09:45:38 - mmengine - INFO - Iter(train) [  5900/320000]  base_lr: 9.8339e-05 lr: 9.8339e-06  eta: 1 day, 13:47:21  time: 0.4325  data_time: 0.0088  memory: 5147  grad_norm: 115.8059  loss: 13.7664  decode.loss_cls: 0.6180  decode.loss_mask: 0.4143  decode.loss_dice: 0.3505  decode.d0.loss_cls: 1.2047  decode.d0.loss_mask: 0.4020  decode.d0.loss_dice: 0.3760  decode.d1.loss_cls: 0.5772  decode.d1.loss_mask: 0.3916  decode.d1.loss_dice: 0.3380  decode.d2.loss_cls: 0.6750  decode.d2.loss_mask: 0.3874  decode.d2.loss_dice: 0.3346  decode.d3.loss_cls: 0.5034  decode.d3.loss_mask: 0.3868  decode.d3.loss_dice: 0.3413  decode.d4.loss_cls: 0.5432  decode.d4.loss_mask: 0.3908  decode.d4.loss_dice: 0.3449  decode.d5.loss_cls: 0.5298  decode.d5.loss_mask: 0.3932  decode.d5.loss_dice: 0.3404  decode.d6.loss_cls: 0.5418  decode.d6.loss_mask: 0.4004  decode.d6.loss_dice: 0.3299  decode.d7.loss_cls: 0.5680  decode.d7.loss_mask: 0.4013  decode.d7.loss_dice: 0.3420  decode.d8.loss_cls: 0.5825  decode.d8.loss_mask: 0.4119  decode.d8.loss_dice: 0.3455
09/30 09:46:00 - mmengine - INFO - Iter(train) [  5950/320000]  base_lr: 9.8325e-05 lr: 9.8325e-06  eta: 1 day, 13:46:55  time: 0.4305  data_time: 0.0087  memory: 5147  grad_norm: 83.6968  loss: 11.5326  decode.loss_cls: 0.4911  decode.loss_mask: 0.3133  decode.loss_dice: 0.2731  decode.d0.loss_cls: 1.1223  decode.d0.loss_mask: 0.3526  decode.d0.loss_dice: 0.3107  decode.d1.loss_cls: 0.5404  decode.d1.loss_mask: 0.3331  decode.d1.loss_dice: 0.2611  decode.d2.loss_cls: 0.5112  decode.d2.loss_mask: 0.3291  decode.d2.loss_dice: 0.2625  decode.d3.loss_cls: 0.4333  decode.d3.loss_mask: 0.3223  decode.d3.loss_dice: 0.2816  decode.d4.loss_cls: 0.5366  decode.d4.loss_mask: 0.3190  decode.d4.loss_dice: 0.2750  decode.d5.loss_cls: 0.4314  decode.d5.loss_mask: 0.3177  decode.d5.loss_dice: 0.2940  decode.d6.loss_cls: 0.4754  decode.d6.loss_mask: 0.3102  decode.d6.loss_dice: 0.2766  decode.d7.loss_cls: 0.5000  decode.d7.loss_mask: 0.3073  decode.d7.loss_dice: 0.2718  decode.d8.loss_cls: 0.5111  decode.d8.loss_mask: 0.3069  decode.d8.loss_dice: 0.2620
09/30 09:46:21 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 09:46:21 - mmengine - INFO - Iter(train) [  6000/320000]  base_lr: 9.8311e-05 lr: 9.8311e-06  eta: 1 day, 13:46:31  time: 0.4315  data_time: 0.0087  memory: 5161  grad_norm: 145.3175  loss: 9.8418  decode.loss_cls: 0.3233  decode.loss_mask: 0.3618  decode.loss_dice: 0.2862  decode.d0.loss_cls: 0.9600  decode.d0.loss_mask: 0.3429  decode.d0.loss_dice: 0.2945  decode.d1.loss_cls: 0.3544  decode.d1.loss_mask: 0.3243  decode.d1.loss_dice: 0.2624  decode.d2.loss_cls: 0.2891  decode.d2.loss_mask: 0.3199  decode.d2.loss_dice: 0.2411  decode.d3.loss_cls: 0.2836  decode.d3.loss_mask: 0.3322  decode.d3.loss_dice: 0.2597  decode.d4.loss_cls: 0.3328  decode.d4.loss_mask: 0.3248  decode.d4.loss_dice: 0.2455  decode.d5.loss_cls: 0.3646  decode.d5.loss_mask: 0.3185  decode.d5.loss_dice: 0.2282  decode.d6.loss_cls: 0.3365  decode.d6.loss_mask: 0.3092  decode.d6.loss_dice: 0.2278  decode.d7.loss_cls: 0.4144  decode.d7.loss_mask: 0.3170  decode.d7.loss_dice: 0.2413  decode.d8.loss_cls: 0.3992  decode.d8.loss_mask: 0.3105  decode.d8.loss_dice: 0.2362
09/30 09:46:43 - mmengine - INFO - Iter(train) [  6050/320000]  base_lr: 9.8297e-05 lr: 9.8297e-06  eta: 1 day, 13:46:04  time: 0.4314  data_time: 0.0088  memory: 5161  grad_norm: 53.4686  loss: 8.0178  decode.loss_cls: 0.2337  decode.loss_mask: 0.2766  decode.loss_dice: 0.2118  decode.d0.loss_cls: 0.9437  decode.d0.loss_mask: 0.2757  decode.d0.loss_dice: 0.2208  decode.d1.loss_cls: 0.2566  decode.d1.loss_mask: 0.2725  decode.d1.loss_dice: 0.2052  decode.d2.loss_cls: 0.2662  decode.d2.loss_mask: 0.2793  decode.d2.loss_dice: 0.2148  decode.d3.loss_cls: 0.2226  decode.d3.loss_mask: 0.2808  decode.d3.loss_dice: 0.2174  decode.d4.loss_cls: 0.2220  decode.d4.loss_mask: 0.2761  decode.d4.loss_dice: 0.2136  decode.d5.loss_cls: 0.2264  decode.d5.loss_mask: 0.2755  decode.d5.loss_dice: 0.2116  decode.d6.loss_cls: 0.1968  decode.d6.loss_mask: 0.2861  decode.d6.loss_dice: 0.2246  decode.d7.loss_cls: 0.2477  decode.d7.loss_mask: 0.2893  decode.d7.loss_dice: 0.2146  decode.d8.loss_cls: 0.2639  decode.d8.loss_mask: 0.2786  decode.d8.loss_dice: 0.2133
09/30 09:47:04 - mmengine - INFO - Iter(train) [  6100/320000]  base_lr: 9.8283e-05 lr: 9.8283e-06  eta: 1 day, 13:45:38  time: 0.4310  data_time: 0.0089  memory: 5161  grad_norm: 127.5403  loss: 10.7678  decode.loss_cls: 0.2060  decode.loss_mask: 0.3228  decode.loss_dice: 0.3576  decode.d0.loss_cls: 1.0330  decode.d0.loss_mask: 0.3293  decode.d0.loss_dice: 0.3614  decode.d1.loss_cls: 0.3681  decode.d1.loss_mask: 0.3276  decode.d1.loss_dice: 0.3579  decode.d2.loss_cls: 0.4147  decode.d2.loss_mask: 0.3241  decode.d2.loss_dice: 0.3476  decode.d3.loss_cls: 0.4176  decode.d3.loss_mask: 0.3204  decode.d3.loss_dice: 0.3291  decode.d4.loss_cls: 0.4290  decode.d4.loss_mask: 0.3184  decode.d4.loss_dice: 0.3204  decode.d5.loss_cls: 0.3517  decode.d5.loss_mask: 0.3243  decode.d5.loss_dice: 0.3485  decode.d6.loss_cls: 0.2827  decode.d6.loss_mask: 0.3252  decode.d6.loss_dice: 0.3644  decode.d7.loss_cls: 0.2926  decode.d7.loss_mask: 0.3217  decode.d7.loss_dice: 0.3345  decode.d8.loss_cls: 0.2722  decode.d8.loss_mask: 0.3208  decode.d8.loss_dice: 0.3442
09/30 09:47:26 - mmengine - INFO - Iter(train) [  6150/320000]  base_lr: 9.8269e-05 lr: 9.8269e-06  eta: 1 day, 13:45:12  time: 0.4322  data_time: 0.0089  memory: 5161  grad_norm: 162.8263  loss: 12.9645  decode.loss_cls: 0.5925  decode.loss_mask: 0.3169  decode.loss_dice: 0.2800  decode.d0.loss_cls: 1.3549  decode.d0.loss_mask: 0.3494  decode.d0.loss_dice: 0.3770  decode.d1.loss_cls: 0.6438  decode.d1.loss_mask: 0.3299  decode.d1.loss_dice: 0.2977  decode.d2.loss_cls: 0.5686  decode.d2.loss_mask: 0.3191  decode.d2.loss_dice: 0.3014  decode.d3.loss_cls: 0.5911  decode.d3.loss_mask: 0.3345  decode.d3.loss_dice: 0.3115  decode.d4.loss_cls: 0.5080  decode.d4.loss_mask: 0.3215  decode.d4.loss_dice: 0.2909  decode.d5.loss_cls: 0.5511  decode.d5.loss_mask: 0.3112  decode.d5.loss_dice: 0.2981  decode.d6.loss_cls: 0.6181  decode.d6.loss_mask: 0.3087  decode.d6.loss_dice: 0.2859  decode.d7.loss_cls: 0.5771  decode.d7.loss_mask: 0.3348  decode.d7.loss_dice: 0.3497  decode.d8.loss_cls: 0.5613  decode.d8.loss_mask: 0.3586  decode.d8.loss_dice: 0.3211
09/30 09:47:48 - mmengine - INFO - Iter(train) [  6200/320000]  base_lr: 9.8255e-05 lr: 9.8255e-06  eta: 1 day, 13:44:55  time: 0.4306  data_time: 0.0090  memory: 5180  grad_norm: 65.7736  loss: 9.5850  decode.loss_cls: 0.4217  decode.loss_mask: 0.2972  decode.loss_dice: 0.2136  decode.d0.loss_cls: 0.9823  decode.d0.loss_mask: 0.3064  decode.d0.loss_dice: 0.2281  decode.d1.loss_cls: 0.4358  decode.d1.loss_mask: 0.2929  decode.d1.loss_dice: 0.2164  decode.d2.loss_cls: 0.3676  decode.d2.loss_mask: 0.2949  decode.d2.loss_dice: 0.2146  decode.d3.loss_cls: 0.3288  decode.d3.loss_mask: 0.2964  decode.d3.loss_dice: 0.2092  decode.d4.loss_cls: 0.3438  decode.d4.loss_mask: 0.2937  decode.d4.loss_dice: 0.2167  decode.d5.loss_cls: 0.4127  decode.d5.loss_mask: 0.2994  decode.d5.loss_dice: 0.2180  decode.d6.loss_cls: 0.3866  decode.d6.loss_mask: 0.3033  decode.d6.loss_dice: 0.2147  decode.d7.loss_cls: 0.3942  decode.d7.loss_mask: 0.2953  decode.d7.loss_dice: 0.2133  decode.d8.loss_cls: 0.3751  decode.d8.loss_mask: 0.2941  decode.d8.loss_dice: 0.2182
09/30 09:48:09 - mmengine - INFO - Iter(train) [  6250/320000]  base_lr: 9.8241e-05 lr: 9.8241e-06  eta: 1 day, 13:44:30  time: 0.4317  data_time: 0.0089  memory: 5161  grad_norm: 127.9812  loss: 9.1581  decode.loss_cls: 0.2673  decode.loss_mask: 0.2701  decode.loss_dice: 0.2262  decode.d0.loss_cls: 0.9351  decode.d0.loss_mask: 0.2977  decode.d0.loss_dice: 0.2738  decode.d1.loss_cls: 0.3617  decode.d1.loss_mask: 0.2865  decode.d1.loss_dice: 0.2386  decode.d2.loss_cls: 0.3780  decode.d2.loss_mask: 0.2829  decode.d2.loss_dice: 0.2333  decode.d3.loss_cls: 0.2211  decode.d3.loss_mask: 0.3979  decode.d3.loss_dice: 0.2443  decode.d4.loss_cls: 0.2774  decode.d4.loss_mask: 0.2910  decode.d4.loss_dice: 0.2526  decode.d5.loss_cls: 0.2047  decode.d5.loss_mask: 0.3885  decode.d5.loss_dice: 0.2476  decode.d6.loss_cls: 0.2266  decode.d6.loss_mask: 0.3860  decode.d6.loss_dice: 0.2497  decode.d7.loss_cls: 0.2248  decode.d7.loss_mask: 0.3888  decode.d7.loss_dice: 0.2480  decode.d8.loss_cls: 0.2094  decode.d8.loss_mask: 0.3913  decode.d8.loss_dice: 0.2573
09/30 09:48:31 - mmengine - INFO - Iter(train) [  6300/320000]  base_lr: 9.8227e-05 lr: 9.8227e-06  eta: 1 day, 13:44:05  time: 0.4326  data_time: 0.0091  memory: 5161  grad_norm: 262.0398  loss: 13.3340  decode.loss_cls: 0.4546  decode.loss_mask: 0.3939  decode.loss_dice: 0.3758  decode.d0.loss_cls: 1.1966  decode.d0.loss_mask: 0.4272  decode.d0.loss_dice: 0.4353  decode.d1.loss_cls: 0.5643  decode.d1.loss_mask: 0.3962  decode.d1.loss_dice: 0.3882  decode.d2.loss_cls: 0.4577  decode.d2.loss_mask: 0.3814  decode.d2.loss_dice: 0.3592  decode.d3.loss_cls: 0.4254  decode.d3.loss_mask: 0.3910  decode.d3.loss_dice: 0.3885  decode.d4.loss_cls: 0.4451  decode.d4.loss_mask: 0.4254  decode.d4.loss_dice: 0.4016  decode.d5.loss_cls: 0.4716  decode.d5.loss_mask: 0.3927  decode.d5.loss_dice: 0.3931  decode.d6.loss_cls: 0.4311  decode.d6.loss_mask: 0.4025  decode.d6.loss_dice: 0.4085  decode.d7.loss_cls: 0.3953  decode.d7.loss_mask: 0.4134  decode.d7.loss_dice: 0.4230  decode.d8.loss_cls: 0.4739  decode.d8.loss_mask: 0.4041  decode.d8.loss_dice: 0.4175
09/30 09:48:52 - mmengine - INFO - Iter(train) [  6350/320000]  base_lr: 9.8213e-05 lr: 9.8213e-06  eta: 1 day, 13:43:39  time: 0.4310  data_time: 0.0089  memory: 5161  grad_norm: 66.4563  loss: 9.2992  decode.loss_cls: 0.2631  decode.loss_mask: 0.2785  decode.loss_dice: 0.2739  decode.d0.loss_cls: 1.0334  decode.d0.loss_mask: 0.2873  decode.d0.loss_dice: 0.3085  decode.d1.loss_cls: 0.4045  decode.d1.loss_mask: 0.2801  decode.d1.loss_dice: 0.2777  decode.d2.loss_cls: 0.2660  decode.d2.loss_mask: 0.2782  decode.d2.loss_dice: 0.2726  decode.d3.loss_cls: 0.2906  decode.d3.loss_mask: 0.2754  decode.d3.loss_dice: 0.2688  decode.d4.loss_cls: 0.3177  decode.d4.loss_mask: 0.2802  decode.d4.loss_dice: 0.2875  decode.d5.loss_cls: 0.2951  decode.d5.loss_mask: 0.2842  decode.d5.loss_dice: 0.2815  decode.d6.loss_cls: 0.3390  decode.d6.loss_mask: 0.2819  decode.d6.loss_dice: 0.2781  decode.d7.loss_cls: 0.2564  decode.d7.loss_mask: 0.2764  decode.d7.loss_dice: 0.2832  decode.d8.loss_cls: 0.2344  decode.d8.loss_mask: 0.2748  decode.d8.loss_dice: 0.2702
09/30 09:49:14 - mmengine - INFO - Iter(train) [  6400/320000]  base_lr: 9.8198e-05 lr: 9.8198e-06  eta: 1 day, 13:43:14  time: 0.4312  data_time: 0.0089  memory: 5160  grad_norm: 167.7359  loss: 12.2773  decode.loss_cls: 0.4658  decode.loss_mask: 0.3886  decode.loss_dice: 0.2923  decode.d0.loss_cls: 1.1538  decode.d0.loss_mask: 0.3738  decode.d0.loss_dice: 0.2821  decode.d1.loss_cls: 0.5694  decode.d1.loss_mask: 0.3930  decode.d1.loss_dice: 0.3233  decode.d2.loss_cls: 0.5645  decode.d2.loss_mask: 0.3685  decode.d2.loss_dice: 0.2875  decode.d3.loss_cls: 0.4499  decode.d3.loss_mask: 0.3890  decode.d3.loss_dice: 0.3227  decode.d4.loss_cls: 0.4563  decode.d4.loss_mask: 0.3993  decode.d4.loss_dice: 0.3033  decode.d5.loss_cls: 0.4647  decode.d5.loss_mask: 0.3866  decode.d5.loss_dice: 0.3086  decode.d6.loss_cls: 0.4865  decode.d6.loss_mask: 0.3273  decode.d6.loss_dice: 0.2863  decode.d7.loss_cls: 0.4696  decode.d7.loss_mask: 0.3462  decode.d7.loss_dice: 0.3042  decode.d8.loss_cls: 0.4762  decode.d8.loss_mask: 0.3707  decode.d8.loss_dice: 0.2675
09/30 09:49:36 - mmengine - INFO - Iter(train) [  6450/320000]  base_lr: 9.8184e-05 lr: 9.8184e-06  eta: 1 day, 13:42:49  time: 0.4313  data_time: 0.0089  memory: 5160  grad_norm: 100.7800  loss: 8.7326  decode.loss_cls: 0.2915  decode.loss_mask: 0.2833  decode.loss_dice: 0.2229  decode.d0.loss_cls: 1.0441  decode.d0.loss_mask: 0.2932  decode.d0.loss_dice: 0.2495  decode.d1.loss_cls: 0.3069  decode.d1.loss_mask: 0.2763  decode.d1.loss_dice: 0.2177  decode.d2.loss_cls: 0.3103  decode.d2.loss_mask: 0.2826  decode.d2.loss_dice: 0.2247  decode.d3.loss_cls: 0.3082  decode.d3.loss_mask: 0.2797  decode.d3.loss_dice: 0.2273  decode.d4.loss_cls: 0.2610  decode.d4.loss_mask: 0.2758  decode.d4.loss_dice: 0.2315  decode.d5.loss_cls: 0.3057  decode.d5.loss_mask: 0.2789  decode.d5.loss_dice: 0.2212  decode.d6.loss_cls: 0.2743  decode.d6.loss_mask: 0.2786  decode.d6.loss_dice: 0.2247  decode.d7.loss_cls: 0.2822  decode.d7.loss_mask: 0.2850  decode.d7.loss_dice: 0.2165  decode.d8.loss_cls: 0.2762  decode.d8.loss_mask: 0.2841  decode.d8.loss_dice: 0.2187
09/30 09:49:57 - mmengine - INFO - Iter(train) [  6500/320000]  base_lr: 9.8170e-05 lr: 9.8170e-06  eta: 1 day, 13:42:23  time: 0.4308  data_time: 0.0087  memory: 5180  grad_norm: 129.3610  loss: 11.8732  decode.loss_cls: 0.4940  decode.loss_mask: 0.2961  decode.loss_dice: 0.3016  decode.d0.loss_cls: 1.2622  decode.d0.loss_mask: 0.2825  decode.d0.loss_dice: 0.2777  decode.d1.loss_cls: 0.5308  decode.d1.loss_mask: 0.2545  decode.d1.loss_dice: 0.2750  decode.d2.loss_cls: 0.5065  decode.d2.loss_mask: 0.2588  decode.d2.loss_dice: 0.2506  decode.d3.loss_cls: 0.5244  decode.d3.loss_mask: 0.2714  decode.d3.loss_dice: 0.3108  decode.d4.loss_cls: 0.5156  decode.d4.loss_mask: 0.3224  decode.d4.loss_dice: 0.3404  decode.d5.loss_cls: 0.4771  decode.d5.loss_mask: 0.3519  decode.d5.loss_dice: 0.3286  decode.d6.loss_cls: 0.5501  decode.d6.loss_mask: 0.3116  decode.d6.loss_dice: 0.3163  decode.d7.loss_cls: 0.5394  decode.d7.loss_mask: 0.3001  decode.d7.loss_dice: 0.3072  decode.d8.loss_cls: 0.4994  decode.d8.loss_mask: 0.3156  decode.d8.loss_dice: 0.3003
09/30 09:50:19 - mmengine - INFO - Iter(train) [  6550/320000]  base_lr: 9.8156e-05 lr: 9.8156e-06  eta: 1 day, 13:41:58  time: 0.4315  data_time: 0.0089  memory: 5180  grad_norm: 221.6001  loss: 14.8832  decode.loss_cls: 0.6006  decode.loss_mask: 0.4039  decode.loss_dice: 0.3675  decode.d0.loss_cls: 1.2144  decode.d0.loss_mask: 0.4094  decode.d0.loss_dice: 0.4019  decode.d1.loss_cls: 0.7311  decode.d1.loss_mask: 0.4182  decode.d1.loss_dice: 0.3948  decode.d2.loss_cls: 0.6158  decode.d2.loss_mask: 0.4268  decode.d2.loss_dice: 0.3900  decode.d3.loss_cls: 0.5863  decode.d3.loss_mask: 0.4064  decode.d3.loss_dice: 0.3710  decode.d4.loss_cls: 0.6116  decode.d4.loss_mask: 0.4228  decode.d4.loss_dice: 0.3793  decode.d5.loss_cls: 0.6445  decode.d5.loss_mask: 0.3890  decode.d5.loss_dice: 0.3849  decode.d6.loss_cls: 0.6258  decode.d6.loss_mask: 0.4084  decode.d6.loss_dice: 0.3727  decode.d7.loss_cls: 0.6145  decode.d7.loss_mask: 0.4029  decode.d7.loss_dice: 0.3925  decode.d8.loss_cls: 0.6937  decode.d8.loss_mask: 0.4117  decode.d8.loss_dice: 0.3908
09/30 09:50:40 - mmengine - INFO - Iter(train) [  6600/320000]  base_lr: 9.8142e-05 lr: 9.8142e-06  eta: 1 day, 13:41:33  time: 0.4323  data_time: 0.0087  memory: 5161  grad_norm: 62.7527  loss: 10.9258  decode.loss_cls: 0.3856  decode.loss_mask: 0.2696  decode.loss_dice: 0.2930  decode.d0.loss_cls: 1.1836  decode.d0.loss_mask: 0.2746  decode.d0.loss_dice: 0.3168  decode.d1.loss_cls: 0.5481  decode.d1.loss_mask: 0.2782  decode.d1.loss_dice: 0.3089  decode.d2.loss_cls: 0.4510  decode.d2.loss_mask: 0.2858  decode.d2.loss_dice: 0.2754  decode.d3.loss_cls: 0.4309  decode.d3.loss_mask: 0.3069  decode.d3.loss_dice: 0.2901  decode.d4.loss_cls: 0.4352  decode.d4.loss_mask: 0.2847  decode.d4.loss_dice: 0.3056  decode.d5.loss_cls: 0.4247  decode.d5.loss_mask: 0.2840  decode.d5.loss_dice: 0.3253  decode.d6.loss_cls: 0.3967  decode.d6.loss_mask: 0.2858  decode.d6.loss_dice: 0.2829  decode.d7.loss_cls: 0.4288  decode.d7.loss_mask: 0.2937  decode.d7.loss_dice: 0.2875  decode.d8.loss_cls: 0.4164  decode.d8.loss_mask: 0.2776  decode.d8.loss_dice: 0.2985
09/30 09:51:02 - mmengine - INFO - Iter(train) [  6650/320000]  base_lr: 9.8128e-05 lr: 9.8128e-06  eta: 1 day, 13:41:08  time: 0.4316  data_time: 0.0088  memory: 5179  grad_norm: 148.5881  loss: 10.5330  decode.loss_cls: 0.3636  decode.loss_mask: 0.3099  decode.loss_dice: 0.2611  decode.d0.loss_cls: 1.1232  decode.d0.loss_mask: 0.3122  decode.d0.loss_dice: 0.2503  decode.d1.loss_cls: 0.4848  decode.d1.loss_mask: 0.3017  decode.d1.loss_dice: 0.2370  decode.d2.loss_cls: 0.3607  decode.d2.loss_mask: 0.3062  decode.d2.loss_dice: 0.2416  decode.d3.loss_cls: 0.4257  decode.d3.loss_mask: 0.3018  decode.d3.loss_dice: 0.2366  decode.d4.loss_cls: 0.4592  decode.d4.loss_mask: 0.3082  decode.d4.loss_dice: 0.2573  decode.d5.loss_cls: 0.4278  decode.d5.loss_mask: 0.3155  decode.d5.loss_dice: 0.2477  decode.d6.loss_cls: 0.4349  decode.d6.loss_mask: 0.3261  decode.d6.loss_dice: 0.2576  decode.d7.loss_cls: 0.4333  decode.d7.loss_mask: 0.3287  decode.d7.loss_dice: 0.2599  decode.d8.loss_cls: 0.3800  decode.d8.loss_mask: 0.3188  decode.d8.loss_dice: 0.2616
09/30 09:51:24 - mmengine - INFO - Iter(train) [  6700/320000]  base_lr: 9.8114e-05 lr: 9.8114e-06  eta: 1 day, 13:40:43  time: 0.4310  data_time: 0.0088  memory: 5146  grad_norm: 65.5738  loss: 8.8802  decode.loss_cls: 0.2194  decode.loss_mask: 0.2848  decode.loss_dice: 0.2733  decode.d0.loss_cls: 1.0540  decode.d0.loss_mask: 0.2897  decode.d0.loss_dice: 0.2705  decode.d1.loss_cls: 0.3182  decode.d1.loss_mask: 0.2842  decode.d1.loss_dice: 0.2851  decode.d2.loss_cls: 0.2652  decode.d2.loss_mask: 0.2842  decode.d2.loss_dice: 0.2729  decode.d3.loss_cls: 0.2658  decode.d3.loss_mask: 0.2860  decode.d3.loss_dice: 0.2724  decode.d4.loss_cls: 0.2776  decode.d4.loss_mask: 0.2827  decode.d4.loss_dice: 0.2629  decode.d5.loss_cls: 0.2343  decode.d5.loss_mask: 0.2813  decode.d5.loss_dice: 0.2649  decode.d6.loss_cls: 0.2293  decode.d6.loss_mask: 0.2828  decode.d6.loss_dice: 0.2712  decode.d7.loss_cls: 0.2171  decode.d7.loss_mask: 0.2789  decode.d7.loss_dice: 0.2635  decode.d8.loss_cls: 0.2811  decode.d8.loss_mask: 0.2775  decode.d8.loss_dice: 0.2497
09/30 09:51:45 - mmengine - INFO - Iter(train) [  6750/320000]  base_lr: 9.8100e-05 lr: 9.8100e-06  eta: 1 day, 13:40:19  time: 0.4317  data_time: 0.0089  memory: 5161  grad_norm: 102.9873  loss: 12.9235  decode.loss_cls: 0.5351  decode.loss_mask: 0.3177  decode.loss_dice: 0.2966  decode.d0.loss_cls: 1.1421  decode.d0.loss_mask: 0.3387  decode.d0.loss_dice: 0.3671  decode.d1.loss_cls: 0.6966  decode.d1.loss_mask: 0.3227  decode.d1.loss_dice: 0.3165  decode.d2.loss_cls: 0.6215  decode.d2.loss_mask: 0.3273  decode.d2.loss_dice: 0.2950  decode.d3.loss_cls: 0.6034  decode.d3.loss_mask: 0.3140  decode.d3.loss_dice: 0.2993  decode.d4.loss_cls: 0.4832  decode.d4.loss_mask: 0.3376  decode.d4.loss_dice: 0.3206  decode.d5.loss_cls: 0.6187  decode.d5.loss_mask: 0.3445  decode.d5.loss_dice: 0.3352  decode.d6.loss_cls: 0.5106  decode.d6.loss_mask: 0.3723  decode.d6.loss_dice: 0.3147  decode.d7.loss_cls: 0.5677  decode.d7.loss_mask: 0.3217  decode.d7.loss_dice: 0.3164  decode.d8.loss_cls: 0.6084  decode.d8.loss_mask: 0.3747  decode.d8.loss_dice: 0.3035
09/30 09:52:07 - mmengine - INFO - Iter(train) [  6800/320000]  base_lr: 9.8086e-05 lr: 9.8086e-06  eta: 1 day, 13:39:55  time: 0.4317  data_time: 0.0084  memory: 5161  grad_norm: 171.3397  loss: 15.6275  decode.loss_cls: 0.5077  decode.loss_mask: 0.4636  decode.loss_dice: 0.5206  decode.d0.loss_cls: 1.3178  decode.d0.loss_mask: 0.4472  decode.d0.loss_dice: 0.4698  decode.d1.loss_cls: 0.6649  decode.d1.loss_mask: 0.4477  decode.d1.loss_dice: 0.4704  decode.d2.loss_cls: 0.6319  decode.d2.loss_mask: 0.4106  decode.d2.loss_dice: 0.4395  decode.d3.loss_cls: 0.5908  decode.d3.loss_mask: 0.4184  decode.d3.loss_dice: 0.4712  decode.d4.loss_cls: 0.5854  decode.d4.loss_mask: 0.4032  decode.d4.loss_dice: 0.4665  decode.d5.loss_cls: 0.5165  decode.d5.loss_mask: 0.4307  decode.d5.loss_dice: 0.4892  decode.d6.loss_cls: 0.5220  decode.d6.loss_mask: 0.4305  decode.d6.loss_dice: 0.5054  decode.d7.loss_cls: 0.5166  decode.d7.loss_mask: 0.4742  decode.d7.loss_dice: 0.5131  decode.d8.loss_cls: 0.5392  decode.d8.loss_mask: 0.4356  decode.d8.loss_dice: 0.5273
09/30 09:52:28 - mmengine - INFO - Iter(train) [  6850/320000]  base_lr: 9.8072e-05 lr: 9.8072e-06  eta: 1 day, 13:39:30  time: 0.4322  data_time: 0.0089  memory: 5180  grad_norm: 85.5046  loss: 9.8315  decode.loss_cls: 0.2713  decode.loss_mask: 0.3560  decode.loss_dice: 0.2534  decode.d0.loss_cls: 0.9461  decode.d0.loss_mask: 0.3563  decode.d0.loss_dice: 0.2712  decode.d1.loss_cls: 0.2774  decode.d1.loss_mask: 0.3547  decode.d1.loss_dice: 0.2692  decode.d2.loss_cls: 0.3197  decode.d2.loss_mask: 0.3516  decode.d2.loss_dice: 0.2690  decode.d3.loss_cls: 0.3046  decode.d3.loss_mask: 0.3583  decode.d3.loss_dice: 0.2570  decode.d4.loss_cls: 0.3157  decode.d4.loss_mask: 0.3592  decode.d4.loss_dice: 0.2554  decode.d5.loss_cls: 0.3099  decode.d5.loss_mask: 0.3534  decode.d5.loss_dice: 0.2528  decode.d6.loss_cls: 0.3007  decode.d6.loss_mask: 0.3495  decode.d6.loss_dice: 0.2506  decode.d7.loss_cls: 0.2948  decode.d7.loss_mask: 0.3475  decode.d7.loss_dice: 0.2525  decode.d8.loss_cls: 0.3717  decode.d8.loss_mask: 0.3487  decode.d8.loss_dice: 0.2532
09/30 09:52:50 - mmengine - INFO - Iter(train) [  6900/320000]  base_lr: 9.8058e-05 lr: 9.8058e-06  eta: 1 day, 13:39:06  time: 0.4325  data_time: 0.0090  memory: 5147  grad_norm: 152.8977  loss: 12.4570  decode.loss_cls: 0.4914  decode.loss_mask: 0.3475  decode.loss_dice: 0.3077  decode.d0.loss_cls: 1.2566  decode.d0.loss_mask: 0.3640  decode.d0.loss_dice: 0.3355  decode.d1.loss_cls: 0.6049  decode.d1.loss_mask: 0.3334  decode.d1.loss_dice: 0.2513  decode.d2.loss_cls: 0.5878  decode.d2.loss_mask: 0.3477  decode.d2.loss_dice: 0.2925  decode.d3.loss_cls: 0.5489  decode.d3.loss_mask: 0.3232  decode.d3.loss_dice: 0.2967  decode.d4.loss_cls: 0.5743  decode.d4.loss_mask: 0.3514  decode.d4.loss_dice: 0.2866  decode.d5.loss_cls: 0.5708  decode.d5.loss_mask: 0.3356  decode.d5.loss_dice: 0.2941  decode.d6.loss_cls: 0.5172  decode.d6.loss_mask: 0.3470  decode.d6.loss_dice: 0.2990  decode.d7.loss_cls: 0.4719  decode.d7.loss_mask: 0.3316  decode.d7.loss_dice: 0.2760  decode.d8.loss_cls: 0.4892  decode.d8.loss_mask: 0.3331  decode.d8.loss_dice: 0.2902
09/30 09:53:11 - mmengine - INFO - Iter(train) [  6950/320000]  base_lr: 9.8043e-05 lr: 9.8043e-06  eta: 1 day, 13:38:41  time: 0.4318  data_time: 0.0089  memory: 5180  grad_norm: 145.8885  loss: 10.6777  decode.loss_cls: 0.3498  decode.loss_mask: 0.3702  decode.loss_dice: 0.2767  decode.d0.loss_cls: 1.0199  decode.d0.loss_mask: 0.3889  decode.d0.loss_dice: 0.2837  decode.d1.loss_cls: 0.3576  decode.d1.loss_mask: 0.3783  decode.d1.loss_dice: 0.2750  decode.d2.loss_cls: 0.3615  decode.d2.loss_mask: 0.3773  decode.d2.loss_dice: 0.2620  decode.d3.loss_cls: 0.3554  decode.d3.loss_mask: 0.3790  decode.d3.loss_dice: 0.2631  decode.d4.loss_cls: 0.4263  decode.d4.loss_mask: 0.3754  decode.d4.loss_dice: 0.2770  decode.d5.loss_cls: 0.3669  decode.d5.loss_mask: 0.3743  decode.d5.loss_dice: 0.2753  decode.d6.loss_cls: 0.2819  decode.d6.loss_mask: 0.3847  decode.d6.loss_dice: 0.2744  decode.d7.loss_cls: 0.2995  decode.d7.loss_mask: 0.3869  decode.d7.loss_dice: 0.2831  decode.d8.loss_cls: 0.3293  decode.d8.loss_mask: 0.3725  decode.d8.loss_dice: 0.2721
09/30 09:53:33 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 09:53:33 - mmengine - INFO - Iter(train) [  7000/320000]  base_lr: 9.8029e-05 lr: 9.8029e-06  eta: 1 day, 13:38:16  time: 0.4315  data_time: 0.0089  memory: 5180  grad_norm: 109.3999  loss: 10.5864  decode.loss_cls: 0.2445  decode.loss_mask: 0.3528  decode.loss_dice: 0.3328  decode.d0.loss_cls: 1.0136  decode.d0.loss_mask: 0.3662  decode.d0.loss_dice: 0.3444  decode.d1.loss_cls: 0.3803  decode.d1.loss_mask: 0.3397  decode.d1.loss_dice: 0.3196  decode.d2.loss_cls: 0.3607  decode.d2.loss_mask: 0.3286  decode.d2.loss_dice: 0.3278  decode.d3.loss_cls: 0.3249  decode.d3.loss_mask: 0.3387  decode.d3.loss_dice: 0.3283  decode.d4.loss_cls: 0.3364  decode.d4.loss_mask: 0.3312  decode.d4.loss_dice: 0.3201  decode.d5.loss_cls: 0.3129  decode.d5.loss_mask: 0.3282  decode.d5.loss_dice: 0.3146  decode.d6.loss_cls: 0.2914  decode.d6.loss_mask: 0.3370  decode.d6.loss_dice: 0.3220  decode.d7.loss_cls: 0.2969  decode.d7.loss_mask: 0.3401  decode.d7.loss_dice: 0.3205  decode.d8.loss_cls: 0.3091  decode.d8.loss_mask: 0.3730  decode.d8.loss_dice: 0.3501
09/30 09:53:55 - mmengine - INFO - Iter(train) [  7050/320000]  base_lr: 9.8015e-05 lr: 9.8015e-06  eta: 1 day, 13:37:51  time: 0.4315  data_time: 0.0090  memory: 5147  grad_norm: 147.2681  loss: 10.2749  decode.loss_cls: 0.3425  decode.loss_mask: 0.3317  decode.loss_dice: 0.2928  decode.d0.loss_cls: 0.9254  decode.d0.loss_mask: 0.3329  decode.d0.loss_dice: 0.3020  decode.d1.loss_cls: 0.3083  decode.d1.loss_mask: 0.3485  decode.d1.loss_dice: 0.3344  decode.d2.loss_cls: 0.3769  decode.d2.loss_mask: 0.3369  decode.d2.loss_dice: 0.3169  decode.d3.loss_cls: 0.3892  decode.d3.loss_mask: 0.3298  decode.d3.loss_dice: 0.2954  decode.d4.loss_cls: 0.3684  decode.d4.loss_mask: 0.3279  decode.d4.loss_dice: 0.2806  decode.d5.loss_cls: 0.2824  decode.d5.loss_mask: 0.3357  decode.d5.loss_dice: 0.3093  decode.d6.loss_cls: 0.2518  decode.d6.loss_mask: 0.3361  decode.d6.loss_dice: 0.3069  decode.d7.loss_cls: 0.2621  decode.d7.loss_mask: 0.3421  decode.d7.loss_dice: 0.3210  decode.d8.loss_cls: 0.3329  decode.d8.loss_mask: 0.3360  decode.d8.loss_dice: 0.3181
09/30 09:54:16 - mmengine - INFO - Iter(train) [  7100/320000]  base_lr: 9.8001e-05 lr: 9.8001e-06  eta: 1 day, 13:37:29  time: 0.4314  data_time: 0.0087  memory: 5161  grad_norm: 113.2437  loss: 11.5161  decode.loss_cls: 0.4437  decode.loss_mask: 0.3052  decode.loss_dice: 0.2402  decode.d0.loss_cls: 1.2480  decode.d0.loss_mask: 0.3143  decode.d0.loss_dice: 0.2911  decode.d1.loss_cls: 0.6316  decode.d1.loss_mask: 0.2998  decode.d1.loss_dice: 0.2506  decode.d2.loss_cls: 0.5386  decode.d2.loss_mask: 0.2974  decode.d2.loss_dice: 0.2536  decode.d3.loss_cls: 0.5479  decode.d3.loss_mask: 0.2959  decode.d3.loss_dice: 0.2432  decode.d4.loss_cls: 0.4800  decode.d4.loss_mask: 0.2979  decode.d4.loss_dice: 0.2409  decode.d5.loss_cls: 0.5718  decode.d5.loss_mask: 0.2966  decode.d5.loss_dice: 0.2483  decode.d6.loss_cls: 0.4871  decode.d6.loss_mask: 0.3083  decode.d6.loss_dice: 0.2579  decode.d7.loss_cls: 0.5318  decode.d7.loss_mask: 0.3099  decode.d7.loss_dice: 0.2569  decode.d8.loss_cls: 0.4799  decode.d8.loss_mask: 0.2988  decode.d8.loss_dice: 0.2489
09/30 09:54:38 - mmengine - INFO - Iter(train) [  7150/320000]  base_lr: 9.7987e-05 lr: 9.7987e-06  eta: 1 day, 13:37:06  time: 0.4317  data_time: 0.0087  memory: 5160  grad_norm: 57.0392  loss: 7.4340  decode.loss_cls: 0.1360  decode.loss_mask: 0.3001  decode.loss_dice: 0.2007  decode.d0.loss_cls: 0.9345  decode.d0.loss_mask: 0.3068  decode.d0.loss_dice: 0.2218  decode.d1.loss_cls: 0.1938  decode.d1.loss_mask: 0.3008  decode.d1.loss_dice: 0.1983  decode.d2.loss_cls: 0.1660  decode.d2.loss_mask: 0.3045  decode.d2.loss_dice: 0.2056  decode.d3.loss_cls: 0.2097  decode.d3.loss_mask: 0.3017  decode.d3.loss_dice: 0.2168  decode.d4.loss_cls: 0.1381  decode.d4.loss_mask: 0.2994  decode.d4.loss_dice: 0.2080  decode.d5.loss_cls: 0.1541  decode.d5.loss_mask: 0.3033  decode.d5.loss_dice: 0.1941  decode.d6.loss_cls: 0.1320  decode.d6.loss_mask: 0.3003  decode.d6.loss_dice: 0.1944  decode.d7.loss_cls: 0.1544  decode.d7.loss_mask: 0.2941  decode.d7.loss_dice: 0.1961  decode.d8.loss_cls: 0.1521  decode.d8.loss_mask: 0.3022  decode.d8.loss_dice: 0.2146
09/30 09:54:59 - mmengine - INFO - Iter(train) [  7200/320000]  base_lr: 9.7973e-05 lr: 9.7973e-06  eta: 1 day, 13:36:42  time: 0.4311  data_time: 0.0086  memory: 5180  grad_norm: 314.7445  loss: 9.9521  decode.loss_cls: 0.3376  decode.loss_mask: 0.4030  decode.loss_dice: 0.3260  decode.d0.loss_cls: 0.9300  decode.d0.loss_mask: 0.3347  decode.d0.loss_dice: 0.2786  decode.d1.loss_cls: 0.3476  decode.d1.loss_mask: 0.3106  decode.d1.loss_dice: 0.2651  decode.d2.loss_cls: 0.3147  decode.d2.loss_mask: 0.3458  decode.d2.loss_dice: 0.3138  decode.d3.loss_cls: 0.3353  decode.d3.loss_mask: 0.2951  decode.d3.loss_dice: 0.2723  decode.d4.loss_cls: 0.3300  decode.d4.loss_mask: 0.3076  decode.d4.loss_dice: 0.2691  decode.d5.loss_cls: 0.3160  decode.d5.loss_mask: 0.2957  decode.d5.loss_dice: 0.2818  decode.d6.loss_cls: 0.3132  decode.d6.loss_mask: 0.2965  decode.d6.loss_dice: 0.2749  decode.d7.loss_cls: 0.3194  decode.d7.loss_mask: 0.2954  decode.d7.loss_dice: 0.2827  decode.d8.loss_cls: 0.3672  decode.d8.loss_mask: 0.3101  decode.d8.loss_dice: 0.2823
09/30 09:55:21 - mmengine - INFO - Iter(train) [  7250/320000]  base_lr: 9.7959e-05 lr: 9.7959e-06  eta: 1 day, 13:36:18  time: 0.4320  data_time: 0.0088  memory: 5146  grad_norm: 144.4174  loss: 8.9024  decode.loss_cls: 0.2384  decode.loss_mask: 0.3395  decode.loss_dice: 0.2525  decode.d0.loss_cls: 0.9361  decode.d0.loss_mask: 0.3442  decode.d0.loss_dice: 0.3220  decode.d1.loss_cls: 0.3946  decode.d1.loss_mask: 0.3339  decode.d1.loss_dice: 0.2402  decode.d2.loss_cls: 0.2540  decode.d2.loss_mask: 0.3206  decode.d2.loss_dice: 0.2364  decode.d3.loss_cls: 0.2195  decode.d3.loss_mask: 0.3287  decode.d3.loss_dice: 0.2501  decode.d4.loss_cls: 0.1976  decode.d4.loss_mask: 0.3334  decode.d4.loss_dice: 0.2478  decode.d5.loss_cls: 0.2080  decode.d5.loss_mask: 0.3180  decode.d5.loss_dice: 0.2672  decode.d6.loss_cls: 0.2173  decode.d6.loss_mask: 0.3184  decode.d6.loss_dice: 0.2337  decode.d7.loss_cls: 0.2226  decode.d7.loss_mask: 0.3116  decode.d7.loss_dice: 0.2365  decode.d8.loss_cls: 0.2350  decode.d8.loss_mask: 0.3118  decode.d8.loss_dice: 0.2326
09/30 09:55:43 - mmengine - INFO - Iter(train) [  7300/320000]  base_lr: 9.7945e-05 lr: 9.7945e-06  eta: 1 day, 13:35:55  time: 0.4319  data_time: 0.0088  memory: 5161  grad_norm: 90.5937  loss: 10.5240  decode.loss_cls: 0.4635  decode.loss_mask: 0.2707  decode.loss_dice: 0.2341  decode.d0.loss_cls: 1.0281  decode.d0.loss_mask: 0.2806  decode.d0.loss_dice: 0.2635  decode.d1.loss_cls: 0.6684  decode.d1.loss_mask: 0.2654  decode.d1.loss_dice: 0.2182  decode.d2.loss_cls: 0.4309  decode.d2.loss_mask: 0.2703  decode.d2.loss_dice: 0.2344  decode.d3.loss_cls: 0.4130  decode.d3.loss_mask: 0.2725  decode.d3.loss_dice: 0.2354  decode.d4.loss_cls: 0.4635  decode.d4.loss_mask: 0.2717  decode.d4.loss_dice: 0.2315  decode.d5.loss_cls: 0.4720  decode.d5.loss_mask: 0.2720  decode.d5.loss_dice: 0.2281  decode.d6.loss_cls: 0.5354  decode.d6.loss_mask: 0.2695  decode.d6.loss_dice: 0.2248  decode.d7.loss_cls: 0.5464  decode.d7.loss_mask: 0.2706  decode.d7.loss_dice: 0.2271  decode.d8.loss_cls: 0.4340  decode.d8.loss_mask: 0.2759  decode.d8.loss_dice: 0.2524
09/30 09:56:04 - mmengine - INFO - Iter(train) [  7350/320000]  base_lr: 9.7931e-05 lr: 9.7931e-06  eta: 1 day, 13:35:30  time: 0.4307  data_time: 0.0085  memory: 5160  grad_norm: 236.0372  loss: 10.3647  decode.loss_cls: 0.4117  decode.loss_mask: 0.2902  decode.loss_dice: 0.2490  decode.d0.loss_cls: 1.0652  decode.d0.loss_mask: 0.2967  decode.d0.loss_dice: 0.3037  decode.d1.loss_cls: 0.3719  decode.d1.loss_mask: 0.2899  decode.d1.loss_dice: 0.2736  decode.d2.loss_cls: 0.4451  decode.d2.loss_mask: 0.2812  decode.d2.loss_dice: 0.2398  decode.d3.loss_cls: 0.3298  decode.d3.loss_mask: 0.2926  decode.d3.loss_dice: 0.2543  decode.d4.loss_cls: 0.3029  decode.d4.loss_mask: 0.4152  decode.d4.loss_dice: 0.2730  decode.d5.loss_cls: 0.3564  decode.d5.loss_mask: 0.4067  decode.d5.loss_dice: 0.2736  decode.d6.loss_cls: 0.3274  decode.d6.loss_mask: 0.3873  decode.d6.loss_dice: 0.2526  decode.d7.loss_cls: 0.2661  decode.d7.loss_mask: 0.3925  decode.d7.loss_dice: 0.2713  decode.d8.loss_cls: 0.3985  decode.d8.loss_mask: 0.3823  decode.d8.loss_dice: 0.2644
09/30 09:56:26 - mmengine - INFO - Iter(train) [  7400/320000]  base_lr: 9.7917e-05 lr: 9.7917e-06  eta: 1 day, 13:35:06  time: 0.4320  data_time: 0.0089  memory: 5160  grad_norm: 276.5490  loss: 10.1769  decode.loss_cls: 0.3866  decode.loss_mask: 0.2792  decode.loss_dice: 0.2372  decode.d0.loss_cls: 1.2370  decode.d0.loss_mask: 0.2802  decode.d0.loss_dice: 0.2843  decode.d1.loss_cls: 0.4749  decode.d1.loss_mask: 0.2757  decode.d1.loss_dice: 0.2230  decode.d2.loss_cls: 0.3760  decode.d2.loss_mask: 0.2792  decode.d2.loss_dice: 0.2433  decode.d3.loss_cls: 0.4202  decode.d3.loss_mask: 0.2801  decode.d3.loss_dice: 0.2480  decode.d4.loss_cls: 0.4280  decode.d4.loss_mask: 0.2741  decode.d4.loss_dice: 0.2437  decode.d5.loss_cls: 0.4317  decode.d5.loss_mask: 0.2745  decode.d5.loss_dice: 0.2503  decode.d6.loss_cls: 0.4168  decode.d6.loss_mask: 0.2765  decode.d6.loss_dice: 0.2441  decode.d7.loss_cls: 0.3966  decode.d7.loss_mask: 0.2819  decode.d7.loss_dice: 0.2579  decode.d8.loss_cls: 0.3711  decode.d8.loss_mask: 0.2752  decode.d8.loss_dice: 0.2297
09/30 09:56:47 - mmengine - INFO - Iter(train) [  7450/320000]  base_lr: 9.7903e-05 lr: 9.7903e-06  eta: 1 day, 13:34:40  time: 0.4305  data_time: 0.0088  memory: 5180  grad_norm: 129.0048  loss: 11.0283  decode.loss_cls: 0.3998  decode.loss_mask: 0.3680  decode.loss_dice: 0.3667  decode.d0.loss_cls: 0.9388  decode.d0.loss_mask: 0.3696  decode.d0.loss_dice: 0.3551  decode.d1.loss_cls: 0.3896  decode.d1.loss_mask: 0.3400  decode.d1.loss_dice: 0.3130  decode.d2.loss_cls: 0.3080  decode.d2.loss_mask: 0.3385  decode.d2.loss_dice: 0.3084  decode.d3.loss_cls: 0.3241  decode.d3.loss_mask: 0.3412  decode.d3.loss_dice: 0.3348  decode.d4.loss_cls: 0.3507  decode.d4.loss_mask: 0.3419  decode.d4.loss_dice: 0.3395  decode.d5.loss_cls: 0.3204  decode.d5.loss_mask: 0.3477  decode.d5.loss_dice: 0.3323  decode.d6.loss_cls: 0.3303  decode.d6.loss_mask: 0.3542  decode.d6.loss_dice: 0.3296  decode.d7.loss_cls: 0.3604  decode.d7.loss_mask: 0.3602  decode.d7.loss_dice: 0.3512  decode.d8.loss_cls: 0.4402  decode.d8.loss_mask: 0.3396  decode.d8.loss_dice: 0.3343
09/30 09:57:09 - mmengine - INFO - Iter(train) [  7500/320000]  base_lr: 9.7888e-05 lr: 9.7888e-06  eta: 1 day, 13:34:16  time: 0.4318  data_time: 0.0085  memory: 5160  grad_norm: 87.6360  loss: 10.3941  decode.loss_cls: 0.3292  decode.loss_mask: 0.3116  decode.loss_dice: 0.3318  decode.d0.loss_cls: 0.9452  decode.d0.loss_mask: 0.3084  decode.d0.loss_dice: 0.3400  decode.d1.loss_cls: 0.3135  decode.d1.loss_mask: 0.3118  decode.d1.loss_dice: 0.3323  decode.d2.loss_cls: 0.3122  decode.d2.loss_mask: 0.3199  decode.d2.loss_dice: 0.3409  decode.d3.loss_cls: 0.3814  decode.d3.loss_mask: 0.3109  decode.d3.loss_dice: 0.3367  decode.d4.loss_cls: 0.3244  decode.d4.loss_mask: 0.3108  decode.d4.loss_dice: 0.3297  decode.d5.loss_cls: 0.3597  decode.d5.loss_mask: 0.3063  decode.d5.loss_dice: 0.3315  decode.d6.loss_cls: 0.3445  decode.d6.loss_mask: 0.3057  decode.d6.loss_dice: 0.3235  decode.d7.loss_cls: 0.3036  decode.d7.loss_mask: 0.3031  decode.d7.loss_dice: 0.3385  decode.d8.loss_cls: 0.3334  decode.d8.loss_mask: 0.3046  decode.d8.loss_dice: 0.3488
09/30 09:57:31 - mmengine - INFO - Iter(train) [  7550/320000]  base_lr: 9.7874e-05 lr: 9.7874e-06  eta: 1 day, 13:33:52  time: 0.4317  data_time: 0.0087  memory: 5160  grad_norm: 55.8370  loss: 7.5868  decode.loss_cls: 0.2928  decode.loss_mask: 0.2350  decode.loss_dice: 0.2012  decode.d0.loss_cls: 0.8727  decode.d0.loss_mask: 0.2233  decode.d0.loss_dice: 0.2185  decode.d1.loss_cls: 0.3544  decode.d1.loss_mask: 0.2144  decode.d1.loss_dice: 0.1756  decode.d2.loss_cls: 0.2828  decode.d2.loss_mask: 0.2186  decode.d2.loss_dice: 0.1854  decode.d3.loss_cls: 0.2695  decode.d3.loss_mask: 0.2206  decode.d3.loss_dice: 0.1913  decode.d4.loss_cls: 0.2202  decode.d4.loss_mask: 0.2328  decode.d4.loss_dice: 0.1926  decode.d5.loss_cls: 0.2597  decode.d5.loss_mask: 0.2340  decode.d5.loss_dice: 0.1901  decode.d6.loss_cls: 0.3028  decode.d6.loss_mask: 0.2209  decode.d6.loss_dice: 0.1910  decode.d7.loss_cls: 0.2836  decode.d7.loss_mask: 0.2226  decode.d7.loss_dice: 0.1915  decode.d8.loss_cls: 0.2863  decode.d8.loss_mask: 0.2217  decode.d8.loss_dice: 0.1809
09/30 09:57:52 - mmengine - INFO - Iter(train) [  7600/320000]  base_lr: 9.7860e-05 lr: 9.7860e-06  eta: 1 day, 13:33:28  time: 0.4318  data_time: 0.0089  memory: 5180  grad_norm: 125.3559  loss: 11.9220  decode.loss_cls: 0.3509  decode.loss_mask: 0.4932  decode.loss_dice: 0.3564  decode.d0.loss_cls: 1.0545  decode.d0.loss_mask: 0.3880  decode.d0.loss_dice: 0.3654  decode.d1.loss_cls: 0.4180  decode.d1.loss_mask: 0.4035  decode.d1.loss_dice: 0.3400  decode.d2.loss_cls: 0.3837  decode.d2.loss_mask: 0.3726  decode.d2.loss_dice: 0.3164  decode.d3.loss_cls: 0.3398  decode.d3.loss_mask: 0.4131  decode.d3.loss_dice: 0.3396  decode.d4.loss_cls: 0.3548  decode.d4.loss_mask: 0.4114  decode.d4.loss_dice: 0.3697  decode.d5.loss_cls: 0.3644  decode.d5.loss_mask: 0.3876  decode.d5.loss_dice: 0.3467  decode.d6.loss_cls: 0.3948  decode.d6.loss_mask: 0.4013  decode.d6.loss_dice: 0.3713  decode.d7.loss_cls: 0.3669  decode.d7.loss_mask: 0.3955  decode.d7.loss_dice: 0.3591  decode.d8.loss_cls: 0.3426  decode.d8.loss_mask: 0.3823  decode.d8.loss_dice: 0.3387
09/30 09:58:14 - mmengine - INFO - Iter(train) [  7650/320000]  base_lr: 9.7846e-05 lr: 9.7846e-06  eta: 1 day, 13:33:03  time: 0.4315  data_time: 0.0089  memory: 5146  grad_norm: 117.9898  loss: 12.1156  decode.loss_cls: 0.5052  decode.loss_mask: 0.3359  decode.loss_dice: 0.3797  decode.d0.loss_cls: 1.0475  decode.d0.loss_mask: 0.3488  decode.d0.loss_dice: 0.3884  decode.d1.loss_cls: 0.4472  decode.d1.loss_mask: 0.3356  decode.d1.loss_dice: 0.3898  decode.d2.loss_cls: 0.5015  decode.d2.loss_mask: 0.3240  decode.d2.loss_dice: 0.3770  decode.d3.loss_cls: 0.4057  decode.d3.loss_mask: 0.3286  decode.d3.loss_dice: 0.3882  decode.d4.loss_cls: 0.4055  decode.d4.loss_mask: 0.3270  decode.d4.loss_dice: 0.3653  decode.d5.loss_cls: 0.4442  decode.d5.loss_mask: 0.3179  decode.d5.loss_dice: 0.3573  decode.d6.loss_cls: 0.4179  decode.d6.loss_mask: 0.3439  decode.d6.loss_dice: 0.3811  decode.d7.loss_cls: 0.4117  decode.d7.loss_mask: 0.3349  decode.d7.loss_dice: 0.3603  decode.d8.loss_cls: 0.4550  decode.d8.loss_mask: 0.3255  decode.d8.loss_dice: 0.3649
09/30 09:58:35 - mmengine - INFO - Iter(train) [  7700/320000]  base_lr: 9.7832e-05 lr: 9.7832e-06  eta: 1 day, 13:32:38  time: 0.4314  data_time: 0.0088  memory: 5147  grad_norm: 79.5319  loss: 11.7445  decode.loss_cls: 0.4807  decode.loss_mask: 0.4006  decode.loss_dice: 0.2949  decode.d0.loss_cls: 1.1623  decode.d0.loss_mask: 0.2481  decode.d0.loss_dice: 0.3141  decode.d1.loss_cls: 0.6338  decode.d1.loss_mask: 0.2472  decode.d1.loss_dice: 0.2864  decode.d2.loss_cls: 0.5260  decode.d2.loss_mask: 0.2461  decode.d2.loss_dice: 0.2977  decode.d3.loss_cls: 0.4924  decode.d3.loss_mask: 0.2401  decode.d3.loss_dice: 0.2730  decode.d4.loss_cls: 0.4147  decode.d4.loss_mask: 0.2384  decode.d4.loss_dice: 0.2746  decode.d5.loss_cls: 0.4755  decode.d5.loss_mask: 0.3543  decode.d5.loss_dice: 0.3537  decode.d6.loss_cls: 0.4464  decode.d6.loss_mask: 0.3882  decode.d6.loss_dice: 0.3086  decode.d7.loss_cls: 0.5064  decode.d7.loss_mask: 0.3955  decode.d7.loss_dice: 0.3085  decode.d8.loss_cls: 0.5001  decode.d8.loss_mask: 0.3264  decode.d8.loss_dice: 0.3096
09/30 09:58:57 - mmengine - INFO - Iter(train) [  7750/320000]  base_lr: 9.7818e-05 lr: 9.7818e-06  eta: 1 day, 13:32:14  time: 0.4312  data_time: 0.0087  memory: 5146  grad_norm: 107.2840  loss: 8.8614  decode.loss_cls: 0.2878  decode.loss_mask: 0.2796  decode.loss_dice: 0.2300  decode.d0.loss_cls: 0.9162  decode.d0.loss_mask: 0.3080  decode.d0.loss_dice: 0.2430  decode.d1.loss_cls: 0.3196  decode.d1.loss_mask: 0.2789  decode.d1.loss_dice: 0.2218  decode.d2.loss_cls: 0.3184  decode.d2.loss_mask: 0.2808  decode.d2.loss_dice: 0.2435  decode.d3.loss_cls: 0.2756  decode.d3.loss_mask: 0.2792  decode.d3.loss_dice: 0.2359  decode.d4.loss_cls: 0.2973  decode.d4.loss_mask: 0.2828  decode.d4.loss_dice: 0.2372  decode.d5.loss_cls: 0.3157  decode.d5.loss_mask: 0.2750  decode.d5.loss_dice: 0.2306  decode.d6.loss_cls: 0.2747  decode.d6.loss_mask: 0.2829  decode.d6.loss_dice: 0.2279  decode.d7.loss_cls: 0.2124  decode.d7.loss_mask: 0.4168  decode.d7.loss_dice: 0.2497  decode.d8.loss_cls: 0.2682  decode.d8.loss_mask: 0.3097  decode.d8.loss_dice: 0.2623
09/30 09:59:18 - mmengine - INFO - Iter(train) [  7800/320000]  base_lr: 9.7804e-05 lr: 9.7804e-06  eta: 1 day, 13:31:51  time: 0.4322  data_time: 0.0088  memory: 5147  grad_norm: 129.4442  loss: 11.0535  decode.loss_cls: 0.3150  decode.loss_mask: 0.4864  decode.loss_dice: 0.2724  decode.d0.loss_cls: 1.0428  decode.d0.loss_mask: 0.3851  decode.d0.loss_dice: 0.2592  decode.d1.loss_cls: 0.4184  decode.d1.loss_mask: 0.4735  decode.d1.loss_dice: 0.3097  decode.d2.loss_cls: 0.3951  decode.d2.loss_mask: 0.4530  decode.d2.loss_dice: 0.2929  decode.d3.loss_cls: 0.3977  decode.d3.loss_mask: 0.4305  decode.d3.loss_dice: 0.2824  decode.d4.loss_cls: 0.4076  decode.d4.loss_mask: 0.3796  decode.d4.loss_dice: 0.2426  decode.d5.loss_cls: 0.3056  decode.d5.loss_mask: 0.3976  decode.d5.loss_dice: 0.2470  decode.d6.loss_cls: 0.2146  decode.d6.loss_mask: 0.4158  decode.d6.loss_dice: 0.2473  decode.d7.loss_cls: 0.2813  decode.d7.loss_mask: 0.4081  decode.d7.loss_dice: 0.2402  decode.d8.loss_cls: 0.3338  decode.d8.loss_mask: 0.4375  decode.d8.loss_dice: 0.2807
09/30 09:59:40 - mmengine - INFO - Iter(train) [  7850/320000]  base_lr: 9.7790e-05 lr: 9.7790e-06  eta: 1 day, 13:31:34  time: 0.4318  data_time: 0.0088  memory: 5180  grad_norm: 88.4748  loss: 11.1708  decode.loss_cls: 0.5215  decode.loss_mask: 0.2420  decode.loss_dice: 0.2909  decode.d0.loss_cls: 1.1265  decode.d0.loss_mask: 0.2481  decode.d0.loss_dice: 0.3039  decode.d1.loss_cls: 0.7247  decode.d1.loss_mask: 0.2362  decode.d1.loss_dice: 0.2995  decode.d2.loss_cls: 0.5700  decode.d2.loss_mask: 0.2360  decode.d2.loss_dice: 0.2786  decode.d3.loss_cls: 0.5068  decode.d3.loss_mask: 0.2383  decode.d3.loss_dice: 0.2695  decode.d4.loss_cls: 0.4949  decode.d4.loss_mask: 0.2312  decode.d4.loss_dice: 0.3029  decode.d5.loss_cls: 0.4251  decode.d5.loss_mask: 0.2338  decode.d5.loss_dice: 0.2892  decode.d6.loss_cls: 0.4911  decode.d6.loss_mask: 0.2376  decode.d6.loss_dice: 0.2813  decode.d7.loss_cls: 0.4807  decode.d7.loss_mask: 0.2595  decode.d7.loss_dice: 0.3153  decode.d8.loss_cls: 0.5181  decode.d8.loss_mask: 0.2382  decode.d8.loss_dice: 0.2795
09/30 10:00:02 - mmengine - INFO - Iter(train) [  7900/320000]  base_lr: 9.7776e-05 lr: 9.7776e-06  eta: 1 day, 13:31:10  time: 0.4321  data_time: 0.0085  memory: 5161  grad_norm: 59.4821  loss: 8.9660  decode.loss_cls: 0.2958  decode.loss_mask: 0.2952  decode.loss_dice: 0.2561  decode.d0.loss_cls: 0.9063  decode.d0.loss_mask: 0.3079  decode.d0.loss_dice: 0.2940  decode.d1.loss_cls: 0.2953  decode.d1.loss_mask: 0.3042  decode.d1.loss_dice: 0.2515  decode.d2.loss_cls: 0.2376  decode.d2.loss_mask: 0.2980  decode.d2.loss_dice: 0.2425  decode.d3.loss_cls: 0.2934  decode.d3.loss_mask: 0.2956  decode.d3.loss_dice: 0.2437  decode.d4.loss_cls: 0.2816  decode.d4.loss_mask: 0.2938  decode.d4.loss_dice: 0.2313  decode.d5.loss_cls: 0.2567  decode.d5.loss_mask: 0.2959  decode.d5.loss_dice: 0.2694  decode.d6.loss_cls: 0.3063  decode.d6.loss_mask: 0.2979  decode.d6.loss_dice: 0.2535  decode.d7.loss_cls: 0.2994  decode.d7.loss_mask: 0.2958  decode.d7.loss_dice: 0.2420  decode.d8.loss_cls: 0.2826  decode.d8.loss_mask: 0.2929  decode.d8.loss_dice: 0.2499
09/30 10:00:23 - mmengine - INFO - Iter(train) [  7950/320000]  base_lr: 9.7762e-05 lr: 9.7762e-06  eta: 1 day, 13:30:47  time: 0.4321  data_time: 0.0087  memory: 5180  grad_norm: 144.0139  loss: 11.2670  decode.loss_cls: 0.2907  decode.loss_mask: 0.3787  decode.loss_dice: 0.3093  decode.d0.loss_cls: 1.2729  decode.d0.loss_mask: 0.4020  decode.d0.loss_dice: 0.3388  decode.d1.loss_cls: 0.4688  decode.d1.loss_mask: 0.3938  decode.d1.loss_dice: 0.3346  decode.d2.loss_cls: 0.3617  decode.d2.loss_mask: 0.3543  decode.d2.loss_dice: 0.3022  decode.d3.loss_cls: 0.3515  decode.d3.loss_mask: 0.4143  decode.d3.loss_dice: 0.3276  decode.d4.loss_cls: 0.3508  decode.d4.loss_mask: 0.3573  decode.d4.loss_dice: 0.2933  decode.d5.loss_cls: 0.3247  decode.d5.loss_mask: 0.3815  decode.d5.loss_dice: 0.3163  decode.d6.loss_cls: 0.2919  decode.d6.loss_mask: 0.3735  decode.d6.loss_dice: 0.3246  decode.d7.loss_cls: 0.3124  decode.d7.loss_mask: 0.3472  decode.d7.loss_dice: 0.2890  decode.d8.loss_cls: 0.3387  decode.d8.loss_mask: 0.3653  decode.d8.loss_dice: 0.2995
09/30 10:00:45 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 10:00:45 - mmengine - INFO - Iter(train) [  8000/320000]  base_lr: 9.7747e-05 lr: 9.7747e-06  eta: 1 day, 13:30:23  time: 0.4313  data_time: 0.0084  memory: 5180  grad_norm: 86.9884  loss: 9.7249  decode.loss_cls: 0.4551  decode.loss_mask: 0.2039  decode.loss_dice: 0.2463  decode.d0.loss_cls: 1.3221  decode.d0.loss_mask: 0.2291  decode.d0.loss_dice: 0.3099  decode.d1.loss_cls: 0.4204  decode.d1.loss_mask: 0.2089  decode.d1.loss_dice: 0.2479  decode.d2.loss_cls: 0.3643  decode.d2.loss_mask: 0.2229  decode.d2.loss_dice: 0.2451  decode.d3.loss_cls: 0.3737  decode.d3.loss_mask: 0.2080  decode.d3.loss_dice: 0.2174  decode.d4.loss_cls: 0.3971  decode.d4.loss_mask: 0.2108  decode.d4.loss_dice: 0.2459  decode.d5.loss_cls: 0.3867  decode.d5.loss_mask: 0.2371  decode.d5.loss_dice: 0.2597  decode.d6.loss_cls: 0.3646  decode.d6.loss_mask: 0.2374  decode.d6.loss_dice: 0.2766  decode.d7.loss_cls: 0.3726  decode.d7.loss_mask: 0.2698  decode.d7.loss_dice: 0.2753  decode.d8.loss_cls: 0.4537  decode.d8.loss_mask: 0.2042  decode.d8.loss_dice: 0.2586
09/30 10:01:07 - mmengine - INFO - Iter(train) [  8050/320000]  base_lr: 9.7733e-05 lr: 9.7733e-06  eta: 1 day, 13:30:00  time: 0.4331  data_time: 0.0088  memory: 5160  grad_norm: 109.4369  loss: 10.5689  decode.loss_cls: 0.3509  decode.loss_mask: 0.3036  decode.loss_dice: 0.3323  decode.d0.loss_cls: 1.1784  decode.d0.loss_mask: 0.2548  decode.d0.loss_dice: 0.3141  decode.d1.loss_cls: 0.5073  decode.d1.loss_mask: 0.2686  decode.d1.loss_dice: 0.3241  decode.d2.loss_cls: 0.4194  decode.d2.loss_mask: 0.2617  decode.d2.loss_dice: 0.3224  decode.d3.loss_cls: 0.4006  decode.d3.loss_mask: 0.2487  decode.d3.loss_dice: 0.2947  decode.d4.loss_cls: 0.3916  decode.d4.loss_mask: 0.2609  decode.d4.loss_dice: 0.3042  decode.d5.loss_cls: 0.3738  decode.d5.loss_mask: 0.2737  decode.d5.loss_dice: 0.3276  decode.d6.loss_cls: 0.3891  decode.d6.loss_mask: 0.2739  decode.d6.loss_dice: 0.3115  decode.d7.loss_cls: 0.3596  decode.d7.loss_mask: 0.2907  decode.d7.loss_dice: 0.3274  decode.d8.loss_cls: 0.2852  decode.d8.loss_mask: 0.2929  decode.d8.loss_dice: 0.3252
09/30 10:01:28 - mmengine - INFO - Iter(train) [  8100/320000]  base_lr: 9.7719e-05 lr: 9.7719e-06  eta: 1 day, 13:29:37  time: 0.4327  data_time: 0.0090  memory: 5180  grad_norm: 49.6592  loss: 7.2721  decode.loss_cls: 0.1512  decode.loss_mask: 0.2425  decode.loss_dice: 0.2172  decode.d0.loss_cls: 0.8545  decode.d0.loss_mask: 0.2498  decode.d0.loss_dice: 0.2560  decode.d1.loss_cls: 0.1834  decode.d1.loss_mask: 0.2524  decode.d1.loss_dice: 0.2263  decode.d2.loss_cls: 0.2108  decode.d2.loss_mask: 0.2450  decode.d2.loss_dice: 0.2250  decode.d3.loss_cls: 0.2091  decode.d3.loss_mask: 0.2445  decode.d3.loss_dice: 0.2188  decode.d4.loss_cls: 0.2028  decode.d4.loss_mask: 0.2424  decode.d4.loss_dice: 0.2080  decode.d5.loss_cls: 0.1950  decode.d5.loss_mask: 0.2454  decode.d5.loss_dice: 0.2241  decode.d6.loss_cls: 0.1746  decode.d6.loss_mask: 0.2430  decode.d6.loss_dice: 0.2195  decode.d7.loss_cls: 0.1971  decode.d7.loss_mask: 0.2442  decode.d7.loss_dice: 0.2317  decode.d8.loss_cls: 0.1844  decode.d8.loss_mask: 0.2459  decode.d8.loss_dice: 0.2275
09/30 10:01:50 - mmengine - INFO - Iter(train) [  8150/320000]  base_lr: 9.7705e-05 lr: 9.7705e-06  eta: 1 day, 13:29:14  time: 0.4317  data_time: 0.0088  memory: 5180  grad_norm: 98.7778  loss: 10.4788  decode.loss_cls: 0.2223  decode.loss_mask: 0.3914  decode.loss_dice: 0.3443  decode.d0.loss_cls: 1.1806  decode.d0.loss_mask: 0.3724  decode.d0.loss_dice: 0.3003  decode.d1.loss_cls: 0.2365  decode.d1.loss_mask: 0.3592  decode.d1.loss_dice: 0.3295  decode.d2.loss_cls: 0.1760  decode.d2.loss_mask: 0.3684  decode.d2.loss_dice: 0.3508  decode.d3.loss_cls: 0.1871  decode.d3.loss_mask: 0.3670  decode.d3.loss_dice: 0.3435  decode.d4.loss_cls: 0.2154  decode.d4.loss_mask: 0.3804  decode.d4.loss_dice: 0.3485  decode.d5.loss_cls: 0.1900  decode.d5.loss_mask: 0.4219  decode.d5.loss_dice: 0.3560  decode.d6.loss_cls: 0.2169  decode.d6.loss_mask: 0.4552  decode.d6.loss_dice: 0.3767  decode.d7.loss_cls: 0.2527  decode.d7.loss_mask: 0.4008  decode.d7.loss_dice: 0.3582  decode.d8.loss_cls: 0.2294  decode.d8.loss_mask: 0.4064  decode.d8.loss_dice: 0.3411
09/30 10:02:11 - mmengine - INFO - Iter(train) [  8200/320000]  base_lr: 9.7691e-05 lr: 9.7691e-06  eta: 1 day, 13:28:52  time: 0.4323  data_time: 0.0086  memory: 5161  grad_norm: 65.9560  loss: 8.0958  decode.loss_cls: 0.1688  decode.loss_mask: 0.3168  decode.loss_dice: 0.2075  decode.d0.loss_cls: 0.9738  decode.d0.loss_mask: 0.3302  decode.d0.loss_dice: 0.2385  decode.d1.loss_cls: 0.3049  decode.d1.loss_mask: 0.3200  decode.d1.loss_dice: 0.2249  decode.d2.loss_cls: 0.2188  decode.d2.loss_mask: 0.3238  decode.d2.loss_dice: 0.2107  decode.d3.loss_cls: 0.1969  decode.d3.loss_mask: 0.3218  decode.d3.loss_dice: 0.2021  decode.d4.loss_cls: 0.1912  decode.d4.loss_mask: 0.3189  decode.d4.loss_dice: 0.2008  decode.d5.loss_cls: 0.1726  decode.d5.loss_mask: 0.3179  decode.d5.loss_dice: 0.2057  decode.d6.loss_cls: 0.1621  decode.d6.loss_mask: 0.3189  decode.d6.loss_dice: 0.2259  decode.d7.loss_cls: 0.1711  decode.d7.loss_mask: 0.3148  decode.d7.loss_dice: 0.2092  decode.d8.loss_cls: 0.1963  decode.d8.loss_mask: 0.3205  decode.d8.loss_dice: 0.2106
09/30 10:02:33 - mmengine - INFO - Iter(train) [  8250/320000]  base_lr: 9.7677e-05 lr: 9.7677e-06  eta: 1 day, 13:28:29  time: 0.4321  data_time: 0.0087  memory: 5180  grad_norm: 58.9332  loss: 9.5773  decode.loss_cls: 0.3285  decode.loss_mask: 0.2606  decode.loss_dice: 0.2909  decode.d0.loss_cls: 1.0212  decode.d0.loss_mask: 0.2769  decode.d0.loss_dice: 0.3143  decode.d1.loss_cls: 0.4387  decode.d1.loss_mask: 0.2587  decode.d1.loss_dice: 0.2838  decode.d2.loss_cls: 0.3691  decode.d2.loss_mask: 0.2529  decode.d2.loss_dice: 0.2682  decode.d3.loss_cls: 0.4041  decode.d3.loss_mask: 0.2528  decode.d3.loss_dice: 0.2910  decode.d4.loss_cls: 0.3566  decode.d4.loss_mask: 0.2558  decode.d4.loss_dice: 0.2703  decode.d5.loss_cls: 0.3185  decode.d5.loss_mask: 0.2574  decode.d5.loss_dice: 0.2797  decode.d6.loss_cls: 0.3178  decode.d6.loss_mask: 0.2559  decode.d6.loss_dice: 0.2677  decode.d7.loss_cls: 0.3238  decode.d7.loss_mask: 0.2586  decode.d7.loss_dice: 0.2699  decode.d8.loss_cls: 0.3128  decode.d8.loss_mask: 0.2565  decode.d8.loss_dice: 0.2640
09/30 10:02:55 - mmengine - INFO - Iter(train) [  8300/320000]  base_lr: 9.7663e-05 lr: 9.7663e-06  eta: 1 day, 13:28:05  time: 0.4328  data_time: 0.0089  memory: 5180  grad_norm: 143.3722  loss: 9.6756  decode.loss_cls: 0.3144  decode.loss_mask: 0.2878  decode.loss_dice: 0.2428  decode.d0.loss_cls: 1.0332  decode.d0.loss_mask: 0.2959  decode.d0.loss_dice: 0.2934  decode.d1.loss_cls: 0.4055  decode.d1.loss_mask: 0.2983  decode.d1.loss_dice: 0.2616  decode.d2.loss_cls: 0.3651  decode.d2.loss_mask: 0.2893  decode.d2.loss_dice: 0.2464  decode.d3.loss_cls: 0.3392  decode.d3.loss_mask: 0.2953  decode.d3.loss_dice: 0.2442  decode.d4.loss_cls: 0.4042  decode.d4.loss_mask: 0.2616  decode.d4.loss_dice: 0.2352  decode.d5.loss_cls: 0.3445  decode.d5.loss_mask: 0.2875  decode.d5.loss_dice: 0.2711  decode.d6.loss_cls: 0.3284  decode.d6.loss_mask: 0.2843  decode.d6.loss_dice: 0.2589  decode.d7.loss_cls: 0.3322  decode.d7.loss_mask: 0.2964  decode.d7.loss_dice: 0.3050  decode.d8.loss_cls: 0.3037  decode.d8.loss_mask: 0.2812  decode.d8.loss_dice: 0.2691
09/30 10:03:16 - mmengine - INFO - Iter(train) [  8350/320000]  base_lr: 9.7649e-05 lr: 9.7649e-06  eta: 1 day, 13:27:42  time: 0.4319  data_time: 0.0088  memory: 5160  grad_norm: 152.0375  loss: 13.0422  decode.loss_cls: 0.5027  decode.loss_mask: 0.3005  decode.loss_dice: 0.3442  decode.d0.loss_cls: 1.3928  decode.d0.loss_mask: 0.3248  decode.d0.loss_dice: 0.4356  decode.d1.loss_cls: 0.6162  decode.d1.loss_mask: 0.2986  decode.d1.loss_dice: 0.3767  decode.d2.loss_cls: 0.5355  decode.d2.loss_mask: 0.2974  decode.d2.loss_dice: 0.3592  decode.d3.loss_cls: 0.6208  decode.d3.loss_mask: 0.2959  decode.d3.loss_dice: 0.3386  decode.d4.loss_cls: 0.5452  decode.d4.loss_mask: 0.2957  decode.d4.loss_dice: 0.3400  decode.d5.loss_cls: 0.5572  decode.d5.loss_mask: 0.3231  decode.d5.loss_dice: 0.3503  decode.d6.loss_cls: 0.5576  decode.d6.loss_mask: 0.3075  decode.d6.loss_dice: 0.3696  decode.d7.loss_cls: 0.5159  decode.d7.loss_mask: 0.3030  decode.d7.loss_dice: 0.3382  decode.d8.loss_cls: 0.5251  decode.d8.loss_mask: 0.3043  decode.d8.loss_dice: 0.3700
09/30 10:03:38 - mmengine - INFO - Iter(train) [  8400/320000]  base_lr: 9.7635e-05 lr: 9.7635e-06  eta: 1 day, 13:27:20  time: 0.4319  data_time: 0.0087  memory: 5180  grad_norm: 115.8570  loss: 8.4408  decode.loss_cls: 0.2303  decode.loss_mask: 0.3078  decode.loss_dice: 0.2622  decode.d0.loss_cls: 0.8374  decode.d0.loss_mask: 0.3014  decode.d0.loss_dice: 0.2891  decode.d1.loss_cls: 0.1978  decode.d1.loss_mask: 0.3109  decode.d1.loss_dice: 0.2497  decode.d2.loss_cls: 0.1987  decode.d2.loss_mask: 0.3021  decode.d2.loss_dice: 0.2796  decode.d3.loss_cls: 0.1799  decode.d3.loss_mask: 0.3134  decode.d3.loss_dice: 0.2843  decode.d4.loss_cls: 0.1889  decode.d4.loss_mask: 0.3276  decode.d4.loss_dice: 0.2631  decode.d5.loss_cls: 0.1879  decode.d5.loss_mask: 0.3190  decode.d5.loss_dice: 0.2561  decode.d6.loss_cls: 0.2004  decode.d6.loss_mask: 0.3457  decode.d6.loss_dice: 0.2686  decode.d7.loss_cls: 0.2004  decode.d7.loss_mask: 0.3159  decode.d7.loss_dice: 0.2575  decode.d8.loss_cls: 0.1311  decode.d8.loss_mask: 0.4071  decode.d8.loss_dice: 0.2272
09/30 10:03:59 - mmengine - INFO - Iter(train) [  8450/320000]  base_lr: 9.7621e-05 lr: 9.7621e-06  eta: 1 day, 13:26:57  time: 0.4321  data_time: 0.0089  memory: 5180  grad_norm: 184.4904  loss: 11.0742  decode.loss_cls: 0.3337  decode.loss_mask: 0.3700  decode.loss_dice: 0.2908  decode.d0.loss_cls: 0.9992  decode.d0.loss_mask: 0.3816  decode.d0.loss_dice: 0.3082  decode.d1.loss_cls: 0.4235  decode.d1.loss_mask: 0.3672  decode.d1.loss_dice: 0.2918  decode.d2.loss_cls: 0.3964  decode.d2.loss_mask: 0.3773  decode.d2.loss_dice: 0.3009  decode.d3.loss_cls: 0.4022  decode.d3.loss_mask: 0.3812  decode.d3.loss_dice: 0.3027  decode.d4.loss_cls: 0.3344  decode.d4.loss_mask: 0.3909  decode.d4.loss_dice: 0.3268  decode.d5.loss_cls: 0.3971  decode.d5.loss_mask: 0.3806  decode.d5.loss_dice: 0.2878  decode.d6.loss_cls: 0.3372  decode.d6.loss_mask: 0.4055  decode.d6.loss_dice: 0.2944  decode.d7.loss_cls: 0.3263  decode.d7.loss_mask: 0.3792  decode.d7.loss_dice: 0.3006  decode.d8.loss_cls: 0.3212  decode.d8.loss_mask: 0.3730  decode.d8.loss_dice: 0.2923
09/30 10:04:21 - mmengine - INFO - Iter(train) [  8500/320000]  base_lr: 9.7606e-05 lr: 9.7606e-06  eta: 1 day, 13:26:34  time: 0.4318  data_time: 0.0088  memory: 5161  grad_norm: 151.3775  loss: 9.5109  decode.loss_cls: 0.2792  decode.loss_mask: 0.3662  decode.loss_dice: 0.2389  decode.d0.loss_cls: 1.0297  decode.d0.loss_mask: 0.3451  decode.d0.loss_dice: 0.2659  decode.d1.loss_cls: 0.2837  decode.d1.loss_mask: 0.3353  decode.d1.loss_dice: 0.2672  decode.d2.loss_cls: 0.2913  decode.d2.loss_mask: 0.3408  decode.d2.loss_dice: 0.2580  decode.d3.loss_cls: 0.2640  decode.d3.loss_mask: 0.3460  decode.d3.loss_dice: 0.2478  decode.d4.loss_cls: 0.2968  decode.d4.loss_mask: 0.3432  decode.d4.loss_dice: 0.2485  decode.d5.loss_cls: 0.2425  decode.d5.loss_mask: 0.3516  decode.d5.loss_dice: 0.2648  decode.d6.loss_cls: 0.2415  decode.d6.loss_mask: 0.3552  decode.d6.loss_dice: 0.2632  decode.d7.loss_cls: 0.2417  decode.d7.loss_mask: 0.3699  decode.d7.loss_dice: 0.2347  decode.d8.loss_cls: 0.2858  decode.d8.loss_mask: 0.3671  decode.d8.loss_dice: 0.2454
09/30 10:04:43 - mmengine - INFO - Iter(train) [  8550/320000]  base_lr: 9.7592e-05 lr: 9.7592e-06  eta: 1 day, 13:26:10  time: 0.4326  data_time: 0.0089  memory: 5161  grad_norm: 144.0129  loss: 13.3775  decode.loss_cls: 0.5261  decode.loss_mask: 0.3955  decode.loss_dice: 0.2896  decode.d0.loss_cls: 1.4286  decode.d0.loss_mask: 0.3780  decode.d0.loss_dice: 0.3149  decode.d1.loss_cls: 0.5570  decode.d1.loss_mask: 0.3833  decode.d1.loss_dice: 0.2861  decode.d2.loss_cls: 0.6156  decode.d2.loss_mask: 0.4002  decode.d2.loss_dice: 0.2912  decode.d3.loss_cls: 0.5630  decode.d3.loss_mask: 0.3781  decode.d3.loss_dice: 0.2889  decode.d4.loss_cls: 0.5645  decode.d4.loss_mask: 0.4131  decode.d4.loss_dice: 0.2941  decode.d5.loss_cls: 0.5441  decode.d5.loss_mask: 0.4082  decode.d5.loss_dice: 0.3024  decode.d6.loss_cls: 0.5566  decode.d6.loss_mask: 0.4114  decode.d6.loss_dice: 0.3037  decode.d7.loss_cls: 0.4824  decode.d7.loss_mask: 0.4335  decode.d7.loss_dice: 0.3217  decode.d8.loss_cls: 0.5561  decode.d8.loss_mask: 0.3943  decode.d8.loss_dice: 0.2954
09/30 10:05:04 - mmengine - INFO - Iter(train) [  8600/320000]  base_lr: 9.7578e-05 lr: 9.7578e-06  eta: 1 day, 13:25:48  time: 0.4321  data_time: 0.0089  memory: 5161  grad_norm: 107.3320  loss: 7.7541  decode.loss_cls: 0.1562  decode.loss_mask: 0.2570  decode.loss_dice: 0.2567  decode.d0.loss_cls: 0.9071  decode.d0.loss_mask: 0.2688  decode.d0.loss_dice: 0.2712  decode.d1.loss_cls: 0.1940  decode.d1.loss_mask: 0.2659  decode.d1.loss_dice: 0.2831  decode.d2.loss_cls: 0.2308  decode.d2.loss_mask: 0.2538  decode.d2.loss_dice: 0.2683  decode.d3.loss_cls: 0.1828  decode.d3.loss_mask: 0.2530  decode.d3.loss_dice: 0.2635  decode.d4.loss_cls: 0.1512  decode.d4.loss_mask: 0.2566  decode.d4.loss_dice: 0.2725  decode.d5.loss_cls: 0.1948  decode.d5.loss_mask: 0.2542  decode.d5.loss_dice: 0.2691  decode.d6.loss_cls: 0.1424  decode.d6.loss_mask: 0.2560  decode.d6.loss_dice: 0.2757  decode.d7.loss_cls: 0.1474  decode.d7.loss_mask: 0.2569  decode.d7.loss_dice: 0.2771  decode.d8.loss_cls: 0.1716  decode.d8.loss_mask: 0.2548  decode.d8.loss_dice: 0.2615
09/30 10:05:26 - mmengine - INFO - Iter(train) [  8650/320000]  base_lr: 9.7564e-05 lr: 9.7564e-06  eta: 1 day, 13:25:26  time: 0.4324  data_time: 0.0090  memory: 5147  grad_norm: 131.6957  loss: 11.6166  decode.loss_cls: 0.5698  decode.loss_mask: 0.3065  decode.loss_dice: 0.2962  decode.d0.loss_cls: 0.9083  decode.d0.loss_mask: 0.3211  decode.d0.loss_dice: 0.2766  decode.d1.loss_cls: 0.4739  decode.d1.loss_mask: 0.3199  decode.d1.loss_dice: 0.2661  decode.d2.loss_cls: 0.4350  decode.d2.loss_mask: 0.3191  decode.d2.loss_dice: 0.3035  decode.d3.loss_cls: 0.5135  decode.d3.loss_mask: 0.3257  decode.d3.loss_dice: 0.3170  decode.d4.loss_cls: 0.5017  decode.d4.loss_mask: 0.3197  decode.d4.loss_dice: 0.2573  decode.d5.loss_cls: 0.5296  decode.d5.loss_mask: 0.3175  decode.d5.loss_dice: 0.2641  decode.d6.loss_cls: 0.5019  decode.d6.loss_mask: 0.3170  decode.d6.loss_dice: 0.3067  decode.d7.loss_cls: 0.5606  decode.d7.loss_mask: 0.3091  decode.d7.loss_dice: 0.2939  decode.d8.loss_cls: 0.5904  decode.d8.loss_mask: 0.3164  decode.d8.loss_dice: 0.2784
09/30 10:05:47 - mmengine - INFO - Iter(train) [  8700/320000]  base_lr: 9.7550e-05 lr: 9.7550e-06  eta: 1 day, 13:25:04  time: 0.4326  data_time: 0.0089  memory: 5160  grad_norm: 122.2691  loss: 10.4308  decode.loss_cls: 0.4324  decode.loss_mask: 0.3715  decode.loss_dice: 0.2822  decode.d0.loss_cls: 1.0398  decode.d0.loss_mask: 0.3257  decode.d0.loss_dice: 0.2637  decode.d1.loss_cls: 0.3680  decode.d1.loss_mask: 0.3553  decode.d1.loss_dice: 0.2555  decode.d2.loss_cls: 0.3755  decode.d2.loss_mask: 0.3378  decode.d2.loss_dice: 0.2539  decode.d3.loss_cls: 0.3121  decode.d3.loss_mask: 0.3381  decode.d3.loss_dice: 0.2675  decode.d4.loss_cls: 0.3122  decode.d4.loss_mask: 0.3354  decode.d4.loss_dice: 0.2468  decode.d5.loss_cls: 0.2914  decode.d5.loss_mask: 0.4492  decode.d5.loss_dice: 0.2675  decode.d6.loss_cls: 0.3794  decode.d6.loss_mask: 0.3925  decode.d6.loss_dice: 0.2775  decode.d7.loss_cls: 0.3084  decode.d7.loss_mask: 0.3646  decode.d7.loss_dice: 0.2741  decode.d8.loss_cls: 0.3047  decode.d8.loss_mask: 0.3684  decode.d8.loss_dice: 0.2795
09/30 10:06:09 - mmengine - INFO - Iter(train) [  8750/320000]  base_lr: 9.7536e-05 lr: 9.7536e-06  eta: 1 day, 13:24:43  time: 0.4329  data_time: 0.0088  memory: 5180  grad_norm: 58.7288  loss: 9.3618  decode.loss_cls: 0.2671  decode.loss_mask: 0.2631  decode.loss_dice: 0.2883  decode.d0.loss_cls: 0.9113  decode.d0.loss_mask: 0.2682  decode.d0.loss_dice: 0.2940  decode.d1.loss_cls: 0.3001  decode.d1.loss_mask: 0.2673  decode.d1.loss_dice: 0.2951  decode.d2.loss_cls: 0.2364  decode.d2.loss_mask: 0.2676  decode.d2.loss_dice: 0.3072  decode.d3.loss_cls: 0.3509  decode.d3.loss_mask: 0.2635  decode.d3.loss_dice: 0.2788  decode.d4.loss_cls: 0.3713  decode.d4.loss_mask: 0.2663  decode.d4.loss_dice: 0.3024  decode.d5.loss_cls: 0.3222  decode.d5.loss_mask: 0.2671  decode.d5.loss_dice: 0.3169  decode.d6.loss_cls: 0.3553  decode.d6.loss_mask: 0.2562  decode.d6.loss_dice: 0.2973  decode.d7.loss_cls: 0.3562  decode.d7.loss_mask: 0.2591  decode.d7.loss_dice: 0.2847  decode.d8.loss_cls: 0.3091  decode.d8.loss_mask: 0.2605  decode.d8.loss_dice: 0.2785
09/30 10:06:31 - mmengine - INFO - Iter(train) [  8800/320000]  base_lr: 9.7522e-05 lr: 9.7522e-06  eta: 1 day, 13:24:20  time: 0.4332  data_time: 0.0090  memory: 5160  grad_norm: 172.8168  loss: 13.9074  decode.loss_cls: 0.5870  decode.loss_mask: 0.3269  decode.loss_dice: 0.3842  decode.d0.loss_cls: 1.2208  decode.d0.loss_mask: 0.4099  decode.d0.loss_dice: 0.4111  decode.d1.loss_cls: 0.6220  decode.d1.loss_mask: 0.3735  decode.d1.loss_dice: 0.4031  decode.d2.loss_cls: 0.6290  decode.d2.loss_mask: 0.3256  decode.d2.loss_dice: 0.3537  decode.d3.loss_cls: 0.5814  decode.d3.loss_mask: 0.3322  decode.d3.loss_dice: 0.3533  decode.d4.loss_cls: 0.5923  decode.d4.loss_mask: 0.3288  decode.d4.loss_dice: 0.3601  decode.d5.loss_cls: 0.6113  decode.d5.loss_mask: 0.3272  decode.d5.loss_dice: 0.3678  decode.d6.loss_cls: 0.5257  decode.d6.loss_mask: 0.3794  decode.d6.loss_dice: 0.3864  decode.d7.loss_cls: 0.5038  decode.d7.loss_mask: 0.4220  decode.d7.loss_dice: 0.3929  decode.d8.loss_cls: 0.6067  decode.d8.loss_mask: 0.3788  decode.d8.loss_dice: 0.4103
09/30 10:06:52 - mmengine - INFO - Iter(train) [  8850/320000]  base_lr: 9.7508e-05 lr: 9.7508e-06  eta: 1 day, 13:23:57  time: 0.4309  data_time: 0.0088  memory: 5160  grad_norm: 143.0933  loss: 12.8575  decode.loss_cls: 0.5541  decode.loss_mask: 0.3505  decode.loss_dice: 0.3304  decode.d0.loss_cls: 1.2131  decode.d0.loss_mask: 0.3624  decode.d0.loss_dice: 0.3364  decode.d1.loss_cls: 0.5606  decode.d1.loss_mask: 0.3467  decode.d1.loss_dice: 0.3157  decode.d2.loss_cls: 0.5338  decode.d2.loss_mask: 0.3532  decode.d2.loss_dice: 0.3219  decode.d3.loss_cls: 0.5202  decode.d3.loss_mask: 0.3470  decode.d3.loss_dice: 0.2562  decode.d4.loss_cls: 0.5073  decode.d4.loss_mask: 0.3468  decode.d4.loss_dice: 0.2941  decode.d5.loss_cls: 0.5416  decode.d5.loss_mask: 0.3693  decode.d5.loss_dice: 0.3398  decode.d6.loss_cls: 0.6143  decode.d6.loss_mask: 0.3409  decode.d6.loss_dice: 0.3171  decode.d7.loss_cls: 0.6059  decode.d7.loss_mask: 0.3354  decode.d7.loss_dice: 0.2845  decode.d8.loss_cls: 0.6277  decode.d8.loss_mask: 0.3312  decode.d8.loss_dice: 0.2994
09/30 10:07:14 - mmengine - INFO - Iter(train) [  8900/320000]  base_lr: 9.7494e-05 lr: 9.7494e-06  eta: 1 day, 13:23:34  time: 0.4316  data_time: 0.0085  memory: 5180  grad_norm: 137.1857  loss: 9.4479  decode.loss_cls: 0.2761  decode.loss_mask: 0.3105  decode.loss_dice: 0.2586  decode.d0.loss_cls: 1.3361  decode.d0.loss_mask: 0.3528  decode.d0.loss_dice: 0.2997  decode.d1.loss_cls: 0.3464  decode.d1.loss_mask: 0.3179  decode.d1.loss_dice: 0.2574  decode.d2.loss_cls: 0.2507  decode.d2.loss_mask: 0.3372  decode.d2.loss_dice: 0.2346  decode.d3.loss_cls: 0.1915  decode.d3.loss_mask: 0.3179  decode.d3.loss_dice: 0.2165  decode.d4.loss_cls: 0.2711  decode.d4.loss_mask: 0.3158  decode.d4.loss_dice: 0.2449  decode.d5.loss_cls: 0.2855  decode.d5.loss_mask: 0.3110  decode.d5.loss_dice: 0.2335  decode.d6.loss_cls: 0.2666  decode.d6.loss_mask: 0.3209  decode.d6.loss_dice: 0.2380  decode.d7.loss_cls: 0.2713  decode.d7.loss_mask: 0.3058  decode.d7.loss_dice: 0.2367  decode.d8.loss_cls: 0.2941  decode.d8.loss_mask: 0.3109  decode.d8.loss_dice: 0.2380
09/30 10:07:35 - mmengine - INFO - Iter(train) [  8950/320000]  base_lr: 9.7480e-05 lr: 9.7480e-06  eta: 1 day, 13:23:11  time: 0.4316  data_time: 0.0088  memory: 5147  grad_norm: 98.5055  loss: 10.4367  decode.loss_cls: 0.4670  decode.loss_mask: 0.3096  decode.loss_dice: 0.2196  decode.d0.loss_cls: 0.9648  decode.d0.loss_mask: 0.3319  decode.d0.loss_dice: 0.2340  decode.d1.loss_cls: 0.5466  decode.d1.loss_mask: 0.3115  decode.d1.loss_dice: 0.2238  decode.d2.loss_cls: 0.4859  decode.d2.loss_mask: 0.3084  decode.d2.loss_dice: 0.2200  decode.d3.loss_cls: 0.4481  decode.d3.loss_mask: 0.3161  decode.d3.loss_dice: 0.2283  decode.d4.loss_cls: 0.4592  decode.d4.loss_mask: 0.3138  decode.d4.loss_dice: 0.2341  decode.d5.loss_cls: 0.4285  decode.d5.loss_mask: 0.3060  decode.d5.loss_dice: 0.2179  decode.d6.loss_cls: 0.4172  decode.d6.loss_mask: 0.3151  decode.d6.loss_dice: 0.2179  decode.d7.loss_cls: 0.4266  decode.d7.loss_mask: 0.3082  decode.d7.loss_dice: 0.2188  decode.d8.loss_cls: 0.4331  decode.d8.loss_mask: 0.3074  decode.d8.loss_dice: 0.2175
09/30 10:07:57 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 10:07:57 - mmengine - INFO - Iter(train) [  9000/320000]  base_lr: 9.7465e-05 lr: 9.7465e-06  eta: 1 day, 13:22:49  time: 0.4335  data_time: 0.0087  memory: 5180  grad_norm: 129.6680  loss: 9.5462  decode.loss_cls: 0.2181  decode.loss_mask: 0.3579  decode.loss_dice: 0.2828  decode.d0.loss_cls: 1.1202  decode.d0.loss_mask: 0.3781  decode.d0.loss_dice: 0.2745  decode.d1.loss_cls: 0.2052  decode.d1.loss_mask: 0.3735  decode.d1.loss_dice: 0.2870  decode.d2.loss_cls: 0.2292  decode.d2.loss_mask: 0.3638  decode.d2.loss_dice: 0.2750  decode.d3.loss_cls: 0.2716  decode.d3.loss_mask: 0.3566  decode.d3.loss_dice: 0.2742  decode.d4.loss_cls: 0.2038  decode.d4.loss_mask: 0.3610  decode.d4.loss_dice: 0.2651  decode.d5.loss_cls: 0.1725  decode.d5.loss_mask: 0.3581  decode.d5.loss_dice: 0.2735  decode.d6.loss_cls: 0.2119  decode.d6.loss_mask: 0.3631  decode.d6.loss_dice: 0.2690  decode.d7.loss_cls: 0.2418  decode.d7.loss_mask: 0.3581  decode.d7.loss_dice: 0.2744  decode.d8.loss_cls: 0.2875  decode.d8.loss_mask: 0.3661  decode.d8.loss_dice: 0.2727
09/30 10:08:19 - mmengine - INFO - Iter(train) [  9050/320000]  base_lr: 9.7451e-05 lr: 9.7451e-06  eta: 1 day, 13:22:25  time: 0.4315  data_time: 0.0086  memory: 5161  grad_norm: 89.9783  loss: 9.6885  decode.loss_cls: 0.3037  decode.loss_mask: 0.2589  decode.loss_dice: 0.3152  decode.d0.loss_cls: 0.9691  decode.d0.loss_mask: 0.2563  decode.d0.loss_dice: 0.2943  decode.d1.loss_cls: 0.3715  decode.d1.loss_mask: 0.2642  decode.d1.loss_dice: 0.2918  decode.d2.loss_cls: 0.3050  decode.d2.loss_mask: 0.2677  decode.d2.loss_dice: 0.3170  decode.d3.loss_cls: 0.3418  decode.d3.loss_mask: 0.2535  decode.d3.loss_dice: 0.3135  decode.d4.loss_cls: 0.4053  decode.d4.loss_mask: 0.2514  decode.d4.loss_dice: 0.3001  decode.d5.loss_cls: 0.3640  decode.d5.loss_mask: 0.2522  decode.d5.loss_dice: 0.3206  decode.d6.loss_cls: 0.3885  decode.d6.loss_mask: 0.2544  decode.d6.loss_dice: 0.2962  decode.d7.loss_cls: 0.2613  decode.d7.loss_mask: 0.2525  decode.d7.loss_dice: 0.3014  decode.d8.loss_cls: 0.3460  decode.d8.loss_mask: 0.2596  decode.d8.loss_dice: 0.3115
09/30 10:08:40 - mmengine - INFO - Iter(train) [  9100/320000]  base_lr: 9.7437e-05 lr: 9.7437e-06  eta: 1 day, 13:22:01  time: 0.4305  data_time: 0.0083  memory: 5146  grad_norm: 76.0190  loss: 8.2136  decode.loss_cls: 0.2123  decode.loss_mask: 0.3080  decode.loss_dice: 0.2279  decode.d0.loss_cls: 0.9405  decode.d0.loss_mask: 0.3175  decode.d0.loss_dice: 0.2643  decode.d1.loss_cls: 0.2834  decode.d1.loss_mask: 0.3097  decode.d1.loss_dice: 0.2468  decode.d2.loss_cls: 0.1601  decode.d2.loss_mask: 0.3095  decode.d2.loss_dice: 0.2422  decode.d3.loss_cls: 0.1749  decode.d3.loss_mask: 0.2995  decode.d3.loss_dice: 0.2596  decode.d4.loss_cls: 0.1749  decode.d4.loss_mask: 0.3082  decode.d4.loss_dice: 0.2366  decode.d5.loss_cls: 0.1774  decode.d5.loss_mask: 0.3075  decode.d5.loss_dice: 0.2457  decode.d6.loss_cls: 0.1705  decode.d6.loss_mask: 0.3210  decode.d6.loss_dice: 0.2638  decode.d7.loss_cls: 0.1360  decode.d7.loss_mask: 0.3078  decode.d7.loss_dice: 0.2625  decode.d8.loss_cls: 0.2111  decode.d8.loss_mask: 0.3091  decode.d8.loss_dice: 0.2255
09/30 10:09:02 - mmengine - INFO - Iter(train) [  9150/320000]  base_lr: 9.7423e-05 lr: 9.7423e-06  eta: 1 day, 13:21:38  time: 0.4313  data_time: 0.0088  memory: 5160  grad_norm: 215.6349  loss: 12.9540  decode.loss_cls: 0.4704  decode.loss_mask: 0.3694  decode.loss_dice: 0.3430  decode.d0.loss_cls: 1.2771  decode.d0.loss_mask: 0.3754  decode.d0.loss_dice: 0.3797  decode.d1.loss_cls: 0.5549  decode.d1.loss_mask: 0.3618  decode.d1.loss_dice: 0.4003  decode.d2.loss_cls: 0.5666  decode.d2.loss_mask: 0.3585  decode.d2.loss_dice: 0.3724  decode.d3.loss_cls: 0.4712  decode.d3.loss_mask: 0.3665  decode.d3.loss_dice: 0.3676  decode.d4.loss_cls: 0.4588  decode.d4.loss_mask: 0.3656  decode.d4.loss_dice: 0.3451  decode.d5.loss_cls: 0.4714  decode.d5.loss_mask: 0.3634  decode.d5.loss_dice: 0.3612  decode.d6.loss_cls: 0.4141  decode.d6.loss_mask: 0.3738  decode.d6.loss_dice: 0.3680  decode.d7.loss_cls: 0.4472  decode.d7.loss_mask: 0.3788  decode.d7.loss_dice: 0.3822  decode.d8.loss_cls: 0.4622  decode.d8.loss_mask: 0.3830  decode.d8.loss_dice: 0.3445
09/30 10:09:23 - mmengine - INFO - Iter(train) [  9200/320000]  base_lr: 9.7409e-05 lr: 9.7409e-06  eta: 1 day, 13:21:13  time: 0.4322  data_time: 0.0085  memory: 5161  grad_norm: 92.2377  loss: 8.0916  decode.loss_cls: 0.2096  decode.loss_mask: 0.2712  decode.loss_dice: 0.2343  decode.d0.loss_cls: 1.1290  decode.d0.loss_mask: 0.2752  decode.d0.loss_dice: 0.2139  decode.d1.loss_cls: 0.3206  decode.d1.loss_mask: 0.2610  decode.d1.loss_dice: 0.2282  decode.d2.loss_cls: 0.1892  decode.d2.loss_mask: 0.2677  decode.d2.loss_dice: 0.2385  decode.d3.loss_cls: 0.2155  decode.d3.loss_mask: 0.2640  decode.d3.loss_dice: 0.2287  decode.d4.loss_cls: 0.1779  decode.d4.loss_mask: 0.2683  decode.d4.loss_dice: 0.2360  decode.d5.loss_cls: 0.1839  decode.d5.loss_mask: 0.2709  decode.d5.loss_dice: 0.2311  decode.d6.loss_cls: 0.2161  decode.d6.loss_mask: 0.2780  decode.d6.loss_dice: 0.2305  decode.d7.loss_cls: 0.1895  decode.d7.loss_mask: 0.2698  decode.d7.loss_dice: 0.2342  decode.d8.loss_cls: 0.2495  decode.d8.loss_mask: 0.2703  decode.d8.loss_dice: 0.2391
09/30 10:09:45 - mmengine - INFO - Iter(train) [  9250/320000]  base_lr: 9.7395e-05 lr: 9.7395e-06  eta: 1 day, 13:20:50  time: 0.4309  data_time: 0.0085  memory: 5147  grad_norm: 98.6179  loss: 10.3008  decode.loss_cls: 0.2437  decode.loss_mask: 0.4663  decode.loss_dice: 0.2902  decode.d0.loss_cls: 0.7809  decode.d0.loss_mask: 0.4720  decode.d0.loss_dice: 0.2977  decode.d1.loss_cls: 0.2023  decode.d1.loss_mask: 0.4790  decode.d1.loss_dice: 0.2929  decode.d2.loss_cls: 0.2182  decode.d2.loss_mask: 0.4582  decode.d2.loss_dice: 0.2875  decode.d3.loss_cls: 0.1847  decode.d3.loss_mask: 0.4578  decode.d3.loss_dice: 0.3000  decode.d4.loss_cls: 0.1918  decode.d4.loss_mask: 0.4559  decode.d4.loss_dice: 0.2831  decode.d5.loss_cls: 0.1918  decode.d5.loss_mask: 0.4693  decode.d5.loss_dice: 0.3206  decode.d6.loss_cls: 0.2336  decode.d6.loss_mask: 0.4608  decode.d6.loss_dice: 0.2799  decode.d7.loss_cls: 0.2041  decode.d7.loss_mask: 0.4644  decode.d7.loss_dice: 0.2850  decode.d8.loss_cls: 0.2666  decode.d8.loss_mask: 0.4666  decode.d8.loss_dice: 0.2961
09/30 10:10:07 - mmengine - INFO - Iter(train) [  9300/320000]  base_lr: 9.7381e-05 lr: 9.7381e-06  eta: 1 day, 13:20:26  time: 0.4320  data_time: 0.0086  memory: 5161  grad_norm: 173.8698  loss: 10.7108  decode.loss_cls: 0.3533  decode.loss_mask: 0.3067  decode.loss_dice: 0.3233  decode.d0.loss_cls: 1.0051  decode.d0.loss_mask: 0.2866  decode.d0.loss_dice: 0.3431  decode.d1.loss_cls: 0.3661  decode.d1.loss_mask: 0.2796  decode.d1.loss_dice: 0.3328  decode.d2.loss_cls: 0.3287  decode.d2.loss_mask: 0.2957  decode.d2.loss_dice: 0.3152  decode.d3.loss_cls: 0.3572  decode.d3.loss_mask: 0.3260  decode.d3.loss_dice: 0.3285  decode.d4.loss_cls: 0.3828  decode.d4.loss_mask: 0.2822  decode.d4.loss_dice: 0.3062  decode.d5.loss_cls: 0.3758  decode.d5.loss_mask: 0.3117  decode.d5.loss_dice: 0.3010  decode.d6.loss_cls: 0.3924  decode.d6.loss_mask: 0.3340  decode.d6.loss_dice: 0.3261  decode.d7.loss_cls: 0.4301  decode.d7.loss_mask: 0.3455  decode.d7.loss_dice: 0.3358  decode.d8.loss_cls: 0.2961  decode.d8.loss_mask: 0.4059  decode.d8.loss_dice: 0.3375
09/30 10:10:28 - mmengine - INFO - Iter(train) [  9350/320000]  base_lr: 9.7367e-05 lr: 9.7367e-06  eta: 1 day, 13:20:03  time: 0.4313  data_time: 0.0087  memory: 5161  grad_norm: 86.5285  loss: 7.6848  decode.loss_cls: 0.1896  decode.loss_mask: 0.2456  decode.loss_dice: 0.2001  decode.d0.loss_cls: 1.0862  decode.d0.loss_mask: 0.2403  decode.d0.loss_dice: 0.2394  decode.d1.loss_cls: 0.3827  decode.d1.loss_mask: 0.2419  decode.d1.loss_dice: 0.2095  decode.d2.loss_cls: 0.3011  decode.d2.loss_mask: 0.2418  decode.d2.loss_dice: 0.2298  decode.d3.loss_cls: 0.1728  decode.d3.loss_mask: 0.2531  decode.d3.loss_dice: 0.2261  decode.d4.loss_cls: 0.1860  decode.d4.loss_mask: 0.2541  decode.d4.loss_dice: 0.2345  decode.d5.loss_cls: 0.1574  decode.d5.loss_mask: 0.2470  decode.d5.loss_dice: 0.2278  decode.d6.loss_cls: 0.1589  decode.d6.loss_mask: 0.2435  decode.d6.loss_dice: 0.2011  decode.d7.loss_cls: 0.1666  decode.d7.loss_mask: 0.2498  decode.d7.loss_dice: 0.2165  decode.d8.loss_cls: 0.2096  decode.d8.loss_mask: 0.2532  decode.d8.loss_dice: 0.2188
09/30 10:10:50 - mmengine - INFO - Iter(train) [  9400/320000]  base_lr: 9.7353e-05 lr: 9.7353e-06  eta: 1 day, 13:19:40  time: 0.4321  data_time: 0.0087  memory: 5147  grad_norm: 184.6544  loss: 12.5112  decode.loss_cls: 0.3411  decode.loss_mask: 0.3667  decode.loss_dice: 0.3524  decode.d0.loss_cls: 1.2974  decode.d0.loss_mask: 0.4334  decode.d0.loss_dice: 0.4068  decode.d1.loss_cls: 0.5954  decode.d1.loss_mask: 0.4183  decode.d1.loss_dice: 0.3791  decode.d2.loss_cls: 0.4519  decode.d2.loss_mask: 0.3975  decode.d2.loss_dice: 0.3736  decode.d3.loss_cls: 0.4226  decode.d3.loss_mask: 0.3864  decode.d3.loss_dice: 0.3842  decode.d4.loss_cls: 0.3922  decode.d4.loss_mask: 0.3806  decode.d4.loss_dice: 0.3947  decode.d5.loss_cls: 0.3878  decode.d5.loss_mask: 0.3726  decode.d5.loss_dice: 0.3626  decode.d6.loss_cls: 0.3343  decode.d6.loss_mask: 0.3718  decode.d6.loss_dice: 0.3511  decode.d7.loss_cls: 0.3245  decode.d7.loss_mask: 0.3832  decode.d7.loss_dice: 0.3564  decode.d8.loss_cls: 0.3173  decode.d8.loss_mask: 0.3812  decode.d8.loss_dice: 0.3940
09/30 10:11:11 - mmengine - INFO - Iter(train) [  9450/320000]  base_lr: 9.7338e-05 lr: 9.7338e-06  eta: 1 day, 13:19:17  time: 0.4314  data_time: 0.0086  memory: 5161  grad_norm: 118.3367  loss: 7.3542  decode.loss_cls: 0.1453  decode.loss_mask: 0.3030  decode.loss_dice: 0.2296  decode.d0.loss_cls: 0.9973  decode.d0.loss_mask: 0.2117  decode.d0.loss_dice: 0.2157  decode.d1.loss_cls: 0.2056  decode.d1.loss_mask: 0.2418  decode.d1.loss_dice: 0.2197  decode.d2.loss_cls: 0.1745  decode.d2.loss_mask: 0.2632  decode.d2.loss_dice: 0.2540  decode.d3.loss_cls: 0.1664  decode.d3.loss_mask: 0.2370  decode.d3.loss_dice: 0.2192  decode.d4.loss_cls: 0.1802  decode.d4.loss_mask: 0.2608  decode.d4.loss_dice: 0.2236  decode.d5.loss_cls: 0.1559  decode.d5.loss_mask: 0.2158  decode.d5.loss_dice: 0.2136  decode.d6.loss_cls: 0.1444  decode.d6.loss_mask: 0.2788  decode.d6.loss_dice: 0.2326  decode.d7.loss_cls: 0.2034  decode.d7.loss_mask: 0.3054  decode.d7.loss_dice: 0.2176  decode.d8.loss_cls: 0.1499  decode.d8.loss_mask: 0.2775  decode.d8.loss_dice: 0.2111
09/30 10:11:33 - mmengine - INFO - Iter(train) [  9500/320000]  base_lr: 9.7324e-05 lr: 9.7324e-06  eta: 1 day, 13:18:58  time: 0.4466  data_time: 0.0085  memory: 5180  grad_norm: 53.1367  loss: 7.9239  decode.loss_cls: 0.1785  decode.loss_mask: 0.2566  decode.loss_dice: 0.2177  decode.d0.loss_cls: 0.9853  decode.d0.loss_mask: 0.2794  decode.d0.loss_dice: 0.2480  decode.d1.loss_cls: 0.2848  decode.d1.loss_mask: 0.2565  decode.d1.loss_dice: 0.2121  decode.d2.loss_cls: 0.2155  decode.d2.loss_mask: 0.2574  decode.d2.loss_dice: 0.2285  decode.d3.loss_cls: 0.2453  decode.d3.loss_mask: 0.2583  decode.d3.loss_dice: 0.2199  decode.d4.loss_cls: 0.2533  decode.d4.loss_mask: 0.2588  decode.d4.loss_dice: 0.2233  decode.d5.loss_cls: 0.2273  decode.d5.loss_mask: 0.2651  decode.d5.loss_dice: 0.2174  decode.d6.loss_cls: 0.2216  decode.d6.loss_mask: 0.2581  decode.d6.loss_dice: 0.2335  decode.d7.loss_cls: 0.2362  decode.d7.loss_mask: 0.2657  decode.d7.loss_dice: 0.2206  decode.d8.loss_cls: 0.2205  decode.d8.loss_mask: 0.2596  decode.d8.loss_dice: 0.2192
09/30 10:11:55 - mmengine - INFO - Iter(train) [  9550/320000]  base_lr: 9.7310e-05 lr: 9.7310e-06  eta: 1 day, 13:18:36  time: 0.4312  data_time: 0.0084  memory: 5161  grad_norm: 54.4238  loss: 7.1528  decode.loss_cls: 0.1581  decode.loss_mask: 0.1909  decode.loss_dice: 0.2313  decode.d0.loss_cls: 0.9308  decode.d0.loss_mask: 0.1982  decode.d0.loss_dice: 0.2508  decode.d1.loss_cls: 0.3199  decode.d1.loss_mask: 0.2087  decode.d1.loss_dice: 0.2656  decode.d2.loss_cls: 0.2238  decode.d2.loss_mask: 0.1990  decode.d2.loss_dice: 0.2279  decode.d3.loss_cls: 0.1931  decode.d3.loss_mask: 0.1961  decode.d3.loss_dice: 0.2135  decode.d4.loss_cls: 0.2215  decode.d4.loss_mask: 0.1969  decode.d4.loss_dice: 0.2197  decode.d5.loss_cls: 0.2412  decode.d5.loss_mask: 0.1937  decode.d5.loss_dice: 0.2151  decode.d6.loss_cls: 0.1761  decode.d6.loss_mask: 0.1916  decode.d6.loss_dice: 0.2332  decode.d7.loss_cls: 0.2059  decode.d7.loss_mask: 0.1938  decode.d7.loss_dice: 0.2165  decode.d8.loss_cls: 0.2201  decode.d8.loss_mask: 0.1954  decode.d8.loss_dice: 0.2243
09/30 10:12:16 - mmengine - INFO - Iter(train) [  9600/320000]  base_lr: 9.7296e-05 lr: 9.7296e-06  eta: 1 day, 13:18:13  time: 0.4316  data_time: 0.0086  memory: 5147  grad_norm: 141.8842  loss: 9.0462  decode.loss_cls: 0.3167  decode.loss_mask: 0.2913  decode.loss_dice: 0.2316  decode.d0.loss_cls: 1.0432  decode.d0.loss_mask: 0.3029  decode.d0.loss_dice: 0.2610  decode.d1.loss_cls: 0.3740  decode.d1.loss_mask: 0.2700  decode.d1.loss_dice: 0.2084  decode.d2.loss_cls: 0.2621  decode.d2.loss_mask: 0.3065  decode.d2.loss_dice: 0.2433  decode.d3.loss_cls: 0.2802  decode.d3.loss_mask: 0.2895  decode.d3.loss_dice: 0.2335  decode.d4.loss_cls: 0.2720  decode.d4.loss_mask: 0.2991  decode.d4.loss_dice: 0.2449  decode.d5.loss_cls: 0.2574  decode.d5.loss_mask: 0.2985  decode.d5.loss_dice: 0.2537  decode.d6.loss_cls: 0.3035  decode.d6.loss_mask: 0.2741  decode.d6.loss_dice: 0.2452  decode.d7.loss_cls: 0.2970  decode.d7.loss_mask: 0.2809  decode.d7.loss_dice: 0.2505  decode.d8.loss_cls: 0.3158  decode.d8.loss_mask: 0.3050  decode.d8.loss_dice: 0.2347
09/30 10:12:38 - mmengine - INFO - Iter(train) [  9650/320000]  base_lr: 9.7282e-05 lr: 9.7282e-06  eta: 1 day, 13:17:50  time: 0.4325  data_time: 0.0087  memory: 5146  grad_norm: 82.5416  loss: 8.8028  decode.loss_cls: 0.3719  decode.loss_mask: 0.2680  decode.loss_dice: 0.2318  decode.d0.loss_cls: 1.0923  decode.d0.loss_mask: 0.2484  decode.d0.loss_dice: 0.2213  decode.d1.loss_cls: 0.4101  decode.d1.loss_mask: 0.2464  decode.d1.loss_dice: 0.2148  decode.d2.loss_cls: 0.3503  decode.d2.loss_mask: 0.2351  decode.d2.loss_dice: 0.1913  decode.d3.loss_cls: 0.3362  decode.d3.loss_mask: 0.2585  decode.d3.loss_dice: 0.2129  decode.d4.loss_cls: 0.3191  decode.d4.loss_mask: 0.2441  decode.d4.loss_dice: 0.2128  decode.d5.loss_cls: 0.2992  decode.d5.loss_mask: 0.2511  decode.d5.loss_dice: 0.2320  decode.d6.loss_cls: 0.2626  decode.d6.loss_mask: 0.2570  decode.d6.loss_dice: 0.2129  decode.d7.loss_cls: 0.3380  decode.d7.loss_mask: 0.2664  decode.d7.loss_dice: 0.2300  decode.d8.loss_cls: 0.2944  decode.d8.loss_mask: 0.2759  decode.d8.loss_dice: 0.2182
09/30 10:12:59 - mmengine - INFO - Iter(train) [  9700/320000]  base_lr: 9.7268e-05 lr: 9.7268e-06  eta: 1 day, 13:17:27  time: 0.4326  data_time: 0.0088  memory: 5180  grad_norm: 148.8102  loss: 15.0944  decode.loss_cls: 0.6646  decode.loss_mask: 0.3788  decode.loss_dice: 0.3841  decode.d0.loss_cls: 1.1987  decode.d0.loss_mask: 0.4111  decode.d0.loss_dice: 0.4794  decode.d1.loss_cls: 0.7366  decode.d1.loss_mask: 0.3931  decode.d1.loss_dice: 0.4053  decode.d2.loss_cls: 0.6964  decode.d2.loss_mask: 0.3683  decode.d2.loss_dice: 0.3904  decode.d3.loss_cls: 0.7437  decode.d3.loss_mask: 0.3702  decode.d3.loss_dice: 0.3662  decode.d4.loss_cls: 0.6544  decode.d4.loss_mask: 0.3860  decode.d4.loss_dice: 0.3670  decode.d5.loss_cls: 0.6423  decode.d5.loss_mask: 0.4029  decode.d5.loss_dice: 0.3568  decode.d6.loss_cls: 0.7247  decode.d6.loss_mask: 0.3929  decode.d6.loss_dice: 0.3686  decode.d7.loss_cls: 0.6489  decode.d7.loss_mask: 0.3808  decode.d7.loss_dice: 0.3650  decode.d8.loss_cls: 0.6689  decode.d8.loss_mask: 0.3764  decode.d8.loss_dice: 0.3719
09/30 10:13:21 - mmengine - INFO - Iter(train) [  9750/320000]  base_lr: 9.7254e-05 lr: 9.7254e-06  eta: 1 day, 13:17:04  time: 0.4318  data_time: 0.0088  memory: 5161  grad_norm: 87.5947  loss: 12.1558  decode.loss_cls: 0.4252  decode.loss_mask: 0.3510  decode.loss_dice: 0.2919  decode.d0.loss_cls: 1.0399  decode.d0.loss_mask: 0.4783  decode.d0.loss_dice: 0.3632  decode.d1.loss_cls: 0.4524  decode.d1.loss_mask: 0.4447  decode.d1.loss_dice: 0.3207  decode.d2.loss_cls: 0.4076  decode.d2.loss_mask: 0.4470  decode.d2.loss_dice: 0.3196  decode.d3.loss_cls: 0.3440  decode.d3.loss_mask: 0.4282  decode.d3.loss_dice: 0.3159  decode.d4.loss_cls: 0.3511  decode.d4.loss_mask: 0.4707  decode.d4.loss_dice: 0.3394  decode.d5.loss_cls: 0.3912  decode.d5.loss_mask: 0.4308  decode.d5.loss_dice: 0.3399  decode.d6.loss_cls: 0.3193  decode.d6.loss_mask: 0.4448  decode.d6.loss_dice: 0.3313  decode.d7.loss_cls: 0.4341  decode.d7.loss_mask: 0.3815  decode.d7.loss_dice: 0.3297  decode.d8.loss_cls: 0.4214  decode.d8.loss_mask: 0.4337  decode.d8.loss_dice: 0.3074
09/30 10:13:43 - mmengine - INFO - Iter(train) [  9800/320000]  base_lr: 9.7240e-05 lr: 9.7240e-06  eta: 1 day, 13:16:42  time: 0.4316  data_time: 0.0087  memory: 5146  grad_norm: 76.1849  loss: 8.2589  decode.loss_cls: 0.2375  decode.loss_mask: 0.2821  decode.loss_dice: 0.2405  decode.d0.loss_cls: 0.8407  decode.d0.loss_mask: 0.2776  decode.d0.loss_dice: 0.3051  decode.d1.loss_cls: 0.1742  decode.d1.loss_mask: 0.2706  decode.d1.loss_dice: 0.2730  decode.d2.loss_cls: 0.1719  decode.d2.loss_mask: 0.2656  decode.d2.loss_dice: 0.2894  decode.d3.loss_cls: 0.2358  decode.d3.loss_mask: 0.2675  decode.d3.loss_dice: 0.2748  decode.d4.loss_cls: 0.1885  decode.d4.loss_mask: 0.2659  decode.d4.loss_dice: 0.2848  decode.d5.loss_cls: 0.2182  decode.d5.loss_mask: 0.2712  decode.d5.loss_dice: 0.2682  decode.d6.loss_cls: 0.2675  decode.d6.loss_mask: 0.2688  decode.d6.loss_dice: 0.2753  decode.d7.loss_cls: 0.2424  decode.d7.loss_mask: 0.2643  decode.d7.loss_dice: 0.2609  decode.d8.loss_cls: 0.2263  decode.d8.loss_mask: 0.2812  decode.d8.loss_dice: 0.2690
09/30 10:14:04 - mmengine - INFO - Iter(train) [  9850/320000]  base_lr: 9.7226e-05 lr: 9.7226e-06  eta: 1 day, 13:16:19  time: 0.4312  data_time: 0.0086  memory: 5161  grad_norm: 66.7074  loss: 8.8271  decode.loss_cls: 0.2343  decode.loss_mask: 0.2947  decode.loss_dice: 0.2287  decode.d0.loss_cls: 1.0690  decode.d0.loss_mask: 0.3021  decode.d0.loss_dice: 0.2468  decode.d1.loss_cls: 0.3829  decode.d1.loss_mask: 0.2866  decode.d1.loss_dice: 0.2329  decode.d2.loss_cls: 0.2959  decode.d2.loss_mask: 0.2855  decode.d2.loss_dice: 0.2370  decode.d3.loss_cls: 0.2692  decode.d3.loss_mask: 0.2878  decode.d3.loss_dice: 0.2310  decode.d4.loss_cls: 0.2443  decode.d4.loss_mask: 0.2852  decode.d4.loss_dice: 0.2419  decode.d5.loss_cls: 0.2424  decode.d5.loss_mask: 0.2836  decode.d5.loss_dice: 0.2399  decode.d6.loss_cls: 0.2521  decode.d6.loss_mask: 0.2870  decode.d6.loss_dice: 0.2603  decode.d7.loss_cls: 0.2684  decode.d7.loss_mask: 0.2852  decode.d7.loss_dice: 0.2572  decode.d8.loss_cls: 0.2560  decode.d8.loss_mask: 0.2921  decode.d8.loss_dice: 0.2471
09/30 10:14:26 - mmengine - INFO - Iter(train) [  9900/320000]  base_lr: 9.7212e-05 lr: 9.7212e-06  eta: 1 day, 13:15:56  time: 0.4314  data_time: 0.0087  memory: 5132  grad_norm: 73.5477  loss: 6.3577  decode.loss_cls: 0.0958  decode.loss_mask: 0.2664  decode.loss_dice: 0.1681  decode.d0.loss_cls: 0.9982  decode.d0.loss_mask: 0.2714  decode.d0.loss_dice: 0.1880  decode.d1.loss_cls: 0.1375  decode.d1.loss_mask: 0.2649  decode.d1.loss_dice: 0.1691  decode.d2.loss_cls: 0.1515  decode.d2.loss_mask: 0.2649  decode.d2.loss_dice: 0.1709  decode.d3.loss_cls: 0.1017  decode.d3.loss_mask: 0.2706  decode.d3.loss_dice: 0.1712  decode.d4.loss_cls: 0.0961  decode.d4.loss_mask: 0.2650  decode.d4.loss_dice: 0.1667  decode.d5.loss_cls: 0.0917  decode.d5.loss_mask: 0.2693  decode.d5.loss_dice: 0.1711  decode.d6.loss_cls: 0.1099  decode.d6.loss_mask: 0.2705  decode.d6.loss_dice: 0.1763  decode.d7.loss_cls: 0.0843  decode.d7.loss_mask: 0.2701  decode.d7.loss_dice: 0.1692  decode.d8.loss_cls: 0.0918  decode.d8.loss_mask: 0.2676  decode.d8.loss_dice: 0.1678
09/30 10:14:47 - mmengine - INFO - Iter(train) [  9950/320000]  base_lr: 9.7197e-05 lr: 9.7197e-06  eta: 1 day, 13:15:33  time: 0.4328  data_time: 0.0088  memory: 5180  grad_norm: 100.9532  loss: 11.7531  decode.loss_cls: 0.4472  decode.loss_mask: 0.3893  decode.loss_dice: 0.3273  decode.d0.loss_cls: 1.0326  decode.d0.loss_mask: 0.3600  decode.d0.loss_dice: 0.3510  decode.d1.loss_cls: 0.4682  decode.d1.loss_mask: 0.3570  decode.d1.loss_dice: 0.3116  decode.d2.loss_cls: 0.3295  decode.d2.loss_mask: 0.3616  decode.d2.loss_dice: 0.3274  decode.d3.loss_cls: 0.3844  decode.d3.loss_mask: 0.3575  decode.d3.loss_dice: 0.3271  decode.d4.loss_cls: 0.4431  decode.d4.loss_mask: 0.3601  decode.d4.loss_dice: 0.3011  decode.d5.loss_cls: 0.4315  decode.d5.loss_mask: 0.3539  decode.d5.loss_dice: 0.3159  decode.d6.loss_cls: 0.4548  decode.d6.loss_mask: 0.3668  decode.d6.loss_dice: 0.3280  decode.d7.loss_cls: 0.4562  decode.d7.loss_mask: 0.3683  decode.d7.loss_dice: 0.3135  decode.d8.loss_cls: 0.4373  decode.d8.loss_mask: 0.3796  decode.d8.loss_dice: 0.3112
09/30 10:15:09 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 10:15:09 - mmengine - INFO - Iter(train) [ 10000/320000]  base_lr: 9.7183e-05 lr: 9.7183e-06  eta: 1 day, 13:15:11  time: 0.4325  data_time: 0.0088  memory: 5160  grad_norm: 356.8860  loss: 15.1555  decode.loss_cls: 0.4583  decode.loss_mask: 0.5216  decode.loss_dice: 0.4197  decode.d0.loss_cls: 1.3169  decode.d0.loss_mask: 0.5285  decode.d0.loss_dice: 0.4063  decode.d1.loss_cls: 0.4953  decode.d1.loss_mask: 0.5264  decode.d1.loss_dice: 0.4214  decode.d2.loss_cls: 0.4874  decode.d2.loss_mask: 0.5063  decode.d2.loss_dice: 0.4000  decode.d3.loss_cls: 0.4828  decode.d3.loss_mask: 0.5322  decode.d3.loss_dice: 0.4026  decode.d4.loss_cls: 0.4571  decode.d4.loss_mask: 0.5008  decode.d4.loss_dice: 0.3894  decode.d5.loss_cls: 0.4772  decode.d5.loss_mask: 0.5803  decode.d5.loss_dice: 0.4165  decode.d6.loss_cls: 0.4819  decode.d6.loss_mask: 0.5782  decode.d6.loss_dice: 0.4319  decode.d7.loss_cls: 0.4865  decode.d7.loss_mask: 0.5324  decode.d7.loss_dice: 0.4043  decode.d8.loss_cls: 0.4743  decode.d8.loss_mask: 0.6058  decode.d8.loss_dice: 0.4331
09/30 10:15:31 - mmengine - INFO - Iter(train) [ 10050/320000]  base_lr: 9.7169e-05 lr: 9.7169e-06  eta: 1 day, 13:14:48  time: 0.4321  data_time: 0.0088  memory: 5160  grad_norm: 184.5324  loss: 15.1434  decode.loss_cls: 0.3640  decode.loss_mask: 0.9115  decode.loss_dice: 0.3460  decode.d0.loss_cls: 1.2024  decode.d0.loss_mask: 0.4769  decode.d0.loss_dice: 0.3782  decode.d1.loss_cls: 0.5511  decode.d1.loss_mask: 0.4793  decode.d1.loss_dice: 0.3148  decode.d2.loss_cls: 0.5253  decode.d2.loss_mask: 0.4945  decode.d2.loss_dice: 0.2937  decode.d3.loss_cls: 0.4875  decode.d3.loss_mask: 0.4951  decode.d3.loss_dice: 0.3544  decode.d4.loss_cls: 0.4226  decode.d4.loss_mask: 0.7281  decode.d4.loss_dice: 0.3095  decode.d5.loss_cls: 0.5461  decode.d5.loss_mask: 0.4888  decode.d5.loss_dice: 0.3281  decode.d6.loss_cls: 0.5478  decode.d6.loss_mask: 0.5080  decode.d6.loss_dice: 0.3134  decode.d7.loss_cls: 0.3931  decode.d7.loss_mask: 0.7527  decode.d7.loss_dice: 0.3751  decode.d8.loss_cls: 0.3964  decode.d8.loss_mask: 1.0128  decode.d8.loss_dice: 0.3459
09/30 10:15:52 - mmengine - INFO - Iter(train) [ 10100/320000]  base_lr: 9.7155e-05 lr: 9.7155e-06  eta: 1 day, 13:14:24  time: 0.4305  data_time: 0.0087  memory: 5180  grad_norm: 66.5665  loss: 7.1246  decode.loss_cls: 0.2192  decode.loss_mask: 0.2435  decode.loss_dice: 0.1731  decode.d0.loss_cls: 1.0032  decode.d0.loss_mask: 0.2547  decode.d0.loss_dice: 0.1921  decode.d1.loss_cls: 0.2152  decode.d1.loss_mask: 0.2576  decode.d1.loss_dice: 0.1810  decode.d2.loss_cls: 0.1755  decode.d2.loss_mask: 0.2554  decode.d2.loss_dice: 0.1751  decode.d3.loss_cls: 0.2052  decode.d3.loss_mask: 0.2479  decode.d3.loss_dice: 0.1690  decode.d4.loss_cls: 0.2246  decode.d4.loss_mask: 0.2464  decode.d4.loss_dice: 0.1689  decode.d5.loss_cls: 0.2192  decode.d5.loss_mask: 0.2529  decode.d5.loss_dice: 0.1773  decode.d6.loss_cls: 0.1999  decode.d6.loss_mask: 0.2438  decode.d6.loss_dice: 0.1706  decode.d7.loss_cls: 0.2061  decode.d7.loss_mask: 0.2499  decode.d7.loss_dice: 0.1750  decode.d8.loss_cls: 0.1993  decode.d8.loss_mask: 0.2488  decode.d8.loss_dice: 0.1742
09/30 10:16:14 - mmengine - INFO - Iter(train) [ 10150/320000]  base_lr: 9.7141e-05 lr: 9.7141e-06  eta: 1 day, 13:14:01  time: 0.4313  data_time: 0.0084  memory: 5160  grad_norm: 146.7148  loss: 11.4465  decode.loss_cls: 0.4665  decode.loss_mask: 0.3473  decode.loss_dice: 0.2564  decode.d0.loss_cls: 1.4712  decode.d0.loss_mask: 0.4036  decode.d0.loss_dice: 0.3410  decode.d1.loss_cls: 0.5271  decode.d1.loss_mask: 0.3653  decode.d1.loss_dice: 0.2837  decode.d2.loss_cls: 0.4381  decode.d2.loss_mask: 0.3312  decode.d2.loss_dice: 0.2834  decode.d3.loss_cls: 0.3823  decode.d3.loss_mask: 0.3201  decode.d3.loss_dice: 0.2657  decode.d4.loss_cls: 0.3669  decode.d4.loss_mask: 0.3099  decode.d4.loss_dice: 0.2566  decode.d5.loss_cls: 0.4035  decode.d5.loss_mask: 0.3391  decode.d5.loss_dice: 0.2606  decode.d6.loss_cls: 0.4074  decode.d6.loss_mask: 0.3274  decode.d6.loss_dice: 0.2511  decode.d7.loss_cls: 0.4481  decode.d7.loss_mask: 0.3307  decode.d7.loss_dice: 0.2627  decode.d8.loss_cls: 0.4108  decode.d8.loss_mask: 0.3217  decode.d8.loss_dice: 0.2670
09/30 10:16:35 - mmengine - INFO - Iter(train) [ 10200/320000]  base_lr: 9.7127e-05 lr: 9.7127e-06  eta: 1 day, 13:13:37  time: 0.4316  data_time: 0.0086  memory: 5179  grad_norm: 49.6348  loss: 6.9076  decode.loss_cls: 0.1618  decode.loss_mask: 0.2682  decode.loss_dice: 0.1771  decode.d0.loss_cls: 0.9832  decode.d0.loss_mask: 0.2736  decode.d0.loss_dice: 0.2124  decode.d1.loss_cls: 0.1626  decode.d1.loss_mask: 0.2740  decode.d1.loss_dice: 0.1754  decode.d2.loss_cls: 0.1510  decode.d2.loss_mask: 0.2685  decode.d2.loss_dice: 0.1852  decode.d3.loss_cls: 0.1486  decode.d3.loss_mask: 0.2652  decode.d3.loss_dice: 0.1977  decode.d4.loss_cls: 0.1258  decode.d4.loss_mask: 0.2711  decode.d4.loss_dice: 0.1976  decode.d5.loss_cls: 0.1424  decode.d5.loss_mask: 0.2667  decode.d5.loss_dice: 0.2010  decode.d6.loss_cls: 0.1272  decode.d6.loss_mask: 0.2709  decode.d6.loss_dice: 0.1974  decode.d7.loss_cls: 0.1381  decode.d7.loss_mask: 0.2680  decode.d7.loss_dice: 0.1958  decode.d8.loss_cls: 0.1467  decode.d8.loss_mask: 0.2652  decode.d8.loss_dice: 0.1891
09/30 10:16:57 - mmengine - INFO - Iter(train) [ 10250/320000]  base_lr: 9.7113e-05 lr: 9.7113e-06  eta: 1 day, 13:13:13  time: 0.4307  data_time: 0.0083  memory: 5180  grad_norm: 197.7228  loss: 11.9588  decode.loss_cls: 0.4096  decode.loss_mask: 0.3431  decode.loss_dice: 0.2918  decode.d0.loss_cls: 1.1382  decode.d0.loss_mask: 0.3672  decode.d0.loss_dice: 0.2854  decode.d1.loss_cls: 0.4837  decode.d1.loss_mask: 0.3742  decode.d1.loss_dice: 0.2705  decode.d2.loss_cls: 0.5125  decode.d2.loss_mask: 0.3639  decode.d2.loss_dice: 0.2754  decode.d3.loss_cls: 0.5258  decode.d3.loss_mask: 0.3509  decode.d3.loss_dice: 0.2625  decode.d4.loss_cls: 0.4840  decode.d4.loss_mask: 0.3671  decode.d4.loss_dice: 0.2903  decode.d5.loss_cls: 0.4732  decode.d5.loss_mask: 0.3985  decode.d5.loss_dice: 0.2882  decode.d6.loss_cls: 0.5030  decode.d6.loss_mask: 0.3605  decode.d6.loss_dice: 0.2844  decode.d7.loss_cls: 0.5734  decode.d7.loss_mask: 0.3454  decode.d7.loss_dice: 0.2889  decode.d8.loss_cls: 0.4206  decode.d8.loss_mask: 0.3496  decode.d8.loss_dice: 0.2768
09/30 10:17:18 - mmengine - INFO - Iter(train) [ 10300/320000]  base_lr: 9.7099e-05 lr: 9.7099e-06  eta: 1 day, 13:12:51  time: 0.4320  data_time: 0.0087  memory: 5180  grad_norm: 143.8030  loss: 8.3529  decode.loss_cls: 0.2086  decode.loss_mask: 0.2257  decode.loss_dice: 0.2662  decode.d0.loss_cls: 0.9540  decode.d0.loss_mask: 0.2303  decode.d0.loss_dice: 0.2272  decode.d1.loss_cls: 0.3597  decode.d1.loss_mask: 0.2228  decode.d1.loss_dice: 0.2509  decode.d2.loss_cls: 0.3435  decode.d2.loss_mask: 0.2237  decode.d2.loss_dice: 0.2547  decode.d3.loss_cls: 0.2575  decode.d3.loss_mask: 0.2276  decode.d3.loss_dice: 0.2714  decode.d4.loss_cls: 0.2728  decode.d4.loss_mask: 0.2220  decode.d4.loss_dice: 0.2711  decode.d5.loss_cls: 0.2375  decode.d5.loss_mask: 0.2330  decode.d5.loss_dice: 0.2964  decode.d6.loss_cls: 0.3108  decode.d6.loss_mask: 0.2355  decode.d6.loss_dice: 0.2518  decode.d7.loss_cls: 0.2594  decode.d7.loss_mask: 0.2289  decode.d7.loss_dice: 0.2476  decode.d8.loss_cls: 0.2685  decode.d8.loss_mask: 0.2355  decode.d8.loss_dice: 0.2583
09/30 10:17:40 - mmengine - INFO - Iter(train) [ 10350/320000]  base_lr: 9.7085e-05 lr: 9.7085e-06  eta: 1 day, 13:12:28  time: 0.4313  data_time: 0.0088  memory: 5161  grad_norm: 121.7700  loss: 10.9665  decode.loss_cls: 0.2282  decode.loss_mask: 0.4041  decode.loss_dice: 0.3515  decode.d0.loss_cls: 1.0476  decode.d0.loss_mask: 0.4136  decode.d0.loss_dice: 0.3622  decode.d1.loss_cls: 0.3331  decode.d1.loss_mask: 0.4083  decode.d1.loss_dice: 0.3327  decode.d2.loss_cls: 0.2774  decode.d2.loss_mask: 0.4091  decode.d2.loss_dice: 0.3241  decode.d3.loss_cls: 0.2859  decode.d3.loss_mask: 0.4059  decode.d3.loss_dice: 0.3292  decode.d4.loss_cls: 0.3063  decode.d4.loss_mask: 0.3995  decode.d4.loss_dice: 0.3395  decode.d5.loss_cls: 0.2861  decode.d5.loss_mask: 0.4078  decode.d5.loss_dice: 0.3327  decode.d6.loss_cls: 0.2482  decode.d6.loss_mask: 0.4058  decode.d6.loss_dice: 0.3408  decode.d7.loss_cls: 0.2402  decode.d7.loss_mask: 0.4048  decode.d7.loss_dice: 0.3534  decode.d8.loss_cls: 0.2206  decode.d8.loss_mask: 0.4061  decode.d8.loss_dice: 0.3618
09/30 10:18:02 - mmengine - INFO - Iter(train) [ 10400/320000]  base_lr: 9.7070e-05 lr: 9.7070e-06  eta: 1 day, 13:12:06  time: 0.4325  data_time: 0.0089  memory: 5161  grad_norm: 135.4844  loss: 7.9738  decode.loss_cls: 0.2506  decode.loss_mask: 0.2866  decode.loss_dice: 0.2425  decode.d0.loss_cls: 0.8700  decode.d0.loss_mask: 0.2838  decode.d0.loss_dice: 0.2365  decode.d1.loss_cls: 0.2146  decode.d1.loss_mask: 0.2706  decode.d1.loss_dice: 0.2324  decode.d2.loss_cls: 0.2266  decode.d2.loss_mask: 0.2846  decode.d2.loss_dice: 0.2222  decode.d3.loss_cls: 0.2026  decode.d3.loss_mask: 0.2755  decode.d3.loss_dice: 0.2190  decode.d4.loss_cls: 0.2309  decode.d4.loss_mask: 0.2730  decode.d4.loss_dice: 0.2201  decode.d5.loss_cls: 0.2250  decode.d5.loss_mask: 0.2775  decode.d5.loss_dice: 0.2207  decode.d6.loss_cls: 0.2017  decode.d6.loss_mask: 0.2797  decode.d6.loss_dice: 0.2270  decode.d7.loss_cls: 0.2526  decode.d7.loss_mask: 0.2830  decode.d7.loss_dice: 0.2230  decode.d8.loss_cls: 0.2323  decode.d8.loss_mask: 0.2863  decode.d8.loss_dice: 0.2229
09/30 10:18:23 - mmengine - INFO - Iter(train) [ 10450/320000]  base_lr: 9.7056e-05 lr: 9.7056e-06  eta: 1 day, 13:11:43  time: 0.4321  data_time: 0.0085  memory: 5180  grad_norm: 56.8347  loss: 8.7438  decode.loss_cls: 0.2511  decode.loss_mask: 0.2665  decode.loss_dice: 0.2566  decode.d0.loss_cls: 0.9852  decode.d0.loss_mask: 0.2656  decode.d0.loss_dice: 0.2587  decode.d1.loss_cls: 0.3585  decode.d1.loss_mask: 0.2676  decode.d1.loss_dice: 0.3060  decode.d2.loss_cls: 0.2653  decode.d2.loss_mask: 0.2613  decode.d2.loss_dice: 0.2656  decode.d3.loss_cls: 0.2526  decode.d3.loss_mask: 0.2601  decode.d3.loss_dice: 0.2610  decode.d4.loss_cls: 0.2441  decode.d4.loss_mask: 0.2694  decode.d4.loss_dice: 0.2729  decode.d5.loss_cls: 0.2574  decode.d5.loss_mask: 0.2640  decode.d5.loss_dice: 0.2721  decode.d6.loss_cls: 0.2573  decode.d6.loss_mask: 0.2723  decode.d6.loss_dice: 0.2772  decode.d7.loss_cls: 0.2714  decode.d7.loss_mask: 0.2626  decode.d7.loss_dice: 0.2646  decode.d8.loss_cls: 0.2460  decode.d8.loss_mask: 0.2672  decode.d8.loss_dice: 0.2634
09/30 10:18:45 - mmengine - INFO - Iter(train) [ 10500/320000]  base_lr: 9.7042e-05 lr: 9.7042e-06  eta: 1 day, 13:11:21  time: 0.4317  data_time: 0.0086  memory: 5160  grad_norm: 79.4162  loss: 7.9997  decode.loss_cls: 0.1341  decode.loss_mask: 0.2997  decode.loss_dice: 0.2349  decode.d0.loss_cls: 0.8989  decode.d0.loss_mask: 0.3145  decode.d0.loss_dice: 0.2447  decode.d1.loss_cls: 0.2772  decode.d1.loss_mask: 0.2966  decode.d1.loss_dice: 0.2304  decode.d2.loss_cls: 0.2247  decode.d2.loss_mask: 0.2965  decode.d2.loss_dice: 0.2480  decode.d3.loss_cls: 0.1844  decode.d3.loss_mask: 0.2982  decode.d3.loss_dice: 0.2332  decode.d4.loss_cls: 0.2152  decode.d4.loss_mask: 0.2940  decode.d4.loss_dice: 0.2333  decode.d5.loss_cls: 0.1926  decode.d5.loss_mask: 0.2921  decode.d5.loss_dice: 0.2291  decode.d6.loss_cls: 0.1964  decode.d6.loss_mask: 0.2957  decode.d6.loss_dice: 0.2335  decode.d7.loss_cls: 0.2023  decode.d7.loss_mask: 0.2943  decode.d7.loss_dice: 0.2386  decode.d8.loss_cls: 0.1330  decode.d8.loss_mask: 0.2976  decode.d8.loss_dice: 0.2359
09/30 10:19:06 - mmengine - INFO - Iter(train) [ 10550/320000]  base_lr: 9.7028e-05 lr: 9.7028e-06  eta: 1 day, 13:10:58  time: 0.4322  data_time: 0.0087  memory: 5161  grad_norm: 132.0740  loss: 9.3919  decode.loss_cls: 0.1978  decode.loss_mask: 0.3159  decode.loss_dice: 0.3236  decode.d0.loss_cls: 0.9821  decode.d0.loss_mask: 0.2936  decode.d0.loss_dice: 0.2851  decode.d1.loss_cls: 0.2920  decode.d1.loss_mask: 0.3173  decode.d1.loss_dice: 0.3050  decode.d2.loss_cls: 0.2678  decode.d2.loss_mask: 0.3377  decode.d2.loss_dice: 0.2912  decode.d3.loss_cls: 0.2368  decode.d3.loss_mask: 0.3213  decode.d3.loss_dice: 0.2954  decode.d4.loss_cls: 0.2513  decode.d4.loss_mask: 0.3127  decode.d4.loss_dice: 0.2986  decode.d5.loss_cls: 0.2172  decode.d5.loss_mask: 0.3062  decode.d5.loss_dice: 0.2994  decode.d6.loss_cls: 0.2622  decode.d6.loss_mask: 0.3038  decode.d6.loss_dice: 0.2975  decode.d7.loss_cls: 0.2847  decode.d7.loss_mask: 0.3178  decode.d7.loss_dice: 0.3120  decode.d8.loss_cls: 0.2358  decode.d8.loss_mask: 0.3167  decode.d8.loss_dice: 0.3132
09/30 10:19:28 - mmengine - INFO - Iter(train) [ 10600/320000]  base_lr: 9.7014e-05 lr: 9.7014e-06  eta: 1 day, 13:10:35  time: 0.4314  data_time: 0.0088  memory: 5160  grad_norm: 110.1955  loss: 11.3924  decode.loss_cls: 0.4199  decode.loss_mask: 0.3274  decode.loss_dice: 0.2569  decode.d0.loss_cls: 1.4275  decode.d0.loss_mask: 0.3550  decode.d0.loss_dice: 0.2854  decode.d1.loss_cls: 0.5525  decode.d1.loss_mask: 0.3400  decode.d1.loss_dice: 0.2715  decode.d2.loss_cls: 0.4139  decode.d2.loss_mask: 0.3677  decode.d2.loss_dice: 0.2421  decode.d3.loss_cls: 0.3991  decode.d3.loss_mask: 0.3635  decode.d3.loss_dice: 0.2606  decode.d4.loss_cls: 0.4502  decode.d4.loss_mask: 0.3376  decode.d4.loss_dice: 0.2479  decode.d5.loss_cls: 0.4596  decode.d5.loss_mask: 0.3306  decode.d5.loss_dice: 0.2414  decode.d6.loss_cls: 0.4462  decode.d6.loss_mask: 0.3357  decode.d6.loss_dice: 0.2518  decode.d7.loss_cls: 0.4695  decode.d7.loss_mask: 0.3315  decode.d7.loss_dice: 0.2403  decode.d8.loss_cls: 0.3933  decode.d8.loss_mask: 0.3273  decode.d8.loss_dice: 0.2466
09/30 10:19:50 - mmengine - INFO - Iter(train) [ 10650/320000]  base_lr: 9.7000e-05 lr: 9.7000e-06  eta: 1 day, 13:10:13  time: 0.4310  data_time: 0.0087  memory: 5160  grad_norm: 67.5458  loss: 8.5426  decode.loss_cls: 0.2665  decode.loss_mask: 0.2806  decode.loss_dice: 0.2350  decode.d0.loss_cls: 0.9303  decode.d0.loss_mask: 0.2769  decode.d0.loss_dice: 0.2533  decode.d1.loss_cls: 0.2984  decode.d1.loss_mask: 0.2876  decode.d1.loss_dice: 0.2238  decode.d2.loss_cls: 0.2368  decode.d2.loss_mask: 0.2825  decode.d2.loss_dice: 0.2221  decode.d3.loss_cls: 0.2431  decode.d3.loss_mask: 0.2816  decode.d3.loss_dice: 0.2323  decode.d4.loss_cls: 0.2674  decode.d4.loss_mask: 0.2783  decode.d4.loss_dice: 0.2456  decode.d5.loss_cls: 0.2782  decode.d5.loss_mask: 0.2853  decode.d5.loss_dice: 0.2379  decode.d6.loss_cls: 0.2771  decode.d6.loss_mask: 0.2828  decode.d6.loss_dice: 0.2549  decode.d7.loss_cls: 0.2901  decode.d7.loss_mask: 0.2806  decode.d7.loss_dice: 0.2550  decode.d8.loss_cls: 0.2420  decode.d8.loss_mask: 0.2849  decode.d8.loss_dice: 0.2315
09/30 10:20:11 - mmengine - INFO - Iter(train) [ 10700/320000]  base_lr: 9.6986e-05 lr: 9.6986e-06  eta: 1 day, 13:09:51  time: 0.4330  data_time: 0.0086  memory: 5180  grad_norm: 217.0639  loss: 12.8607  decode.loss_cls: 0.4990  decode.loss_mask: 0.2985  decode.loss_dice: 0.3126  decode.d0.loss_cls: 1.2362  decode.d0.loss_mask: 0.3032  decode.d0.loss_dice: 0.3750  decode.d1.loss_cls: 0.6926  decode.d1.loss_mask: 0.2850  decode.d1.loss_dice: 0.3259  decode.d2.loss_cls: 0.5433  decode.d2.loss_mask: 0.2943  decode.d2.loss_dice: 0.2875  decode.d3.loss_cls: 0.6199  decode.d3.loss_mask: 0.2991  decode.d3.loss_dice: 0.3360  decode.d4.loss_cls: 0.6915  decode.d4.loss_mask: 0.2889  decode.d4.loss_dice: 0.3099  decode.d5.loss_cls: 0.5329  decode.d5.loss_mask: 0.2846  decode.d5.loss_dice: 0.3192  decode.d6.loss_cls: 0.5973  decode.d6.loss_mask: 0.3142  decode.d6.loss_dice: 0.3397  decode.d7.loss_cls: 0.5476  decode.d7.loss_mask: 0.4005  decode.d7.loss_dice: 0.3531  decode.d8.loss_cls: 0.5019  decode.d8.loss_mask: 0.3344  decode.d8.loss_dice: 0.3367
09/30 10:20:33 - mmengine - INFO - Iter(train) [ 10750/320000]  base_lr: 9.6972e-05 lr: 9.6972e-06  eta: 1 day, 13:09:29  time: 0.4330  data_time: 0.0083  memory: 5180  grad_norm: 75.4275  loss: 9.2318  decode.loss_cls: 0.3514  decode.loss_mask: 0.2499  decode.loss_dice: 0.2474  decode.d0.loss_cls: 1.2266  decode.d0.loss_mask: 0.2428  decode.d0.loss_dice: 0.2462  decode.d1.loss_cls: 0.5116  decode.d1.loss_mask: 0.2393  decode.d1.loss_dice: 0.2348  decode.d2.loss_cls: 0.3265  decode.d2.loss_mask: 0.2359  decode.d2.loss_dice: 0.2280  decode.d3.loss_cls: 0.2891  decode.d3.loss_mask: 0.2386  decode.d3.loss_dice: 0.2353  decode.d4.loss_cls: 0.3444  decode.d4.loss_mask: 0.2346  decode.d4.loss_dice: 0.2458  decode.d5.loss_cls: 0.3059  decode.d5.loss_mask: 0.2416  decode.d5.loss_dice: 0.2517  decode.d6.loss_cls: 0.3097  decode.d6.loss_mask: 0.2360  decode.d6.loss_dice: 0.2341  decode.d7.loss_cls: 0.3440  decode.d7.loss_mask: 0.2377  decode.d7.loss_dice: 0.2440  decode.d8.loss_cls: 0.4165  decode.d8.loss_mask: 0.2470  decode.d8.loss_dice: 0.2356
09/30 10:20:55 - mmengine - INFO - Iter(train) [ 10800/320000]  base_lr: 9.6958e-05 lr: 9.6958e-06  eta: 1 day, 13:09:08  time: 0.4324  data_time: 0.0085  memory: 5145  grad_norm: 110.7498  loss: 10.6851  decode.loss_cls: 0.4223  decode.loss_mask: 0.2835  decode.loss_dice: 0.2735  decode.d0.loss_cls: 1.1044  decode.d0.loss_mask: 0.2735  decode.d0.loss_dice: 0.2852  decode.d1.loss_cls: 0.4367  decode.d1.loss_mask: 0.2711  decode.d1.loss_dice: 0.2737  decode.d2.loss_cls: 0.3456  decode.d2.loss_mask: 0.3318  decode.d2.loss_dice: 0.2848  decode.d3.loss_cls: 0.3900  decode.d3.loss_mask: 0.3180  decode.d3.loss_dice: 0.2783  decode.d4.loss_cls: 0.3806  decode.d4.loss_mask: 0.3162  decode.d4.loss_dice: 0.2738  decode.d5.loss_cls: 0.3974  decode.d5.loss_mask: 0.3294  decode.d5.loss_dice: 0.2838  decode.d6.loss_cls: 0.4045  decode.d6.loss_mask: 0.3053  decode.d6.loss_dice: 0.2860  decode.d7.loss_cls: 0.4268  decode.d7.loss_mask: 0.3473  decode.d7.loss_dice: 0.2896  decode.d8.loss_cls: 0.4453  decode.d8.loss_mask: 0.3391  decode.d8.loss_dice: 0.2877
09/30 10:21:16 - mmengine - INFO - Iter(train) [ 10850/320000]  base_lr: 9.6943e-05 lr: 9.6943e-06  eta: 1 day, 13:08:47  time: 0.4312  data_time: 0.0086  memory: 5180  grad_norm: 98.8186  loss: 9.9915  decode.loss_cls: 0.2720  decode.loss_mask: 0.3503  decode.loss_dice: 0.3281  decode.d0.loss_cls: 1.0380  decode.d0.loss_mask: 0.3273  decode.d0.loss_dice: 0.3531  decode.d1.loss_cls: 0.2293  decode.d1.loss_mask: 0.3482  decode.d1.loss_dice: 0.3390  decode.d2.loss_cls: 0.2491  decode.d2.loss_mask: 0.3311  decode.d2.loss_dice: 0.3262  decode.d3.loss_cls: 0.2696  decode.d3.loss_mask: 0.3471  decode.d3.loss_dice: 0.3323  decode.d4.loss_cls: 0.2453  decode.d4.loss_mask: 0.3348  decode.d4.loss_dice: 0.3202  decode.d5.loss_cls: 0.2198  decode.d5.loss_mask: 0.3553  decode.d5.loss_dice: 0.3144  decode.d6.loss_cls: 0.2333  decode.d6.loss_mask: 0.3615  decode.d6.loss_dice: 0.3148  decode.d7.loss_cls: 0.2445  decode.d7.loss_mask: 0.3593  decode.d7.loss_dice: 0.3282  decode.d8.loss_cls: 0.2508  decode.d8.loss_mask: 0.3472  decode.d8.loss_dice: 0.3215
09/30 10:21:38 - mmengine - INFO - Iter(train) [ 10900/320000]  base_lr: 9.6929e-05 lr: 9.6929e-06  eta: 1 day, 13:08:25  time: 0.4321  data_time: 0.0088  memory: 5161  grad_norm: 47.8875  loss: 7.4518  decode.loss_cls: 0.1509  decode.loss_mask: 0.2712  decode.loss_dice: 0.2024  decode.d0.loss_cls: 0.9949  decode.d0.loss_mask: 0.2767  decode.d0.loss_dice: 0.2423  decode.d1.loss_cls: 0.2137  decode.d1.loss_mask: 0.2682  decode.d1.loss_dice: 0.2258  decode.d2.loss_cls: 0.2328  decode.d2.loss_mask: 0.2714  decode.d2.loss_dice: 0.2237  decode.d3.loss_cls: 0.1746  decode.d3.loss_mask: 0.2666  decode.d3.loss_dice: 0.2177  decode.d4.loss_cls: 0.1709  decode.d4.loss_mask: 0.2724  decode.d4.loss_dice: 0.2126  decode.d5.loss_cls: 0.1701  decode.d5.loss_mask: 0.2712  decode.d5.loss_dice: 0.2169  decode.d6.loss_cls: 0.1925  decode.d6.loss_mask: 0.2702  decode.d6.loss_dice: 0.1980  decode.d7.loss_cls: 0.1312  decode.d7.loss_mask: 0.2703  decode.d7.loss_dice: 0.2059  decode.d8.loss_cls: 0.1689  decode.d8.loss_mask: 0.2686  decode.d8.loss_dice: 0.1992
09/30 10:21:59 - mmengine - INFO - Iter(train) [ 10950/320000]  base_lr: 9.6915e-05 lr: 9.6915e-06  eta: 1 day, 13:08:04  time: 0.4316  data_time: 0.0085  memory: 5161  grad_norm: 127.6968  loss: 9.2138  decode.loss_cls: 0.2538  decode.loss_mask: 0.3243  decode.loss_dice: 0.2640  decode.d0.loss_cls: 0.9157  decode.d0.loss_mask: 0.3261  decode.d0.loss_dice: 0.3066  decode.d1.loss_cls: 0.2380  decode.d1.loss_mask: 0.3169  decode.d1.loss_dice: 0.2815  decode.d2.loss_cls: 0.3252  decode.d2.loss_mask: 0.3256  decode.d2.loss_dice: 0.2833  decode.d3.loss_cls: 0.2441  decode.d3.loss_mask: 0.3428  decode.d3.loss_dice: 0.2646  decode.d4.loss_cls: 0.2991  decode.d4.loss_mask: 0.3020  decode.d4.loss_dice: 0.2646  decode.d5.loss_cls: 0.2755  decode.d5.loss_mask: 0.3287  decode.d5.loss_dice: 0.2429  decode.d6.loss_cls: 0.2631  decode.d6.loss_mask: 0.2979  decode.d6.loss_dice: 0.2507  decode.d7.loss_cls: 0.2322  decode.d7.loss_mask: 0.3023  decode.d7.loss_dice: 0.2533  decode.d8.loss_cls: 0.2560  decode.d8.loss_mask: 0.3424  decode.d8.loss_dice: 0.2907
09/30 10:22:21 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250930_090242
09/30 10:22:21 - mmengine - INFO - Iter(train) [ 11000/320000]  base_lr: 9.6901e-05 lr: 9.6901e-06  eta: 1 day, 13:07:43  time: 0.4331  data_time: 0.0088  memory: 5180  grad_norm: 109.8551  loss: 7.9984  decode.loss_cls: 0.1325  decode.loss_mask: 0.2947  decode.loss_dice: 0.2613  decode.d0.loss_cls: 0.8854  decode.d0.loss_mask: 0.3022  decode.d0.loss_dice: 0.2687  decode.d1.loss_cls: 0.2177  decode.d1.loss_mask: 0.2915  decode.d1.loss_dice: 0.2749  decode.d2.loss_cls: 0.1766  decode.d2.loss_mask: 0.2948  decode.d2.loss_dice: 0.2596  decode.d3.loss_cls: 0.1657  decode.d3.loss_mask: 0.2963  decode.d3.loss_dice: 0.2500  decode.d4.loss_cls: 0.1650  decode.d4.loss_mask: 0.2869  decode.d4.loss_dice: 0.2610  decode.d5.loss_cls: 0.1859  decode.d5.loss_mask: 0.2835  decode.d5.loss_dice: 0.2494  decode.d6.loss_cls: 0.1936  decode.d6.loss_mask: 0.2892  decode.d6.loss_dice: 0.2501  decode.d7.loss_cls: 0.1992  decode.d7.loss_mask: 0.2900  decode.d7.loss_dice: 0.2494  decode.d8.loss_cls: 0.1802  decode.d8.loss_mask: 0.2925  decode.d8.loss_dice: 0.2505
09/30 10:22:43 - mmengine - INFO - Iter(train) [ 11050/320000]  base_lr: 9.6887e-05 lr: 9.6887e-06  eta: 1 day, 13:07:21  time: 0.4319  data_time: 0.0085  memory: 5161  grad_norm: 311.8745  loss: 10.7245  decode.loss_cls: 0.2769  decode.loss_mask: 0.3671  decode.loss_dice: 0.3598  decode.d0.loss_cls: 1.0635  decode.d0.loss_mask: 0.3917  decode.d0.loss_dice: 0.3740  decode.d1.loss_cls: 0.3453  decode.d1.loss_mask: 0.3308  decode.d1.loss_dice: 0.3325  decode.d2.loss_cls: 0.2917  decode.d2.loss_mask: 0.3406  decode.d2.loss_dice: 0.3335  decode.d3.loss_cls: 0.2744  decode.d3.loss_mask: 0.3660  decode.d3.loss_dice: 0.3442  decode.d4.loss_cls: 0.3464  decode.d4.loss_mask: 0.3418  decode.d4.loss_dice: 0.3460  decode.d5.loss_cls: 0.2738  decode.d5.loss_mask: 0.3936  decode.d5.loss_dice: 0.3675  decode.d6.loss_cls: 0.3363  decode.d6.loss_mask: 0.3240  decode.d6.loss_dice: 0.3167  decode.d7.loss_cls: 0.2941  decode.d7.loss_mask: 0.3178  decode.d7.loss_dice: 0.2980  decode.d8.loss_cls: 0.2776  decode.d8.loss_mask: 0.3569  decode.d8.loss_dice: 0.3418
09/30 10:23:04 - mmengine - INFO - Iter(train) [ 11100/320000]  base_lr: 9.6873e-05 lr: 9.6873e-06  eta: 1 day, 13:06:58  time: 0.4311  data_time: 0.0086  memory: 5147  grad_norm: 62.0605  loss: 9.1922  decode.loss_cls: 0.2217  decode.loss_mask: 0.2752  decode.loss_dice: 0.2989  decode.d0.loss_cls: 0.8213  decode.d0.loss_mask: 0.2679  decode.d0.loss_dice: 0.3482  decode.d1.loss_cls: 0.4920  decode.d1.loss_mask: 0.2561  decode.d1.loss_dice: 0.3150  decode.d2.loss_cls: 0.2174  decode.d2.loss_mask: 0.2782  decode.d2.loss_dice: 0.3291  decode.d3.loss_cls: 0.2188  decode.d3.loss_mask: 0.2845  decode.d3.loss_dice: 0.3164  decode.d4.loss_cls: 0.3517  decode.d4.loss_mask: 0.2500  decode.d4.loss_dice: 0.3189  decode.d5.loss_cls: 0.3201  decode.d5.loss_mask: 0.2517  decode.d5.loss_dice: 0.3064  decode.d6.loss_cls: 0.2115  decode.d6.loss_mask: 0.2847  decode.d6.loss_dice: 0.3132  decode.d7.loss_cls: 0.2777  decode.d7.loss_mask: 0.2637  decode.d7.loss_dice: 0.3192  decode.d8.loss_cls: 0.2032  decode.d8.loss_mask: 0.2773  decode.d8.loss_dice: 0.3024
09/30 10:23:26 - mmengine - INFO - Iter(train) [ 11150/320000]  base_lr: 9.6859e-05 lr: 9.6859e-06  eta: 1 day, 13:06:40  time: 0.4488  data_time: 0.0084  memory: 5160  grad_norm: 67.3946  loss: 8.6621  decode.loss_cls: 0.1745  decode.loss_mask: 0.2959  decode.loss_dice: 0.2469  decode.d0.loss_cls: 1.1502  decode.d0.loss_mask: 0.2921  decode.d0.loss_dice: 0.2483  decode.d1.loss_cls: 0.2699  decode.d1.loss_mask: 0.3090  decode.d1.loss_dice: 0.2458  decode.d2.loss_cls: 0.2952  decode.d2.loss_mask: 0.2974  decode.d2.loss_dice: 0.2363  decode.d3.loss_cls: 0.2093  decode.d3.loss_mask: 0.3037  decode.d3.loss_dice: 0.2413  decode.d4.loss_cls: 0.2055  decode.d4.loss_mask: 0.3034  decode.d4.loss_dice: 0.2367  decode.d5.loss_cls: 0.2281  decode.d5.loss_mask: 0.2964  decode.d5.loss_dice: 0.2397  decode.d6.loss_cls: 0.2971  decode.d6.loss_mask: 0.2931  decode.d6.loss_dice: 0.2387  decode.d7.loss_cls: 0.2071  decode.d7.loss_mask: 0.2949  decode.d7.loss_dice: 0.2435  decode.d8.loss_cls: 0.2300  decode.d8.loss_mask: 0.2992  decode.d8.loss_dice: 0.2331
09/30 10:23:48 - mmengine - INFO - Iter(train) [ 11200/320000]  base_lr: 9.6845e-05 lr: 9.6845e-06  eta: 1 day, 13:06:17  time: 0.4322  data_time: 0.0086  memory: 5161  grad_norm: 119.6967  loss: 15.6594  decode.loss_cls: 0.5505  decode.loss_mask: 0.6118  decode.loss_dice: 0.4461  decode.d0.loss_cls: 1.1667  decode.d0.loss_mask: 0.4035  decode.d0.loss_dice: 0.4523  decode.d1.loss_cls: 0.6915  decode.d1.loss_mask: 0.4451  decode.d1.loss_dice: 0.4398  decode.d2.loss_cls: 0.5746  decode.d2.loss_mask: 0.4193  decode.d2.loss_dice: 0.4322  decode.d3.loss_cls: 0.5092  decode.d3.loss_mask: 0.3909  decode.d3.loss_dice: 0.4300  decode.d4.loss_cls: 0.6373  decode.d4.loss_mask: 0.4996  decode.d4.loss_dice: 0.4163  decode.d5.loss_cls: 0.5314  decode.d5.loss_mask: 0.5326  decode.d5.loss_dice: 0.4171  decode.d6.loss_cls: 0.6788  decode.d6.loss_mask: 0.4709  decode.d6.loss_dice: 0.4325  decode.d7.loss_cls: 0.5940  decode.d7.loss_mask: 0.5120  decode.d7.loss_dice: 0.4609  decode.d8.loss_cls: 0.5182  decode.d8.loss_mask: 0.5274  decode.d8.loss_dice: 0.4671
09/30 10:24:09 - mmengine - INFO - Iter(train) [ 11250/320000]  base_lr: 9.6831e-05 lr: 9.6831e-06  eta: 1 day, 13:05:54  time: 0.4319  data_time: 0.0087  memory: 5161  grad_norm: 193.9095  loss: 9.0192  decode.loss_cls: 0.2690  decode.loss_mask: 0.3109  decode.loss_dice: 0.2218  decode.d0.loss_cls: 1.1371  decode.d0.loss_mask: 0.3314  decode.d0.loss_dice: 0.2496  decode.d1.loss_cls: 0.2481  decode.d1.loss_mask: 0.3321  decode.d1.loss_dice: 0.2363  decode.d2.loss_cls: 0.2449  decode.d2.loss_mask: 0.3202  decode.d2.loss_dice: 0.2513  decode.d3.loss_cls: 0.2308  decode.d3.loss_mask: 0.3240  decode.d3.loss_dice: 0.2390  decode.d4.loss_cls: 0.2568  decode.d4.loss_mask: 0.3201  decode.d4.loss_dice: 0.2483  decode.d5.loss_cls: 0.2108  decode.d5.loss_mask: 0.3328  decode.d5.loss_dice: 0.2601  decode.d6.loss_cls: 0.1742  decode.d6.loss_mask: 0.3437  decode.d6.loss_dice: 0.2560  decode.d7.loss_cls: 0.2174  decode.d7.loss_mask: 0.3613  decode.d7.loss_dice: 0.2579  decode.d8.loss_cls: 0.2925  decode.d8.loss_mask: 0.3088  decode.d8.loss_dice: 0.2322
09/30 10:24:31 - mmengine - INFO - Iter(train) [ 11300/320000]  base_lr: 9.6816e-05 lr: 9.6816e-06  eta: 1 day, 13:05:31  time: 0.4310  data_time: 0.0085  memory: 5145  grad_norm: 176.7901  loss: 8.9285  decode.loss_cls: 0.2571  decode.loss_mask: 0.2607  decode.loss_dice: 0.2575  decode.d0.loss_cls: 1.0399  decode.d0.loss_mask: 0.2804  decode.d0.loss_dice: 0.3028  decode.d1.loss_cls: 0.3457  decode.d1.loss_mask: 0.2775  decode.d1.loss_dice: 0.2663  decode.d2.loss_cls: 0.3322  decode.d2.loss_mask: 0.2708  decode.d2.loss_dice: 0.2721  decode.d3.loss_cls: 0.3002  decode.d3.loss_mask: 0.2663  decode.d3.loss_dice: 0.2496  decode.d4.loss_cls: 0.2950  decode.d4.loss_mask: 0.2626  decode.d4.loss_dice: 0.2484  decode.d5.loss_cls: 0.2922  decode.d5.loss_mask: 0.2641  decode.d5.loss_dice: 0.2620  decode.d6.loss_cls: 0.2954  decode.d6.loss_mask: 0.2639  decode.d6.loss_dice: 0.2647  decode.d7.loss_cls: 0.2450  decode.d7.loss_mask: 0.2663  decode.d7.loss_dice: 0.2525  decode.d8.loss_cls: 0.2344  decode.d8.loss_mask: 0.2620  decode.d8.loss_dice: 0.2410
09/30 10:24:52 - mmengine - INFO - Iter(train) [ 11350/320000]  base_lr: 9.6802e-05 lr: 9.6802e-06  eta: 1 day, 13:05:09  time: 0.4320  data_time: 0.0085  memory: 5161  grad_norm: 164.3440  loss: 9.7188  decode.loss_cls: 0.2645  decode.loss_mask: 0.3303  decode.loss_dice: 0.2928  decode.d0.loss_cls: 1.3016  decode.d0.loss_mask: 0.3321  decode.d0.loss_dice: 0.2750  decode.d1.loss_cls: 0.3281  decode.d1.loss_mask: 0.3476  decode.d1.loss_dice: 0.2687  decode.d2.loss_cls: 0.2564  decode.d2.loss_mask: 0.3384  decode.d2.loss_dice: 0.2650  decode.d3.loss_cls: 0.2205  decode.d3.loss_mask: 0.3298  decode.d3.loss_dice: 0.2766  decode.d4.loss_cls: 0.2139  decode.d4.loss_mask: 0.3248  decode.d4.loss_dice: 0.2701  decode.d5.loss_cls: 0.2243  decode.d5.loss_mask: 0.3072  decode.d5.loss_dice: 0.2591  decode.d6.loss_cls: 0.2799  decode.d6.loss_mask: 0.3400  decode.d6.loss_dice: 0.2994  decode.d7.loss_cls: 0.3080  decode.d7.loss_mask: 0.3308  decode.d7.loss_dice: 0.2663  decode.d8.loss_cls: 0.2723  decode.d8.loss_mask: 0.3230  decode.d8.loss_dice: 0.2724
